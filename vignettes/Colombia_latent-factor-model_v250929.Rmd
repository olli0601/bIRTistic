---
title: "Colombia latent factor model"
date: "2025-09-29"
output: 
  bookdown::html_document2:
    toc: TRUE
    toc_float: TRUE
    highlight: tango
  bookdown::pdf_book:
    keep_tex: yes
---

<style type="text/css">
h1{
  font-size: 24pt;
}
h2{
  font-size: 18pt;
}
body{
  font-size: 12pt;
}
</style>

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
# Data loading and pre-processing

```{r Load packages, include=TRUE, eval=TRUE, message=FALSE, echo=TRUE, warning=FALSE, tidy=TRUE}
#library(lmerTest)
#library(clubSandwich)
#library(rstanarm)
#library(rstan)
library(tidyverse)
library(data.table)
library(purrr)
require(ggplot2) # for plotting
require(ggsci) # for plotting colors
require(hexbin) # for plotting pair plots
require(bayesplot) # for plotting Stan outputs
require(knitr) # for Rmarkdown
require(kableExtra) # for Rmarkdown
require(cmdstanr) # for Stan
require(loo) # for Bayesian LOO
require(polycor) # esetimate polychoric correlations via ML
require(GGally) # plotting of pair plots
```

```{r Define data locations, include=TRUE, eval=TRUE, message=FALSE, echo=TRUE, warning=FALSE, tidy=TRUE}
dir.home <- '/Users/or105/Library/CloudStorage/OneDrive-ImperialCollegeLondon/OR_Work/2025/2025_project_Hope_Groups_Jordan'
dir.data <- file.path(dir.home, 'data')
dir.out <- file.path(dir.home, 'colombia-250929')
file.data <- file.path(dir.data,'Colombia_data_baseline_endline_itemised_250927.csv')
file.data.quasi <- file.path(dir.data,'Colombia_data_quasi_assignments_250927.csv')
set.seed(42L)
```

```{r Load and preprocess data, include=TRUE, eval=TRUE, message=FALSE, echo=TRUE, warning=FALSE, tidy=TRUE}
# Read in data
dp <- as.data.table(read.csv( file.data )) 

setnames(dp, c('SID','Timepoint','staff_name'),c('pid','time_label','f_label'))

# Separate out meta data/ covariates
col.meta <- c('age','sex','household_adults','household_children','child_impairment',
              'education','moved','maritalstat','income','income.adj',
              'income.per.person','outfhelp','ngp','services','services_FOOD',
              'services_HOUSING_SUBS','services_CHILDCARE','services_COUNSELING',
              'stressmeals','ruv','time_since_death', 
              'Months.since.caregiver.death','Months.since.caregiver.death.v2',
              'Months.since.caregiver.death.v3')
dmeta <- subset(dp, select = c('pid','time_label',col.meta))

# Keep core outcome data
set(dp, NULL, col.meta, NULL)

# Clean up outcome labels
setnames(dp, 
         c('CAREGIVER_MENTAL_HEALTH','nervous','hopeless','restless','sad','effort','worthless'),
         c('CG-MH_agg','CG-MH_nervous','CG-MH_hopeless','CG-MH_restless','CG-MH_sad','CG-MH_effort','CG-MH_worthless')
         )
setnames(dp, 
         c('PHYSICAL_EMOTIONAL_VIOLENCE','physic_punish','scream'),
         c('CG-VIO_agg','CG-VIO_ph-punish','CG-VIO_scream')
         )
setnames(dp, 
         c('POSITIVE_PARENTING','praise','play'),
         c('CG-POS_agg','CG-POS_praise','CG-POS_play')
         )
setnames(dp, 
         c('CHILD_MONITORING','safe_time','child_safe'),
         c('CG-MONITOR-CHI_agg','CG-MONITOR-CHI_safe-time','CG-MONITOR-CHI_child-safe')
         )
setnames(dp, 
         c('PARENTAL_INVOLVEMENT','help_learn','child_problems'),
         c('CG-INVOLVE_agg','CG-INVOLVE_help-learn','CG-INVOLVE_child-problems')
         )
setnames(dp, 
         c('CHILD_BEHAVIOURAL_ISSUES','angry','unhappy','no_interest'),
         c('CHI-BEHAVIOUR_agg','CHI-BEHAVIOUR_angry','CHI-BEHAVIOUR_unhappy','CHI-BEHAVIOUR_no-interest')
         )
setnames(dp, 
         c('DEPRESSION','SELFCARE','RESILIENCE','NONVIOLENT_DISCIPLINE'), 
         c('CG-DEPRESSION','CG-SELFCARE','CG-RESILIENCE','CG-NONVIOLENT-DISCIPLINE'))

# Clean up date
set(dp, NULL, 'submission_date', dp[, as.Date(submission_date, format = '%m/%d/%y')])

# Set time id
dp[, time := as.integer(time_label == 'Endline')]

# Remove participant with double endline - remove last record
dp <- subset(dp, !(pid %in% c('otmar20231963') & submission_date == '2024-12-12'))

# Remove participants with only baseline records 
# as we are interested in pre-post comparison
tmp <- dp[, list(n = length(submission_date)), by = 'pid']
tmp <- subset(tmp, n == 2, select = pid)
dp <- merge(tmp, dp, by = 'pid')

# Set participant id
tmp <- data.table( pid = dp[, sort(unique(pid))] )
tmp[, pid_new := 1:nrow(tmp)]
dp <- merge(dp, tmp, by = 'pid')
setnames(dp, c('pid','pid_new'), c('pid_label','pid'))

# Set facilitator id
tmp <- data.table( f_label = dp[, sort(unique(f_label))] )
tmp[, fid := 1:nrow(tmp)]
dp <- merge(dp, tmp, by = 'f_label')

# Bring table into long format
dp <- data.table::melt(dp, 
                       id.vars = c('time','time_label','pid','pid_label','fid','f_label','submission_date','d_year'),
                       variable.name = 'item_label',
                       value.name = 'y')

# Remove NA's
dp <- subset(dp, !is.na(y))

# Define character values for y
dp[, y_label := NA_character_]
tmp <- dp[, which(grepl('^CG-MH_.*',item_label) & !grepl('agg',item_label))]
set(dp, 
    tmp, 
    'y_label',
    c('a - none of the time','b - a little of the time','c - some of the time','d - most of the time', 'e - all of the time')[ dp[tmp,y] + 1L ]
    )
tmp <- dp[, which(is.na(y_label) & !grepl('agg',item_label))]
set(dp, tmp, 'y_label', dp[tmp, paste0(as.character(y), ' of 7 days')])

# Show value mins and maxs
dp[, list(ymin = min(y), ymax = max(y)), by = 'item_label']

dit <- unique(subset(dp, select = 'item_label'))
dit[, item_type := ifelse(grepl('CG-MH',item_label), 'categorical', 'out-of-7')]
dit[, item_high_label := ifelse(grepl('CG-MH|CG-DEPRESSION|CG-VIO|CHI-BEHAVIOUR',item_label),'lower_is_better','higher_is_better') ]
dit[, group_label := factor(gsub('([^_]+)_([^_]+)','\\1',item_label))]
dit[, item_label_short := gsub('([^_]+)_([^_]+)','\\2',item_label)]
set(dit, dit[, which(grepl('^CG', item_label_short))], 'item_label_short', '')
dit[, group_label_long := group_label]
tmp <- dit[, levels(group_label_long) ]
tmp <- sapply(tmp, function(x) switch(x,
    'CG-MH' = "Caregiver mental health",
    'CG-VIO' = "Caregiver exercising physical or emotional violence",
    'CG-MONITOR-CHI_agg' = 'Child monitoring',
    'CG-INVOLVE' = 'Caregiver involvement',
    'CHI-BEHAVIOUR' = 'Child behavioural issues',
    'CG-DEPRESSION' = 'Caregiver depression',
    'CG-SELFCARE' = 'Caregiver self-care',
    'CG-RESILIENCE' = 'Caregiver resilience',
    'CG-POS' = 'Caregiver positive parenting',
    'CG-MONITOR-CHI' = 'Caregiver monitoring child',
    'CG-NONVIOLENT-DISCIPLINE' = 'Caregiver exercising nonviolent discipline',
    x  
))
levels(dit$group_label_long) <- tmp
dit[, endpoint_measure := sapply(item_type, function(x) switch(x,
  'categorical' = 'events occurring most or all of the time',
  'out-of-7' = 'mean days in week',
  x
  ))]
```

# Data viz

```{r data_viz_response_frequencies, include=TRUE, eval=TRUE, message=FALSE, echo=FALSE, warning=FALSE, tidy=TRUE}
# Select ordered categorical outcome
dp1 <- subset(dp, !grepl('agg', item_label))
file.prefix <- 'colombia_outcomes_'

# Frequency plot
tmp <- dp1[, list(n = length(pid)), by = c('time_label','item_label','y_label')]
tmp2 <- tmp[, list(total = sum(n)), by = c('time_label','item_label')]
tmp <- merge(tmp, tmp2, by = c('time_label','item_label'))
tmp[, p := n/total]
tmp[, group_label := gsub('([^_]+)_([^_]+)','\\1',item_label)]

pal <- colorRampPalette(ggsci::pal_futurama("planetexpress")(12))(tmp[, length(unique(item_label))])

p <- ggplot(tmp, aes(x = y_label, y = p)) +
  geom_col(aes(fill = item_label), position = position_dodge2(width = 0.9, preserve = "single"), width = 0.8) +
  scale_y_continuous(labels = scales::percent) +
  scale_fill_manual(values = pal) +
  facet_grid(group_label ~ time_label, scales = "free_x", space = "free_x") +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 45,vjust = 1,hjust = 1),
        legend.position = 'bottom'
        ) +
  labs(x = '', fill = 'survey items', y = 'proportion of outcomes')
ggsave(file = file.path(dir.out, paste0(file.prefix,'response_frequencies.png')), 
       plot = p, 
       h = 17, 
       w = 12
       )
```

```{r data_viz_response_frequencies_plot, include=TRUE, eval=TRUE, echo=FALSE, tidy=FALSE, fig.align='center', out.width='80%'}
knitr::include_graphics( file.path(dir.out, paste0(file.prefix,'response_frequencies.png')) )
```

```{r data_viz_polychoric_correlations, include=TRUE, eval=TRUE, echo=TRUE, message=FALSE, tidy=FALSE, cache=TRUE}
tmp <- data.table:::dcast( dp1, time_label + pid ~ item_label, value.var = 'y' )
tmp <- melt(tmp, id.vars = c('time_label','pid','CG-SELFCARE'))
setnames(tmp, 'CG-SELFCARE', 'value0')
tmp <- tmp[,
    {
      z <- polycor::polychor(value0, value, std.err = TRUE, ML = FALSE)
      list(polychor = z$rho, polychor_sd = sqrt(as.numeric(z$var))) 
    }, 
    by = c('variable','time_label')
    ]
tmp[, polychor_cl := polychor - 2*polychor_sd]
tmp[, polychor_cu := polychor + 2*polychor_sd]
kbl(tmp, caption = 'Polychoric correlations between CG-SELFCARE and all other variables') %>%
	kable_styling(bootstrap_options = c("striped", "hover", "condensed"), font_size = 12) %>%
	scroll_box(height = "300px")
```

```{r data_viz_polychoric_correlations2, include=TRUE, eval=TRUE, echo=TRUE, message=FALSE, tidy=FALSE, cache=TRUE}
tmp[, polychor_sig95 := 1 - as.integer(polychor_cl < 0 & 0 < polychor_cu)]
tmp[, list(polychor_sig95_n = sum(polychor_sig95)) , by = 'time_label']
kbl(tmp, caption = 'Number of significant polychoric correlations between CG-SELFCARE and all other variables') %>%
	kable_styling(bootstrap_options = c("striped", "hover", "condensed"), font_size = 12) %>%
	scroll_box(height = "300px")
```


# Joint latent factor model

## First model: caregiver mental health only

Next up, I will investigate a more expressive multi-group model with group-specific item baselines and
group-specific loadings. I intentionally do not introducce group-specific participant effects, because the
same participants are present at Baseline and Endline.

\begin{align*}
& \text{Prob}(Y_{ij} = k) = \text{Ordered-Logistic}(k; \eta_{ij}, c_{G(i)}) \\
& \eta_{ij}= \gamma_{G(i),j} + \lambda_{G(i),j} * (\beta_{G(i)} + \beta^{\text{RE}}_i)\\
& \gamma_g \sim \text{S2Z-Normal}(0,1)\\
& \lambda_{g,1} = 1\\
& \lambda_{g,2:J} \sim \text{Normal}(0,1)\\
& \beta \sim \text{S2Z-Normal}(0, 1)\\
& \beta^{\text{RE}} \sim \text{S2Z-Normal}(0,1)\\
& c_g = [c_{g1}, c_{g1} + \text{cumsum}(\delta_{g,1:{K-2}})] \\
& c_{g1} \sim \text{Normal}(0 , 3.5^2)\\
& \delta_{g,1:{K-2}} \sim \text{Normal}(0 , 2.5^2)\\
\end{align*}

Here is the Stan code:

```{r ordered_logistic_latent_factor-b_stan_code, include=TRUE, eval=TRUE, echo=TRUE, tidy=FALSE, cache=TRUE}
ordered_categorical_logit_txt_m5b <- "
data
{
  int<lower=1> N; // number of observations
  int<lower=3> K; // number of categories
  int<lower=2> Q; // number of questions
  int<lower=2> P; // number of groups
  int<lower=1> U; // number of units
  array [N] int<lower=1, upper=K> y; // observations
  array [N] int<lower=1,upper=P> group_of_obs;
  array [N] int<lower=1,upper=Q> question_of_obs;
  array [N] int<lower=1,upper=U> unit_of_obs;
}
transformed data
{
    real s2z_sd_groups;
    real s2z_sd_unit;
    real s2z_sd_questions;
    s2z_sd_groups = inv(sqrt(1. - inv(P)));
    s2z_sd_unit = inv(sqrt(1. - inv(U)));
    s2z_sd_questions = inv(sqrt(1. - inv(Q)));
}
parameters
{
  sum_to_zero_vector[P] factor_group;
  array [P] sum_to_zero_vector[Q] beta_questions;
  array [P] vector<lower=0>[Q-1] loadings_questions_m1;
  sum_to_zero_vector[U] beta_unit;
  real<lower=0> beta_unit_sd;
  
  // the cutpoints in logit space, using the 'ordered' variable type
  real cut_point_1;
  matrix<lower=0>[K - 2,P] cut_point_gaps_groups;
}
transformed parameters
{
  array [P] ordered[K-1] cut_points;
  array [P] vector<lower=0>[Q] loadings_questions;
  
  for (p in 1:P)
  {
    cut_points[p] = 
      append_row(
        rep_vector(cut_point_1, 1), 
        rep_vector(cut_point_1, K-2) + cumulative_sum(cut_point_gaps_groups[,p])
        );
        
    loadings_questions[p] = append_row(1.0, loadings_questions_m1[p]);
  }
}
model
{
  // likelihood, cannot be vectorized
  for (n in 1:N) {
    y[n] ~ ordered_logistic(
      beta_questions[group_of_obs[n]][question_of_obs[n]] + 
        loadings_questions[group_of_obs[n]][question_of_obs[n]] *
        (
          factor_group[group_of_obs[n]] + 
          beta_unit_sd * beta_unit[unit_of_obs[n]]
        ), 
      cut_points[group_of_obs[n]]);
  }
  
  // priors
  factor_group ~ normal(0, s2z_sd_groups); 
  beta_unit ~ normal(0, s2z_sd_unit); 
  for (p in 1:P)
  {
    beta_questions[p] ~ normal(0, s2z_sd_questions); 
    loadings_questions_m1[p] ~ lognormal(0,1);
  }
  beta_unit_sd ~ exponential(2); 
  cut_point_1 ~ normal(0, 3.5); 
  to_vector(cut_point_gaps_groups) ~ normal(0, 2.5); 
}
generated quantities
{
  array [P,Q] real mean_eta;
  array [K,N] real<lower=0, upper=1> ordered_prob_by_obs;
  array [N] int<lower=0> ypred;
  array [N] real log_lik;
  vector[U] beta_unit_re;
  
  for (q in 1:Q) 
  {
    for (p in 1:P) 
    {
      mean_eta[p,q] = beta_questions[p][q] + loadings_questions[p][q] * factor_group[p];
    }
  }
  
  {
    real tmp_real;
    for (n in 1:N) 
    {
        tmp_real = beta_questions[group_of_obs[n]][question_of_obs[n]] + 
                    loadings_questions[group_of_obs[n]][question_of_obs[n]] *
                    (
                      factor_group[group_of_obs[n]] + 
                      beta_unit_sd * beta_unit[unit_of_obs[n]]
                    );
        ordered_prob_by_obs[1,n]       = 1 - inv_logit( tmp_real - cut_points[group_of_obs[n]][1] );
        ordered_prob_by_obs[2:(K-1),n] = to_array_1d( inv_logit( tmp_real - cut_points[group_of_obs[n]][1:(K-2)] ) 
                                                              - 
                                                              inv_logit( tmp_real - cut_points[group_of_obs[n]][2:(K-1)] )
                                                              );
        ordered_prob_by_obs[K,n]       = inv_logit( tmp_real - cut_points[group_of_obs[n]][K-1] );
    }
  }
  
  for( n in 1:N ) 
  {
    ypred[n] = ordered_logistic_rng( 
                beta_questions[group_of_obs[n]][question_of_obs[n]] + 
                  loadings_questions[group_of_obs[n]][question_of_obs[n]] *
                  (
                    factor_group[group_of_obs[n]] + 
                    beta_unit_sd * beta_unit[unit_of_obs[n]]
                  ), 
                cut_points[group_of_obs[n]]
                ); 
    log_lik[n] =  ordered_logistic_lpmf( y[n] |
                    beta_questions[group_of_obs[n]][question_of_obs[n]] + 
                      loadings_questions[group_of_obs[n]][question_of_obs[n]] *
                      (
                        factor_group[group_of_obs[n]] + 
                        beta_unit_sd * beta_unit[unit_of_obs[n]]
                      ),
                    cut_points[group_of_obs[n]]);
  }
  
  beta_unit_re = beta_unit_sd * beta_unit;
}
"

# compile the model
ordered_categorical_logit_m5b_filename <- cmdstanr::write_stan_file(
  gsub('\t',' ', ordered_categorical_logit_txt_m5b),
  dir = dir.out,
  basename = NULL,
  force_overwrite = FALSE,
  hash_salt = ""
)

# compile Stan model
ordered_categorical_logit_compiled_m5b <- cmdstanr::cmdstan_model(ordered_categorical_logit_m5b_filename)
```

Let's fit the model!

```{r ordered_logistic_latent_factor-b_prepare data, include=TRUE, eval=TRUE, echo=TRUE, message=FALSE, tidy=FALSE, cache=TRUE}
file.prefix <- 'cgmh_ordered_categorical_m5b_'
dcat1 <- subset(dp1, grepl('CG-MH',item_label))
dcat1[, time := time + 1L]
dcat1[, y_stan := y + 1L]
tmp <- data.table(item_label = sort(unique(dcat1$item_label)))
tmp[, item_id := 1:nrow(tmp)]
dcat1 <- merge(dcat1, tmp, by = 'item_label')
dcat1[, oid := 1:nrow(dcat1)]
```

```{r ordered_logistic_latent_factor-b_run_model, include=TRUE, eval=TRUE, echo=TRUE, message=FALSE, tidy=FALSE, cache=TRUE}


# define data in format needed for model specification
stan_data <- list()
stan_data$N <- nrow(dcat1)
stan_data$P <- max(dcat1$time)
stan_data$Q <- max(dcat1$item_id)
stan_data$U <- max(dcat1$pid)
stan_data$K <- length(unique(dcat1$y_stan))
stan_data$y <- dcat1$y_stan
stan_data$group_of_obs <- dcat1$time
stan_data$question_of_obs <- dcat1$item_id
stan_data$unit_of_obs <- dcat1$pid

# sample
ordered_categorical_logit_fit_m5b <- ordered_categorical_logit_compiled_m5b$sample(
  data = stan_data,
  seed = 123,
  chains = 2,
  parallel_chains = 2,
  iter_warmup = 5e2,
  iter_sampling = 15e2,
  refresh = 500,
  save_warmup = TRUE
)

# save output to RDS
ordered_categorical_logit_fit_m5b$save_object(file = file.path(dir.out, paste0(file.prefix,"stan.rds")))
```

```{r ordered_logistic_latent_factor-b_load_model, include=FALSE, eval=TRUE, echo=FALSE, tidy=FALSE, cache=TRUE}
ordered_categorical_logit_fit_m5b <- readRDS(file = file.path(dir.out, paste0(file.prefix,'stan.rds')))
```

```{r ordered_logistic_latent_factor-b_check_mixing_convergence, include=FALSE, eval=TRUE, echo=FALSE, tidy=FALSE, cache=TRUE}
#	check convergence and check mixing
#	get 95% credible intervals
tmp <- ordered_categorical_logit_fit_m5b$summary(
  variables = c('factor_group','beta_unit','beta_unit_sd','beta_questions',
                'loadings_questions_m1','cut_point_1','cut_point_gaps_groups'),
  posterior::default_summary_measures(),
  posterior::default_convergence_measures(),
  extra_quantiles = ~posterior::quantile2(., probs = c(.0275, .975))
)
tmp <- as.data.table(tmp)
tmp
  
# parameter with lowest ess_bulk
worst_var <- tmp$variable[ which.min(tmp$ess_bulk) ]

# extract samples
po <- ordered_categorical_logit_fit_m5b$draws(
  variables = c("lp__", worst_var),
  inc_warmup = TRUE,
  format = "draws_array"
  )

# make worst trace plot
p <- bayesplot:::mcmc_trace(po,  
                            pars = c("lp__",worst_var), 
                            n_warmup = 500,
                            facet_args = list(nrow = 2)
                            )
p <- p + theme_bw()
ggsave(file = file.path(dir.out, paste0(file.prefix,'worsttrace.png')), 
       plot = p, 
       h = 10, 
       w = 8
       )

# make intervals plot
po <- ordered_categorical_logit_fit_m5b$draws(
  variables = c('factor_group','beta_questions','loadings_questions',
                'cut_point_1','cut_point_gaps_groups',
                'beta_unit_sd','beta_unit'),
  inc_warmup = FALSE,
  format = "draws_array"
  )

color_scheme_set("teal")
p <- bayesplot::mcmc_intervals(po, prob = 0.5, prob_outer = 0.95, outer_size = 1, point_size = 2) +
  theme_bw()
ggsave(file = file.path(dir.out, paste0(file.prefix,'intervals.png')), 
       plot = p, 
       h = 50, 
       w = 8, 
       limitsize = FALSE
       )

# make pairs plot
po <- ordered_categorical_logit_fit_m5b$draws(
  variables = c('factor_group','cut_point_1','cut_point_gaps_groups'),
  inc_warmup = FALSE,
  format = "draws_array"
  )

bayesplot::color_scheme_set('viridisC')
p <- bayesplot::mcmc_pairs(po, 
                           diag_fun = "dens", 
                           off_diag_fun = "hex"
                           ) 
ggsave(file = file.path(dir.out, paste0(file.prefix,'pairsplot.png')), 
       plot = p, 
       h = 30, 
       w = 30, 
       limitsize = FALSE
       )
bayesplot::color_scheme_set('brewer-RdYlBu')

# make posterior predictive check

# create median and 95\% credible intervals
po <- ordered_categorical_logit_fit_m5b$draws(
  variables = c('ypred'),
  inc_warmup = FALSE,
  format = "draws_df"
  )
po <- as.data.table(po)
po <- data.table::melt(po, id.vars = c('.draw','.chain','.iteration'), value.name = 'ypred')
po[, oid := as.integer(gsub('ypred\\[([0-9]+)\\]','\\1',variable)) ]
pos <- 
  po[,
     list( summary_value = quantile(ypred, prob = c(0.025, 0.25, 0.5, 0.75, 0.975)),
           summary_name = c('q_lower','iqr_lower','median','iqr_upper','q_upper') 
           ),
     by = 'oid'
     ]
pos <- 
  data.table::dcast(pos,
                    oid ~ summary_name, 
                    value.var = 'summary_value'
                    )
pos <- merge(pos, dcat1, by = 'oid')
pos[, IN_PPI := y_stan >= q_lower & y_stan <= q_upper]
pos[, mean(IN_PPI)]

# plot posterior predictive check
p <- ggplot(pos, aes(x = oid, group = oid)) + 
  geom_boxplot( aes( ymin = q_lower,
                     lower = iqr_lower,
                     middle = median,
                     upper = iqr_upper,
                     ymax = q_upper),
                stat = 'identity') +
  geom_point( aes(y = y_stan, colour = IN_PPI ) ) +
  facet_grid(item_label ~ time_label, scales = 'free') +
  scale_x_discrete() +
  scale_y_continuous() +
  ggsci::scale_color_npg() +
  labs(x = '', 
       y = 'outcome', 
       colour = 'within\n95% posterior\nprediction\ninterval') +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 45,vjust = 1,hjust = 1))
ggsave(file = file.path(dir.out, paste0(file.prefix,'ppcheck.png')), 
       p, 
       w = 12, 
       h = 12)
```

```{r ordered_logistic_latent_factor-b_extract_participant_effects, include=FALSE, eval=TRUE, echo=FALSE, tidy=FALSE, cache=TRUE}
# extract participant effects
po <- ordered_categorical_logit_fit_m5b$draws(
  variables = c('beta_unit_re'),
  inc_warmup = FALSE,
  format = "draws_df"
  )
po <- as.data.table(po)
po <- data.table::melt(po, id.vars = c('.draw','.chain','.iteration'))
po[, pid := as.integer(gsub('beta_unit_re\\[([0-9]+)\\]','\\1',variable)) ]

pos <- po[, 
          list( summary_value = quantile(value, prob = c(0.025, 0.25, 0.5, 0.75, 0.975)),
                summary_name = c('q_lower','iqr_lower','median','iqr_upper','q_upper') 
                ), 
          by = c('pid')
          ]
pos <- data.table::dcast(pos, pid ~ summary_name, value.var = 'summary_value')
tmp <- unique(subset(dcat1, select = c(pid, time_label)))
pos <- merge(pos, tmp, by = c('pid'))
tmp <- dcat1[, list(avg_value = mean(y)), by = 'pid']
tmp[, avg_value_jitter := avg_value + runif(nrow(tmp), min = -1/8, max = 1/8)]
pos <- merge(pos, tmp, by = 'pid')

tmp <- pos[, list(avg_effect = mean(median)), by = c('time_label')]

p <- ggplot(data = pos, aes( x = avg_value_jitter)) +
  geom_hline(data = tmp, aes( yintercept = avg_effect), colour = 'grey50') +
  geom_linerange(aes( ymin = q_lower, ymax = q_upper), 
           stat = 'identity') +
  geom_point(aes( y = median)) +
  labs(x = '\naverage response of participant', y = 'participant effect') +
  theme_bw() +
  facet_grid(~ time_label, scales = 'free') +
  theme(axis.text.x = element_text(angle = 45,vjust = 1,hjust = 1),
        legend.position = 'bottom'
        )
ggsave(file = file.path(dir.out, paste0(file.prefix,'participant_effects.png')), 
       plot = p, 
       h = 8, 
       w = 12
       )
```

```{r ordered_logistic_latent_factor-b_participant_effect_plot, include=FALSE, eval=FALSE, echo=FALSE, tidy=FALSE, fig.align='center', out.width='80%'}
knitr::include_graphics( file.path(dir.out, paste0(file.prefix,'participant_effects.png')) )
```

```{r ordered_logistic_latent_factor-b_latent_eta, include=FALSE, eval=TRUE, echo=FALSE, tidy=FALSE, cache=TRUE}
# extract model probabilities
po <- ordered_categorical_logit_fit_m5b$draws(
  variables = c('mean_eta'),
  inc_warmup = FALSE,
  format = "draws_df"
  )
po <- as.data.table(po)
po <- data.table::melt(po, id.vars = c('.draw','.chain','.iteration'), value.name = 'mean_eta')
po[, time := as.integer(gsub('(.*)\\[([0-9]),([0-9])\\]$','\\2',variable)) ]
po[, item_id := as.integer(gsub('(.*)\\[([0-9]),([0-9])\\]$','\\3',variable)) ]
set(po, NULL, 'variable', NULL)
tmp <- unique( subset(dcat1, select = c(time, time_label, item_id, item_label)) )
po <- merge(po, tmp, by = c('time','item_id'))
po <- data.table::dcast(po, .draw + time + time_label ~ item_label, value.var = 'mean_eta')
p <- GGally::ggpairs(po, 
                columns = dcat1[, as.character(unique(item_label))],
                ggplot2::aes(colour = time_label),
                lower = list(continuous = ggally_density),
                diag = list(continuous = 'blankDiag')
                )
p <- p + scale_fill_npg() + 
  theme_bw()
ggsave(file = file.path(dir.out, paste0(file.prefix,'eta_pairsplot.png')), 
       plot = p, 
       h = 10, 
       w = 10
       )
```

```{r ordered_logistic_latent_factor-b_latent_eta_plot, include=FALSE, eval=FALSE, echo=FALSE, tidy=FALSE, fig.align='center', out.width='80%'}
knitr::include_graphics( file.path(dir.out, paste0(file.prefix,'eta_pairsplot.png')) )
```

```{r ordered_logistic_latent_factor-b_extract_class_probs_by_n, include=FALSE, eval=TRUE, echo=FALSE, tidy=FALSE, cache=TRUE}
po <- ordered_categorical_logit_fit_m5b$draws(
  variables = c('ordered_prob_by_obs'),
  inc_warmup = FALSE,
  format = "draws_df"
  )
po <- as.data.table(po)
po <- data.table::melt(po, id.vars = c('.draw','.chain','.iteration'), value.name = 'prob')
po[, y_stan := as.integer(gsub('ordered_prob_by_obs\\[([0-9]),([0-9]+)\\]','\\1',variable)) ]
po[, oid := as.integer(gsub('ordered_prob_by_obs\\[([0-9]),([0-9]+)\\]','\\2',variable)) ]
set(po, NULL, 'variable', NULL)
tmp <- unique(subset(dcat1, select = c(oid, pid, item_id, time )))
po <- merge(po, tmp, by = c('oid'))
po <- po[, list(prob = mean(prob)), by = c('.draw','time','y_stan')]

pos <- po[, 
          list( summary_value = quantile(prob, prob = c(0.025, 0.25, 0.5, 0.75, 0.975)),
                summary_name = c('q_lower','iqr_lower','median','iqr_upper','q_upper') 
                ), 
          by = c('time','y_stan')
          ]
pos <- data.table::dcast(pos, time + y_stan ~ summary_name, value.var = 'summary_value')

tmp <- unique( subset(dcat1, select = c(time, time_label, y_stan, y, y_label)) )
pos <- merge(pos, tmp, by = c('time','y_stan'))

tmp <- dcat1[, list(n = length(pid)), by = c('time_label','y_label','y_stan')]
tmp2 <- tmp[, list(total = sum(n)), by = c('time_label')]
tmp <- merge(tmp, tmp2, by = c('time_label'))
tmp[, p_emp := n/total]
pos <- merge(pos, subset(tmp, select = -c(y_stan, n, total)), by = c('time_label','y_label'))

p <- ggplot(data = pos, aes( x = y_label, fill = time_label)) +
  geom_bar(aes( y = p_emp), 
           stat = 'identity', 
           position = position_dodge()) +
  geom_point(aes( y = median),
             position = position_dodge(0.9), colour = 'black') +
  geom_boxplot( aes(ymin = q_lower, lower = iqr_lower, middle = median, upper = iqr_upper, ymax = q_upper), 
                position = position_dodge(0.9),
                stat = 'identity',
                alpha = 0,
                width = 0.3) +
  scale_fill_d3() +
  scale_y_continuous(labels = scales::percent) +
  labs(x = '\nCaregiver mental health outcome', y = 'group-level\nposterior probability', fill = '') +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 45,vjust = 1,hjust = 1),
        legend.position = 'bottom'
        )
ggsave(file = file.path(dir.out, paste0(file.prefix,'probs_barplot.png')), 
       plot = p, 
       h = 8, 
       w = 8
       )
```

```{r ordered_logistic_latent_factor-b_plot, include=FALSE, eval=FALSE, echo=FALSE, tidy=FALSE, fig.align='center', out.width='80%'}
knitr::include_graphics( file.path(dir.out, paste0(file.prefix,'probs_barplot.png')) )
```

```{r ordered_logistic_latent_factor-b_extract_class_probs_by_question, include=FALSE, eval=TRUE, echo=FALSE, tidy=FALSE, cache=TRUE}
po <- ordered_categorical_logit_fit_m5b$draws(
  variables = c('ordered_prob_by_obs'),
  inc_warmup = FALSE,
  format = "draws_df"
  )
po <- as.data.table(po)
po <- data.table::melt(po, id.vars = c('.draw','.chain','.iteration'), value.name = 'prob')
po[, y_stan := as.integer(gsub('ordered_prob_by_obs\\[([0-9]),([0-9]+)\\]','\\1',variable)) ]
po[, oid := as.integer(gsub('ordered_prob_by_obs\\[([0-9]),([0-9]+)\\]','\\2',variable)) ]
set(po, NULL, 'variable', NULL)
tmp <- unique(subset(dcat1, select = c(oid, pid, item_id, time )))
po <- merge(po, tmp, by = c('oid'))
po <- po[, list(prob = mean(prob)), by = c('.draw','time','item_id','y_stan')]

pos <- po[, 
          list( summary_value = quantile(prob, prob = c(0.025, 0.25, 0.5, 0.75, 0.975)),
                summary_name = c('q_lower','iqr_lower','median','iqr_upper','q_upper') 
                ), 
          by = c('time','item_id','y_stan')
          ]
pos <- data.table::dcast(pos, time + item_id + y_stan ~ summary_name, value.var = 'summary_value')

tmp <- unique( subset(dcat1, select = c(time, time_label, item_id, item_label, y, y_stan, y_label)) )
pos <- merge(pos, tmp, by = c('time','item_id','y_stan'))

tmp <- dcat1[, list(n = length(pid)), by = c('time_label','item_label','y_label','y')]
tmp2 <- tmp[, list(total = sum(n)), by = c('time_label','item_label')]
tmp <- merge(tmp, tmp2, by = c('time_label','item_label'))
tmp[, p_emp := n/total]
pos <- merge(pos, subset(tmp, select = -c(y, n, total)), by = c('time_label','item_label','y_label'))

p <- ggplot(pos, aes(x = y_label, group = interaction(item_label, y_label))) +
  geom_col(aes(fill = item_label, y = p_emp), 
           position = position_dodge(0.9)
           ) +
  geom_boxplot( aes(ymin = q_lower, lower = iqr_lower, middle = median, upper = iqr_upper, ymax = q_upper), 
                position = position_dodge(0.9),
                stat = 'identity',
                alpha = 0,
                width = 0.3) +
  scale_fill_futurama() +
  scale_y_continuous(labels = scales::percent) +
  facet_grid(~ time_label) +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 45,vjust = 1,hjust = 1),
        legend.position = 'bottom'
        ) +
  labs(x = '', x = 'Likert scale', y = 'proportion of outcomes', fill = '')
ggsave(file = file.path(dir.out, paste0(file.prefix,'probs_barplot_v2.png')), 
       plot = p, 
       h = 10, 
       w = 8
       )
```

```{r ordered_logistic_latent_factor-b_extract_class_probs_by_question_plot, include=TRUE, eval=TRUE, echo=FALSE, tidy=FALSE, fig.align='center', out.width='80%'}
knitr::include_graphics( file.path(dir.out, paste0(file.prefix,'probs_barplot_v2.png')) )
```

## Second model: add CG-VIO and CG-MONITOR-CHI with lots of for loops

Here is the Stan code:

```{r mlf1_stan_code, include=TRUE, eval=TRUE, echo=TRUE, tidy=FALSE, cache=TRUE}
mixed_lfm_logit_m1_txt <- "
data
{
  int<lower=1> Ncat; // number of ordered categorial observations
  int<lower=2> Qcat; // number of ordered categorial questions
  int<lower=3> K; // number of categories
  int<lower=1> Nbin; // number of binomial observations
  int<lower=2> Qbin; // number of binomial questions
  int<lower=1> NbinMax; // maximum number of success
  int<lower=2> P; // number of groups
  int<lower=1> U; // number of units
  array [Ncat] int<lower=1, upper=K> ycat; // observations
  array [Ncat] int<lower=1,upper=Qcat> question_of_cat_obs;
  array [Ncat] int<lower=1,upper=P> group_of_cat_obs;
  array [Ncat] int<lower=1,upper=U> unit_of_cat_obs;
  array [Nbin] int<lower=0, upper=NbinMax> ybin; // observations
  array [Nbin] int<lower=1,upper=Qbin> question_of_bin_obs;
  array [Nbin] int<lower=1,upper=P> group_of_bin_obs;
  array [Nbin] int<lower=1,upper=U> unit_of_bin_obs;
}
transformed data
{
    real s2z_sd_groups;
    real s2z_sd_unit;
    real s2z_sd_cat_questions;
    real s2z_sd_bin_questions;
    s2z_sd_groups = inv(sqrt(1. - inv(P)));
    s2z_sd_unit = inv(sqrt(1. - inv(U)));
    s2z_sd_cat_questions = inv(sqrt(1. - inv(Qcat)));
    s2z_sd_bin_questions = inv(sqrt(1. - inv(Qbin)));
}
parameters
{
  sum_to_zero_vector[P] latent_factor_group;
  sum_to_zero_vector[U] latent_factor_unit;
  real<lower=0> latent_factor_unit_sd;
  
  array [P] sum_to_zero_vector[Qcat] beta_cat_questions;
  array [P] vector<lower=0>[Qcat-1] loadings_cat_questions_m1;
  real cut_point_1;
  matrix<lower=0>[K - 2,P] cut_point_gaps_groups;
  
  array [P] real beta_bin_baseline;
  array [P] sum_to_zero_vector[Qbin] beta_bin_questions;
  array [P] vector<lower=0>[Qbin-1] loadings_bin_questions_m1;
}
transformed parameters
{
  array [P] ordered[K-1] cut_points;
  array [P] vector<lower=0>[Qcat] loadings_cat_questions;
  array [P] vector<lower=0>[Qbin] loadings_bin_questions;
  
  for (p in 1:P)
  {
    cut_points[p] = 
      append_row(
        rep_vector(cut_point_1, 1), 
        rep_vector(cut_point_1, K-2) + cumulative_sum(cut_point_gaps_groups[,p])
        );
        
    loadings_cat_questions[p] = append_row(1.0, loadings_cat_questions_m1[p]);
    loadings_bin_questions[p] = append_row(1.0, loadings_bin_questions_m1[p]);
  }
}
model
{
  // ordered cat likelihood, cannot be vectorized, for cat obs
  for (n in 1:Ncat) 
  {
    ycat[n] ~ ordered_logistic(
      beta_cat_questions[group_of_cat_obs[n]][question_of_cat_obs[n]] + 
        loadings_cat_questions[group_of_cat_obs[n]][question_of_cat_obs[n]] *
        (
          latent_factor_group[group_of_cat_obs[n]] + 
          latent_factor_unit_sd * latent_factor_unit[unit_of_cat_obs[n]]
        ), 
      cut_points[group_of_cat_obs[n]]);
  }
  
  // binomial likelihood for binomial obs
  {
    vector[Nbin] eta;
    for (n in 1:Nbin) 
    {
      eta[n] =
        beta_bin_baseline[group_of_bin_obs[n]] +
          beta_bin_questions[group_of_bin_obs[n]][question_of_bin_obs[n]] + 
          loadings_bin_questions[group_of_bin_obs[n]][question_of_bin_obs[n]] *
          (
            latent_factor_group[group_of_bin_obs[n]] + 
            latent_factor_unit_sd * latent_factor_unit[unit_of_bin_obs[n]]
          );
    }
    ybin ~ binomial_logit(NbinMax, eta);
  }

  // priors for latent factors
  latent_factor_group ~ normal(0, s2z_sd_groups); 
  latent_factor_unit ~ normal(0, s2z_sd_unit); 
  latent_factor_unit_sd ~ exponential(2); 
  
  // priors for cat loadings
  cut_point_1 ~ normal(0, 3.5); 
  to_vector(cut_point_gaps_groups) ~ normal(0, 2.5); 
  for (p in 1:P)
  {
    beta_cat_questions[p] ~ normal(0, s2z_sd_cat_questions); 
    loadings_cat_questions_m1[p] ~ lognormal(0,1);
  }
  
  // priors for bin loadings
  beta_bin_baseline ~ normal(0, 3.5);
  for (p in 1:P)
  {
    beta_bin_questions[p] ~ normal(0, s2z_sd_bin_questions); 
    loadings_bin_questions_m1[p] ~ lognormal(0,1);
  }
}
generated quantities
{
  vector[U] latent_factor_unit_re;
  array [P,Qcat+Qbin] real mean_eta;
  array [K,Ncat] real<lower=0, upper=1> cat_ordered_prob_by_obs;
  vector[Nbin] bin_prob_by_obs;
  array [Ncat] int<lower=0> cat_ypred;
  array [Nbin] int<lower=0> bin_ypred;
  array [Ncat+Nbin] real log_lik;
  
  latent_factor_unit_re = latent_factor_unit_sd * latent_factor_unit;
  
  for (p in 1:P) 
  {
    for (q in 1:Qcat) 
    {
      mean_eta[p,q] = beta_cat_questions[p][q] + loadings_cat_questions[p][q] * latent_factor_group[p];
    }
    for (q in 1:Qbin) 
    {
      mean_eta[p,Qcat+q] = beta_bin_questions[p][q] + loadings_bin_questions[p][q] * latent_factor_group[p];
    }
  }
  
  {
    real tmp_real;
    for (n in 1:Ncat) 
    {
        tmp_real = beta_cat_questions[group_of_cat_obs[n]][question_of_cat_obs[n]] + 
                    loadings_cat_questions[group_of_cat_obs[n]][question_of_cat_obs[n]] *
                    (
                      latent_factor_group[group_of_cat_obs[n]] + 
                      latent_factor_unit_sd * latent_factor_unit[unit_of_cat_obs[n]]
                    );
        cat_ordered_prob_by_obs[1,n]       = 1 - inv_logit( tmp_real - cut_points[group_of_cat_obs[n]][1] );
        cat_ordered_prob_by_obs[2:(K-1),n] = to_array_1d( inv_logit( tmp_real - cut_points[group_of_cat_obs[n]][1:(K-2)] ) 
                                                          - 
                                                          inv_logit( tmp_real - cut_points[group_of_cat_obs[n]][2:(K-1)] )
                                                          );
        cat_ordered_prob_by_obs[K,n]       = inv_logit( tmp_real - cut_points[group_of_cat_obs[n]][K-1] );
    }
  }
  
  for( n in 1:Ncat ) 
  {
    cat_ypred[n] = ordered_logistic_rng( 
                beta_cat_questions[group_of_cat_obs[n]][question_of_cat_obs[n]] + 
                  loadings_cat_questions[group_of_cat_obs[n]][question_of_cat_obs[n]] *
                  (
                    latent_factor_group[group_of_cat_obs[n]] + 
                    latent_factor_unit_sd * latent_factor_unit[unit_of_cat_obs[n]]
                  ), 
                cut_points[group_of_cat_obs[n]]
                ); 
  }
  
  {
    vector[Nbin] eta;
    for (n in 1:Nbin) 
    {
      eta[n] =
        beta_bin_baseline[group_of_bin_obs[n]] +
          beta_bin_questions[group_of_bin_obs[n]][question_of_bin_obs[n]] + 
          loadings_bin_questions[group_of_bin_obs[n]][question_of_bin_obs[n]] *
          (
            latent_factor_group[group_of_bin_obs[n]] + 
            latent_factor_unit_sd * latent_factor_unit[unit_of_bin_obs[n]]
          );
    }
    bin_prob_by_obs = inv_logit(eta);
    bin_ypred = binomial_rng(NbinMax, inv_logit(eta));
  }
  
  for(n in 1:Ncat)
  {
    log_lik[n] =  ordered_logistic_lpmf( 
                    ycat[n] 
                    |
                    beta_cat_questions[group_of_cat_obs[n]][question_of_cat_obs[n]] + 
                      loadings_cat_questions[group_of_cat_obs[n]][question_of_cat_obs[n]] *
                      (
                        latent_factor_group[group_of_cat_obs[n]] + 
                        latent_factor_unit_sd * latent_factor_unit[unit_of_cat_obs[n]]
                      ),
                    cut_points[group_of_cat_obs[n]]);
  }
  
  for(n in 1:Nbin)
  {
    log_lik[Ncat+n] = binomial_logit_lpmf( 
                        ybin[n] 
                        |
                        NbinMax,
                        beta_bin_baseline[group_of_bin_obs[n]] +
                          beta_bin_questions[group_of_bin_obs[n]][question_of_bin_obs[n]] + 
                          loadings_bin_questions[group_of_bin_obs[n]][question_of_bin_obs[n]] *
                          (
                            latent_factor_group[group_of_bin_obs[n]] + 
                            latent_factor_unit_sd * latent_factor_unit[unit_of_bin_obs[n]]
                          )
                        );
  }
}
"

# compile the model
mixed_lfm_logit_m1_filename <- cmdstanr::write_stan_file(
  gsub('\t',' ', mixed_lfm_logit_m1_txt),
  dir = dir.out,
  basename = NULL,
  force_overwrite = FALSE,
  hash_salt = ""
)

# compile Stan model
mixed_lfm_logit_m1_compiled <- cmdstanr::cmdstan_model(mixed_lfm_logit_m1_filename)
```

Let's fit the model!

```{r mlf1_prepare data, include=TRUE, eval=TRUE, echo=TRUE, message=FALSE, tidy=FALSE, cache=TRUE}
file.prefix <- 'cgmh_vio_monitor_mlf_1_'
dcat2 <- subset(dp1, grepl('CG-MH|CG-VIO|CG-MONITOR-CHI',item_label))
dcat2[, time := time + 1L]
tmp <- dcat2[, which(grepl('CG-MH',item_label))]
dcat2[, y_stan := y]
set(dcat2, tmp, 'y_stan', dcat2[tmp, y + 1L])
tmp <- data.table(item_label = sort(unique(dcat2$item_label)))
tmp[, item_type := ifelse(grepl('CG-MH',item_label), 'categorical', 'binomial')]
tmp <- tmp[, list(item_label = item_label, item_id = 1:length(item_label)), by = 'item_type']
dcat2 <- merge(dcat2, tmp, by = 'item_label')
setkey(dcat2, pid, time, item_label)
dcat2[, oid := 1:nrow(dcat2)]
tmp <- dcat2[, list(oid = oid, oidt = 1:length(y_stan)), by = 'item_type' ]
dcat2 <- merge(dcat2, tmp, by  = c('item_type','oid'))
setkey(dcat2, pid, time, item_label)
```

```{r mlf1_run_model, include=TRUE, eval=TRUE, echo=TRUE, message=FALSE, tidy=FALSE, cache=TRUE}
# define data in format needed for model specification
stan_data <- list()
stan_data$P <- max(dcat2$time)
stan_data$U <- max(dcat2$pid)
stan_data$Ncat <- nrow(dcat2[item_type == 'categorical',])
stan_data$Qcat <- max(dcat2[item_type == 'categorical', item_id])
stan_data$K <- length(unique(dcat2[item_type == 'categorical', y_stan]))
stan_data$ycat <- dcat2[item_type == 'categorical', y_stan]
stan_data$group_of_cat_obs <- dcat2[item_type == 'categorical', time]
stan_data$question_of_cat_obs <- dcat2[item_type == 'categorical', item_id]
stan_data$unit_of_cat_obs <- dcat2[item_type == 'categorical', pid]
stan_data$Nbin <- nrow(dcat2[item_type == 'binomial',])
stan_data$Qbin <- max(dcat2[item_type == 'binomial', item_id])
stan_data$NbinMax <- 7L
stan_data$ybin <- dcat2[item_type == 'binomial', y_stan]
stan_data$group_of_bin_obs <- dcat2[item_type == 'binomial', time]
stan_data$question_of_bin_obs <- dcat2[item_type == 'binomial', item_id]
stan_data$unit_of_bin_obs <- dcat2[item_type == 'binomial', pid]

stan_data <- list()
stan_data$P <- max(dcat2$time)
stan_data$U <- max(dcat2$pid)
stan_data$Ncat <- nrow(dcat2[item_type == 'categorical',])
stan_data$Qcat <- max(dcat2[item_type == 'categorical', item_id])
stan_data$Kcat <- length(unique(dcat2[item_type == 'categorical', y_stan]))
stan_data$ycat <- dcat2[item_type == 'categorical', y_stan]
stan_data$group_of_cat_obs <- dcat2[item_type == 'categorical', time]
stan_data$question_of_cat_obs <- dcat2[item_type == 'categorical', item_id]
stan_data$unit_of_cat_obs <- dcat2[item_type == 'categorical', pid]
tmp <- dcat2[item_type == 'categorical', list(n = length(oidt)),  by = 'time']
stan_data$NcatP_start_idx <- c(1, 1 + cumsum(tmp$n))
tmp <- dcat2[item_type == 'categorical', list(oidt = oidt),  by = 'time']
stan_data$cat_obs_of_group <- tmp$oidt
stan_data$Nbin <- nrow(dcat2[item_type == 'binomial',])
stan_data$Qbin <- max(dcat2[item_type == 'binomial', item_id])
stan_data$NbinMax <- 7L
stan_data$ybin <- dcat2[item_type == 'binomial', y_stan]
stan_data$question_of_bin_obs <- dcat2[item_type == 'binomial', item_id]
stan_data$unit_of_bin_obs <- dcat2[item_type == 'binomial', pid]
tmp <- dcat2[item_type == 'binomial', list(n = length(oidt)),  by = 'time']
stan_data$NbinP_start_idx <- c(1, 1 + cumsum(tmp$n))
tmp <- dcat2[item_type == 'binomial', list(oidt = oidt),  by = 'time']
stan_data$bin_obs_of_group <- tmp$oidt

# sample
mixed_lfm_logit_m1_fit <- mixed_lfm_logit_m1_compiled$sample(
  data = stan_data,
  seed = 123,
  chains = 2,
  parallel_chains = 2,
  iter_warmup = 5e2,
  iter_sampling = 15e2,
  refresh = 500,
  save_warmup = TRUE
)

# save output to RDS
mixed_lfm_logit_m1_fit$save_object(file = file.path(dir.out, paste0(file.prefix,"stan.rds")))
```

## Second model: neat but slow

Here is the Stan code:

```{r mlf1b_stan_code, include=TRUE, eval=TRUE, echo=TRUE, tidy=FALSE, cache=TRUE}
mixed_lfm_logit_m1_txt <- "
functions
{
  vector lf_get_eta( real beta_baseline,
                     vector beta_questions, 
                     vector loadings_questions,
                     array [] int question_of_obs,
                     array [] int unit_of_obs,
                     real latent_factor_baseline,
                     real latent_factor_unit_sd,
                     vector latent_factor_unit
                   )
  {
    int N = num_elements(unit_of_obs);
    vector [N] eta;
    eta = latent_factor_baseline + latent_factor_unit_sd * latent_factor_unit[unit_of_obs];
    eta = loadings_questions[question_of_obs] .* eta;
    eta = beta_baseline + beta_questions[question_of_obs] + eta;
    return eta;
  }
}
data
{
  int<lower=1> Ncat; // number of ordered categorial observations
  int<lower=2> Qcat; // number of ordered categorial questions
  int<lower=3> Kcat; // number of categories
  int<lower=1> Nbin; // number of binomial observations
  int<lower=2> Qbin; // number of binomial questions
  int<lower=1> NbinMax; // maximum number of success
  int<lower=2> P; // number of groups
  int<lower=1> U; // number of units
  
  array [Ncat] int<lower=1, upper=Kcat> ycat; // observations
  array [Ncat] int<lower=1,upper=Qcat> question_of_cat_obs;
  array [Ncat] int<lower=1,upper=U> unit_of_cat_obs;
  array [Ncat] int<lower=1,upper=P> group_of_cat_obs;
  array [P+1] int<lower=1, upper=Ncat+1> NcatP_start_idx;
  array [Ncat] int<lower=1,upper=Ncat> cat_obs_of_group;
  
  array [Nbin] int<lower=0, upper=NbinMax> ybin; // observations
  array [Nbin] int<lower=1,upper=U> unit_of_bin_obs;
  array [Nbin] int<lower=1,upper=Qbin> question_of_bin_obs;
  array [P+1] int<lower=1, upper=Nbin+1> NbinP_start_idx;
  array [Nbin] int<lower=1,upper=Nbin> bin_obs_of_group;
}
transformed data
{
    real s2z_sd_groups;
    real s2z_sd_unit;
    real s2z_sd_cat_questions;
    real s2z_sd_bin_questions;
    s2z_sd_groups = inv(sqrt(1. - inv(P)));
    s2z_sd_unit = inv(sqrt(1. - inv(U)));
    s2z_sd_cat_questions = inv(sqrt(1. - inv(Qcat)));
    s2z_sd_bin_questions = inv(sqrt(1. - inv(Qbin)));
}
parameters
{
  sum_to_zero_vector[P] latent_factor_group;
  sum_to_zero_vector[U] latent_factor_unit;
  real<lower=0> latent_factor_unit_sd;
  
  array [P] sum_to_zero_vector[Qcat] beta_cat_questions;
  array [P] vector<lower=0>[Qcat-1] loadings_cat_questions_m1;
  real cut_point_1;
  matrix<lower=0>[Kcat - 2,P] cut_point_gaps_groups;
  
  array [P] real beta_bin_baseline;
  array [P] sum_to_zero_vector[Qbin] beta_bin_questions;
  array [P] vector<lower=0>[Qbin-1] loadings_bin_questions_m1;
}
transformed parameters
{
  array [P] ordered[Kcat-1] cut_points;
  array [P] vector<lower=0>[Qcat] loadings_cat_questions;
  array [P] vector<lower=0>[Qbin] loadings_bin_questions;
  vector [Ncat] cat_eta;
  vector [Nbin] bin_eta;
  
  for (p in 1:P)
  {
    cut_points[p] = 
      append_row(
        rep_vector(cut_point_1, 1), 
        rep_vector(cut_point_1, Kcat-2) + cumulative_sum(cut_point_gaps_groups[,p])
        );
        
    loadings_cat_questions[p] = append_row(1.0, loadings_cat_questions_m1[p]);
    loadings_bin_questions[p] = append_row(1.0, loadings_bin_questions_m1[p]);
    
    cat_eta[ cat_obs_of_group[ NcatP_start_idx[p]:(NcatP_start_idx[p+1]-1) ] ]  
        = 
        lf_get_eta( 0.,
                    beta_cat_questions[p], 
                    loadings_cat_questions[p],
                    question_of_cat_obs,
                    unit_of_cat_obs,
                    latent_factor_group[p],
                    latent_factor_unit_sd,
                    latent_factor_unit
                    )
                    [ cat_obs_of_group[ NcatP_start_idx[p]:(NcatP_start_idx[p+1]-1) ] ];
     
                    
    bin_eta[ bin_obs_of_group[ NbinP_start_idx[p]:(NbinP_start_idx[p+1]-1) ] ] 
        =
        lf_get_eta( beta_bin_baseline[p],
                    beta_bin_questions[p], 
                    loadings_bin_questions[p],
                    question_of_bin_obs,
                    unit_of_bin_obs,
                    latent_factor_group[p],
                    latent_factor_unit_sd,
                    latent_factor_unit
                    )
                    [ bin_obs_of_group[ NbinP_start_idx[p]:(NbinP_start_idx[p+1]-1) ] ];
  }
}
model
{
  // ordered cat likelihood, cannot be vectorized, for cat obs
  for (n in 1:Ncat) 
  {
    ycat[n] ~ ordered_logistic(cat_eta[n], cut_points[group_of_cat_obs[n]]);
  }
  
  // binomial likelihood for binomial obs
  ybin ~ binomial_logit(NbinMax, bin_eta);

  // priors for latent factors
  latent_factor_group ~ normal(0, s2z_sd_groups); 
  latent_factor_unit ~ normal(0, s2z_sd_unit); 
  latent_factor_unit_sd ~ exponential(2); 
  
  // priors for cat loadings
  cut_point_1 ~ normal(0, 3.5); 
  to_vector(cut_point_gaps_groups) ~ normal(0, 2.5); 
  for (p in 1:P)
  {
    beta_cat_questions[p] ~ normal(0, s2z_sd_cat_questions); 
    loadings_cat_questions_m1[p] ~ lognormal(0,1);
  }
  
  // priors for bin loadings
  beta_bin_baseline ~ normal(0, 3.5);
  for (p in 1:P)
  {
    beta_bin_questions[p] ~ normal(0, s2z_sd_bin_questions); 
    loadings_bin_questions_m1[p] ~ lognormal(0,1);
  }
}
generated quantities
{
  vector[U] latent_factor_unit_re;
  array [P,Qcat+Qbin] real mean_eta;
  array [Kcat,Ncat] real<lower=0, upper=1> cat_ordered_prob_by_obs;
  vector[Nbin] bin_prob_by_obs;
  array [Ncat] int<lower=0> cat_ypred;
  array [Nbin] int<lower=0> bin_ypred;
  array [Ncat+Nbin] real log_lik;
  
  latent_factor_unit_re = latent_factor_unit_sd * latent_factor_unit;
  
  for (p in 1:P) 
  {
    for (q in 1:Qcat) 
    {
      mean_eta[p,q] = beta_cat_questions[p][q] + loadings_cat_questions[p][q] * latent_factor_group[p];
    }
    for (q in 1:Qbin) 
    {
      mean_eta[p,Qcat+q] = beta_bin_questions[p][q] + loadings_bin_questions[p][q] * latent_factor_group[p];
    }
  }
  
  for (n in 1:Ncat) 
  {
    cat_ordered_prob_by_obs[1,n]          = 1 - inv_logit( cat_eta[n] - cut_points[group_of_cat_obs[n]][1] );
    cat_ordered_prob_by_obs[2:(Kcat-1),n] = to_array_1d( inv_logit( cat_eta[n] - cut_points[group_of_cat_obs[n]][1:(Kcat-2)] ) 
                                                         - 
                                                         inv_logit( cat_eta[n] - cut_points[group_of_cat_obs[n]][2:(Kcat-1)] )
                                                         );
    cat_ordered_prob_by_obs[Kcat,n]       = inv_logit( cat_eta[n] - cut_points[group_of_cat_obs[n]][Kcat-1] );
  }
  
  bin_prob_by_obs = inv_logit(bin_eta);
  
  for( n in 1:Ncat ) 
  {
    cat_ypred[n] = ordered_logistic_rng( cat_eta[n], cut_points[group_of_cat_obs[n]] ); 
  }
  
  bin_ypred = binomial_rng(NbinMax, inv_logit(bin_eta));
  
  for(n in 1:Ncat)
  {
    log_lik[n] =  ordered_logistic_lpmf( ycat[n] | cat_eta[n], cut_points[group_of_cat_obs[n]] );
  }
  for(n in 1:Nbin)
  {
    log_lik[ Ncat+n ] = binomial_logit_lpmf( ybin[n] | NbinMax, bin_eta[n] );
  }
}
"
# compile the model
mixed_lfm_logit_m1_filename <- cmdstanr::write_stan_file(
  gsub('\t',' ', mixed_lfm_logit_m1_txt),
  dir = dir.out,
  basename = NULL,
  force_overwrite = FALSE,
  hash_salt = ""
)

# compile Stan model
mixed_lfm_logit_m1_compiled <- cmdstanr::cmdstan_model(mixed_lfm_logit_m1_filename)
```

```{r mlf1b_prepare data, include=TRUE, eval=TRUE, echo=TRUE, message=FALSE, tidy=FALSE, cache=TRUE}
file.prefix <- 'cgmh_vio_monitor_mlf_1_'
dcat2 <- subset(dp1, grepl('CG-MH|CG-VIO|CG-MONITOR-CHI',item_label))
dcat2[, time := time + 1L]
tmp <- dcat2[, which(grepl('CG-MH',item_label))]
dcat2[, y_stan := y]
set(dcat2, tmp, 'y_stan', dcat2[tmp, y + 1L])
tmp <- data.table(item_label = sort(unique(dcat2$item_label)))
tmp[, item_type := ifelse(grepl('CG-MH',item_label), 'categorical', 'binomial')]
tmp <- tmp[, list(item_label = item_label, item_id = 1:length(item_label)), by = 'item_type']
dcat2 <- merge(dcat2, tmp, by = 'item_label')
setkey(dcat2, pid, time, item_label)
dcat2[, oid := 1:nrow(dcat2)]
tmp <- dcat2[, list(oid = oid, oidt = 1:length(y_stan)), by = 'item_type' ]
dcat2 <- merge(dcat2, tmp, by  = c('item_type','oid'))
setkey(dcat2, pid, time, item_label)
```

```{r mlf1b_run_model, include=TRUE, eval=TRUE, echo=TRUE, message=FALSE, tidy=FALSE, cache=TRUE}
stan_data <- list()
stan_data$P <- max(dcat2$time)
stan_data$U <- max(dcat2$pid)
stan_data$Ncat <- nrow(dcat2[item_type == 'categorical',])
stan_data$Qcat <- max(dcat2[item_type == 'categorical', item_id])
stan_data$Kcat <- length(unique(dcat2[item_type == 'categorical', y_stan]))
stan_data$ycat <- dcat2[item_type == 'categorical', y_stan]
stan_data$group_of_cat_obs <- dcat2[item_type == 'categorical', time]
stan_data$question_of_cat_obs <- dcat2[item_type == 'categorical', item_id]
stan_data$unit_of_cat_obs <- dcat2[item_type == 'categorical', pid]
tmp <- dcat2[item_type == 'categorical', list(n = length(oidt)),  by = 'time']
stan_data$NcatP_start_idx <- c(1, 1 + cumsum(tmp$n))
tmp <- dcat2[item_type == 'categorical', list(oidt = oidt),  by = 'time']
stan_data$cat_obs_of_group <- tmp$oidt
stan_data$Nbin <- nrow(dcat2[item_type == 'binomial',])
stan_data$Qbin <- max(dcat2[item_type == 'binomial', item_id])
stan_data$NbinMax <- 7L
stan_data$ybin <- dcat2[item_type == 'binomial', y_stan]
stan_data$question_of_bin_obs <- dcat2[item_type == 'binomial', item_id]
stan_data$unit_of_bin_obs <- dcat2[item_type == 'binomial', pid]
tmp <- dcat2[item_type == 'binomial', list(n = length(oidt)),  by = 'time']
stan_data$NbinP_start_idx <- c(1, 1 + cumsum(tmp$n))
tmp <- dcat2[item_type == 'binomial', list(oidt = oidt),  by = 'time']
stan_data$bin_obs_of_group <- tmp$oidt

# sample
mixed_lfm_logit_m1_fit <- mixed_lfm_logit_m1_compiled$sample(
  data = stan_data,
  seed = 123,
  chains = 2,
  parallel_chains = 2,
  iter_warmup = 5e2,
  iter_sampling = 15e2,
  refresh = 500,
  save_warmup = TRUE
)

# save output to RDS
mixed_lfm_logit_m1_fit$save_object(file = file.path(dir.out, paste0(file.prefix,"stan.rds")))
```


## Second model: neat and similar speed

Here is the Stan code:

```{r mlf1c_stan_code, include=TRUE, eval=TRUE, echo=TRUE, tidy=FALSE, cache=TRUE}
mixed_lfm_logit_m1_txt <- "
functions
{
  vector lf_get_eta( array [] real beta_baseline,
                     array [] vector beta_questions, 
                     array [] vector loadings_questions,
                     array [] int group_of_obs,
                     array [] int question_of_obs,
                     array [] int unit_of_obs,
                     vector latent_factor_baseline,
                     real latent_factor_unit_sd,
                     vector latent_factor_unit
                   )
  {
    int N = num_elements(unit_of_obs);
    vector [N] eta;
    eta = latent_factor_unit_sd * latent_factor_unit[unit_of_obs];
    for(n in 1:N)
    {
      eta[n] 
        = 
        beta_baseline[group_of_obs[n]] +
        beta_questions[group_of_obs[n]][question_of_obs[n]] + 
        loadings_questions[group_of_obs[n]][question_of_obs[n]] * 
        (
          latent_factor_baseline[group_of_obs[n]] + eta[n]
        );
    }
    return eta;
  }
}
data
{
  int<lower=1> Ncat; // number of ordered categorial observations
  int<lower=2> Qcat; // number of ordered categorial questions
  int<lower=3> Kcat; // number of categories
  int<lower=1> Nbin; // number of binomial observations
  int<lower=2> Qbin; // number of binomial questions
  int<lower=1> NbinMax; // maximum number of success
  int<lower=2> P; // number of groups
  int<lower=1> U; // number of units
  
  array [Ncat] int<lower=1, upper=Kcat> ycat; // observations
  array [Ncat] int<lower=1,upper=U> unit_of_cat_obs;
  array [Ncat] int<lower=1,upper=P> group_of_cat_obs;
  array [Ncat] int<lower=1,upper=Qcat> question_of_cat_obs;
  
  array [Nbin] int<lower=0, upper=NbinMax> ybin; // observations
  array [Nbin] int<lower=1,upper=U> unit_of_bin_obs;
  array [Nbin] int<lower=1,upper=P> group_of_bin_obs;
  array [Nbin] int<lower=1,upper=Qbin> question_of_bin_obs;
}
transformed data
{
    real s2z_sd_groups;
    real s2z_sd_unit;
    real s2z_sd_cat_questions;
    real s2z_sd_bin_questions;
    array [P] real beta_cat_baseline;
    
    s2z_sd_groups = inv(sqrt(1. - inv(P)));
    s2z_sd_unit = inv(sqrt(1. - inv(U)));
    s2z_sd_cat_questions = inv(sqrt(1. - inv(Qcat)));
    s2z_sd_bin_questions = inv(sqrt(1. - inv(Qbin)));
    beta_cat_baseline = rep_array(0., P);
}
parameters
{
  sum_to_zero_vector[P] latent_factor_group;
  sum_to_zero_vector[U] latent_factor_unit;
  real<lower=0> latent_factor_unit_sd;
  
  array [P] sum_to_zero_vector[Qcat] beta_cat_questions;
  array [P] vector<lower=0>[Qcat-1] loadings_cat_questions_m1;
  real cut_point_1;
  matrix<lower=0>[Kcat - 2,P] cut_point_gaps_groups;
  
  array [P] real beta_bin_baseline;
  array [P] sum_to_zero_vector[Qbin] beta_bin_questions;
  array [P] vector<lower=0>[Qbin-1] loadings_bin_questions_m1;
}
transformed parameters
{
  array [P] ordered[Kcat-1] cut_points;
  array [P] vector<lower=0>[Qcat] loadings_cat_questions;
  array [P] vector<lower=0>[Qbin] loadings_bin_questions;
  vector [Ncat] cat_eta;
  vector [Nbin] bin_eta;
  
  for (p in 1:P)
  {
    cut_points[p] = 
      append_row(
        rep_vector(cut_point_1, 1), 
        rep_vector(cut_point_1, Kcat-2) + cumulative_sum(cut_point_gaps_groups[,p])
        );
        
    loadings_cat_questions[p] = append_row(1.0, loadings_cat_questions_m1[p]);
    loadings_bin_questions[p] = append_row(1.0, loadings_bin_questions_m1[p]);
  }
  
  cat_eta
        = 
        lf_get_eta( beta_cat_baseline,
                    beta_cat_questions, 
                    loadings_cat_questions,
                    group_of_cat_obs,
                    question_of_cat_obs,
                    unit_of_cat_obs,
                    latent_factor_group,
                    latent_factor_unit_sd,
                    latent_factor_unit
                    );

  bin_eta
        =
        lf_get_eta( beta_bin_baseline,
                    beta_bin_questions, 
                    loadings_bin_questions,
                    group_of_bin_obs,
                    question_of_bin_obs,
                    unit_of_bin_obs,
                    latent_factor_group,
                    latent_factor_unit_sd,
                    latent_factor_unit
                    );
  
}
model
{
  // ordered cat likelihood, cannot be vectorized, for cat obs
  for (n in 1:Ncat) 
  {
    ycat[n] ~ ordered_logistic(cat_eta[n], cut_points[group_of_cat_obs[n]]);
  }
  
  // binomial likelihood for binomial obs
  ybin ~ binomial_logit(NbinMax, bin_eta);

  // priors for latent factors
  latent_factor_group ~ normal(0, s2z_sd_groups); 
  latent_factor_unit ~ normal(0, s2z_sd_unit); 
  latent_factor_unit_sd ~ exponential(2); 
  
  // priors for cat loadings
  cut_point_1 ~ normal(0, 3.5); 
  to_vector(cut_point_gaps_groups) ~ normal(0, 2.5); 
  for (p in 1:P)
  {
    beta_cat_questions[p] ~ normal(0, s2z_sd_cat_questions); 
    loadings_cat_questions_m1[p] ~ lognormal(0,1);
  }
  
  // priors for bin loadings
  beta_bin_baseline ~ normal(0, 3.5);
  for (p in 1:P)
  {
    beta_bin_questions[p] ~ normal(0, s2z_sd_bin_questions); 
    loadings_bin_questions_m1[p] ~ lognormal(0,1);
  }
}
generated quantities
{
  vector[U] latent_factor_unit_re;
  array [P,Qcat+Qbin] real mean_eta;
  array [Kcat,Ncat] real<lower=0, upper=1> cat_ordered_prob_by_obs;
  vector[Nbin] bin_prob_by_obs;
  array [Ncat] int<lower=0> cat_ypred;
  array [Nbin] int<lower=0> bin_ypred;
  array [Ncat+Nbin] real log_lik;
  
  latent_factor_unit_re = latent_factor_unit_sd * latent_factor_unit;
  
  for (p in 1:P) 
  {
    for (q in 1:Qcat) 
    {
      mean_eta[p,q] = beta_cat_questions[p][q] + loadings_cat_questions[p][q] * latent_factor_group[p];
    }
    for (q in 1:Qbin) 
    {
      mean_eta[p,Qcat+q] = beta_bin_questions[p][q] + loadings_bin_questions[p][q] * latent_factor_group[p];
    }
  }
  
  for (n in 1:Ncat) 
  {
    cat_ordered_prob_by_obs[1,n]          = 1 - inv_logit( cat_eta[n] - cut_points[group_of_cat_obs[n]][1] );
    cat_ordered_prob_by_obs[2:(Kcat-1),n] = to_array_1d( inv_logit( cat_eta[n] - cut_points[group_of_cat_obs[n]][1:(Kcat-2)] ) 
                                                         - 
                                                         inv_logit( cat_eta[n] - cut_points[group_of_cat_obs[n]][2:(Kcat-1)] )
                                                         );
    cat_ordered_prob_by_obs[Kcat,n]       = inv_logit( cat_eta[n] - cut_points[group_of_cat_obs[n]][Kcat-1] );
  }
  
  bin_prob_by_obs = inv_logit(bin_eta);
  
  for( n in 1:Ncat ) 
  {
    cat_ypred[n] = ordered_logistic_rng( cat_eta[n], cut_points[group_of_cat_obs[n]] ); 
  }
  
  bin_ypred = binomial_rng(NbinMax, inv_logit(bin_eta));
  
  for(n in 1:Ncat)
  {
    log_lik[n] =  ordered_logistic_lpmf( ycat[n] | cat_eta[n], cut_points[group_of_cat_obs[n]] );
  }
  for(n in 1:Nbin)
  {
    log_lik[ Ncat+n ] = binomial_logit_lpmf( ybin[n] | NbinMax, bin_eta[n] );
  }
}
"
# compile the model
mixed_lfm_logit_m1_filename <- cmdstanr::write_stan_file(
  gsub('\t',' ', mixed_lfm_logit_m1_txt),
  dir = dir.out,
  basename = NULL,
  force_overwrite = FALSE,
  hash_salt = ""
)

# compile Stan model
mixed_lfm_logit_m1_compiled <- cmdstanr::cmdstan_model(mixed_lfm_logit_m1_filename)
```

```{r mlf1c_prepare data, include=TRUE, eval=TRUE, echo=TRUE, message=FALSE, tidy=FALSE, cache=TRUE}
file.prefix <- 'cgmh_vio_monitor_mlf_1_'
dcat2 <- subset(dp1, grepl('CG-MH|CG-VIO|CG-MONITOR-CHI',item_label))
dcat2[, time := time + 1L]
tmp <- dcat2[, which(grepl('CG-MH',item_label))]
dcat2[, y_stan := y]
set(dcat2, tmp, 'y_stan', dcat2[tmp, y + 1L])
tmp <- data.table(item_label = sort(unique(dcat2$item_label)))
tmp[, item_type := ifelse(grepl('CG-MH',item_label), 'categorical', 'binomial')]
tmp <- tmp[, list(item_label = item_label, item_id = 1:length(item_label)), by = 'item_type']
dcat2 <- merge(dcat2, tmp, by = 'item_label')
setkey(dcat2, pid, time, item_label)
dcat2[, oid := 1:nrow(dcat2)]
tmp <- dcat2[, list(oid = oid, oidt = 1:length(y_stan)), by = 'item_type' ]
dcat2 <- merge(dcat2, tmp, by  = c('item_type','oid'))
setkey(dcat2, pid, time, item_label)

```

```{r mlf1c_run_model, include=TRUE, eval=TRUE, echo=TRUE, message=FALSE, tidy=FALSE, cache=TRUE}
stan_data <- list()
stan_data$P <- max(dcat2$time)
stan_data$U <- max(dcat2$pid)
stan_data$Ncat <- nrow(dcat2[item_type == 'categorical',])
stan_data$Qcat <- max(dcat2[item_type == 'categorical', item_id])
stan_data$Kcat <- length(unique(dcat2[item_type == 'categorical', y_stan]))
stan_data$ycat <- dcat2[item_type == 'categorical', y_stan]
stan_data$group_of_cat_obs <- dcat2[item_type == 'categorical', time]
stan_data$question_of_cat_obs <- dcat2[item_type == 'categorical', item_id]
stan_data$unit_of_cat_obs <- dcat2[item_type == 'categorical', pid]
stan_data$Nbin <- nrow(dcat2[item_type == 'binomial',])
stan_data$Qbin <- max(dcat2[item_type == 'binomial', item_id])
stan_data$NbinMax <- 7L
stan_data$ybin <- dcat2[item_type == 'binomial', y_stan]
stan_data$group_of_bin_obs <- dcat2[item_type == 'binomial', time]
stan_data$question_of_bin_obs <- dcat2[item_type == 'binomial', item_id]
stan_data$unit_of_bin_obs <- dcat2[item_type == 'binomial', pid]

# sample
mixed_lfm_logit_m1_fit <- mixed_lfm_logit_m1_compiled$sample(
  data = stan_data,
  seed = 123,
  chains = 2,
  parallel_chains = 2,
  iter_warmup = 5e2,
  iter_sampling = 15e2,
  refresh = 500,
  save_warmup = TRUE
)

# save output to RDS
mixed_lfm_logit_m1_fit$save_object(file = file.path(dir.out, paste0(file.prefix,"stan.rds")))
```



```{r mlf1c_load_model, include=FALSE, eval=TRUE, echo=FALSE, tidy=FALSE, cache=TRUE}
mixed_lfm_logit_m1_fit <- readRDS(file = file.path(dir.out, paste0(file.prefix,'stan.rds')))
```

```{r mlf1c_check_mixing_convergence, include=FALSE, eval=TRUE, echo=FALSE, tidy=FALSE, cache=TRUE}
#	check convergence and check mixing
#	get 95% credible intervals
tmp <- mixed_lfm_logit_m1_fit$summary(
  variables = c('latent_factor_group','latent_factor_unit','latent_factor_unit_sd',
                'beta_cat_questions','loadings_cat_questions_m1','cut_point_1','cut_point_gaps_groups',
                'beta_bin_baseline','beta_bin_questions','loadings_bin_questions_m1'
                ),
  posterior::default_summary_measures(),
  posterior::default_convergence_measures(),
  extra_quantiles = ~posterior::quantile2(., probs = c(.0275, .975))
)
tmp <- as.data.table(tmp)
tmp[order(ess_bulk),]
  
# parameter with lowest ess_bulk
worst_var <- tmp$variable[ which.min(tmp$ess_bulk) ]

# extract samples
po <- mixed_lfm_logit_m1_fit$draws(
  variables = c("lp__", worst_var),
  inc_warmup = TRUE,
  format = "draws_array"
  )

# make worst trace plot
p <- bayesplot:::mcmc_trace(po,  
                            pars = c("lp__",worst_var), 
                            n_warmup = 500,
                            facet_args = list(nrow = 2)
                            )
p <- p + theme_bw()
ggsave(file = file.path(dir.out, paste0(file.prefix,'worsttrace.png')), 
       plot = p, 
       h = 10, 
       w = 8
       )

# make intervals plot
po <- mixed_lfm_logit_m1_fit$draws(
  variables = c('latent_factor_group','latent_factor_unit','latent_factor_unit_sd',
                'beta_cat_questions','loadings_cat_questions_m1','cut_point_1','cut_point_gaps_groups',
                'beta_bin_baseline','beta_bin_questions','loadings_bin_questions_m1'
                ),
  inc_warmup = FALSE,
  format = "draws_array"
  )

color_scheme_set("teal")
p <- bayesplot::mcmc_intervals(po, prob = 0.5, prob_outer = 0.95, outer_size = 1, point_size = 2) +
  theme_bw()
ggsave(file = file.path(dir.out, paste0(file.prefix,'intervals.png')), 
       plot = p, 
       h = 50, 
       w = 8, 
       limitsize = FALSE
       )


# make posterior predictive check

# create median and 95\% credible intervals
po <- mixed_lfm_logit_m1_fit$draws(
  variables = c('cat_ypred','bin_ypred'),
  inc_warmup = FALSE,
  format = "draws_df"
  )
po <- as.data.table(po)
po <- data.table::melt(po, id.vars = c('.draw','.chain','.iteration'), value.name = 'ypred')
po[, item_type := gsub('([a-z_]+)\\[([0-9]+)\\]','\\1',variable) ]
po[, oidt := as.integer(gsub('([a-z_]+)\\[([0-9]+)\\]','\\2',variable)) ]
tmp <- po[, which(grepl('cat',item_type))]
set(po, tmp, 'item_type', 'categorical')
set(po, tmp, 'ypred', po[tmp, ypred - 1L])
set(po, po[, which(grepl('bin',item_type))], 'item_type', 'binomial')  

pos <- 
  po[,
     list( summary_value = quantile(ypred, prob = c(0.025, 0.25, 0.5, 0.75, 0.975)),
           summary_name = c('q_lower','iqr_lower','median','iqr_upper','q_upper') 
           ),
     by = c('item_type','oidt')
     ]
pos <- 
  data.table::dcast(pos,
                    item_type + oidt ~ summary_name, 
                    value.var = 'summary_value'
                    )

pos <- merge(pos, dcat2, by = c('item_type','oidt'))
pos[, IN_PPI := y >= q_lower & y <= q_upper]
pos[, mean(IN_PPI)]

# plot posterior predictive check
p <- ggplot(pos, aes(x = oid, group = oid)) + 
  geom_boxplot( aes( ymin = q_lower,
                     lower = iqr_lower,
                     middle = median,
                     upper = iqr_upper,
                     ymax = q_upper),
                stat = 'identity') +
  geom_point( aes(y = y_stan, colour = IN_PPI ) ) +
  facet_grid(item_label ~ time_label, scales = 'free') +
  scale_x_discrete() +
  scale_y_continuous() +
  ggsci::scale_color_npg() +
  labs(x = '', 
       y = 'outcome', 
       colour = 'within\n95% posterior\nprediction\ninterval') +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 45,vjust = 1,hjust = 1))
ggsave(file = file.path(dir.out, paste0(file.prefix,'ppcheck.png')), 
       p, 
       w = 20, 
       h = 20)
```

```{r mlf1c_extract_class_probs_by_question, include=FALSE, eval=TRUE, echo=FALSE, tidy=FALSE, cache=TRUE}
po <- mixed_lfm_logit_m1_fit$draws(
  variables = c('cat_ordered_prob_by_obs'),
  inc_warmup = FALSE,
  format = "draws_df"
  )
po <- as.data.table(po)
po <- data.table::melt(po, id.vars = c('.draw','.chain','.iteration'), value.name = 'prob')
po[, y_stan := as.integer(gsub('([a-z_]+)\\[([0-9]),([0-9]+)\\]','\\2',variable)) ]
po[, oidt := as.integer(gsub('([a-z_]+)\\[([0-9]),([0-9]+)\\]','\\3',variable)) ]
set(po, NULL, 'variable', NULL)
tmp <- unique(subset(dcat2[item_type == 'categorical',], select = c(oidt, pid, item_id, time )))
po <- merge(po, tmp, by = c('oidt'))
po <- po[, list(prob = mean(prob)), by = c('.draw','time','item_id','y_stan')]
pos <- po[, 
          list( summary_value = quantile(prob, prob = c(0.025, 0.25, 0.5, 0.75, 0.975)),
                summary_name = c('q_lower','iqr_lower','median','iqr_upper','q_upper') 
                ), 
          by = c('time','item_id','y_stan')
          ]
pos <- data.table::dcast(pos, time + item_id + y_stan ~ summary_name, value.var = 'summary_value')
tmp <- unique( subset(dcat2[item_type == 'categorical',], select = c(time, time_label, item_id, item_label, y, y_stan, y_label)) )
pos <- merge(pos, tmp, by = c('time','item_id','y_stan'))
tmp <- dcat2[item_type == 'categorical', list(n = length(pid)), by = c('time_label','item_label','y_label')]
tmp2 <- tmp[, list(total = sum(n)), by = c('time_label','item_label')]
tmp <- merge(tmp, tmp2, by = c('time_label','item_label'))
tmp[, p_emp := n/total]
pos <- merge(pos, subset(tmp, select = -c(n, total)), by = c('time_label','item_label','y_label'))
pos.cat <- copy(pos)
pos.cat[, y_stan := NULL]

po <- mixed_lfm_logit_m1_fit$draws(
  variables = c('bin_prob_by_obs'),
  inc_warmup = FALSE,
  format = "draws_df"
  )
po <- as.data.table(po)
po <- data.table::melt(po, id.vars = c('.draw','.chain','.iteration'), value.name = 'prob')
po[, oidt := as.integer(gsub('([a-z_]+)\\[([0-9]+)\\]','\\2',variable)) ]
set(po, NULL, 'variable', NULL)
tmp <- unique(subset(dcat2[item_type == 'binomial',], select = c(oidt, pid, item_id, time )))
po <- merge(po, tmp, by = c('oidt'))
po <- po[, 
         list(prob = dbinom(0:stan_data$NbinMax, stan_data$NbinMax, mean(prob)),
              y = 0:stan_data$NbinMax), 
         by = c('.draw','time','item_id')
         ]
pos <- po[, 
          list( summary_value = quantile(prob, prob = c(0.025, 0.25, 0.5, 0.75, 0.975)),
                summary_name = c('q_lower','iqr_lower','median','iqr_upper','q_upper') 
                ), 
          by = c('time','item_id','y')
          ]
pos <- data.table::dcast(pos, time + item_id + y ~ summary_name, value.var = 'summary_value')
tmp <- unique( subset(dcat2[item_type == 'binomial',], select = c(time, time_label)) )
pos <- merge(pos, tmp, by = c('time'))
tmp <- unique( subset(dcat2[item_type == 'binomial',], select = c(item_id, item_label)) )
pos <- merge(pos, tmp, by = c('item_id'))
tmp <- unique( subset(dcat2[item_type == 'binomial',], select = c(y, y_label)) )
pos <- merge(pos, tmp, by = c('y'))
tmp <- dcat2[item_type == 'binomial', list(n = length(pid)), by = c('time_label','item_label','y_label')]
tmp2 <- tmp[, list(total = sum(n)), by = c('time_label','item_label')]
tmp <- merge(tmp, tmp2, by = c('time_label','item_label'))
tmp[, p_emp := n/total]
pos <- merge(pos, subset(tmp, select = -c(n, total)), by = c('time_label','item_label','y_label'), all.x = TRUE)
set(pos, pos[, which(is.na(p_emp))], 'p_emp', 0.)
pos.bin <- copy(pos)

pos <- rbind(pos, pos.cat)
pos[, group_label := gsub('([^_]+)_([^_]+)','\\1',item_label)]

pal <- colorRampPalette(ggsci::pal_futurama("planetexpress")(12))(pos[, length(unique(item_label))])


p <- ggplot(pos, aes(x = y_label, group = interaction(item_label, y_label))) +
  geom_col(aes(fill = item_label, y = p_emp), 
           position = position_dodge(width = 0.9, preserve = "single"), 
           alpha = 0.8,
           width = 0.8 ) +
  geom_boxplot( aes(ymin = q_lower, lower = iqr_lower, middle = median, upper = iqr_upper, ymax = q_upper), 
                position = position_dodge(0.9, preserve = "single"),
                stat = 'identity',
                alpha = 0,
                width = 0.3) +
  scale_y_continuous(labels = scales::percent) +
  scale_fill_manual(values = pal) +
  facet_wrap(group_label ~ time_label, scales = "free_x", ncol = 2) +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 45,vjust = 1,hjust = 1),
        legend.position = 'bottom'
        ) +
  labs(x = '', y = 'proportion of outcomes', fill = 'survey items')
ggsave(file = file.path(dir.out, paste0(file.prefix,'probs_barplot_v2.png')), 
       plot = p, 
       h = 12, 
       w = 12
       )
```

## Third model: CG-VIO with >1 latent factors

Here is the Stan code:

```{r mlf3_stan_code, include=TRUE, eval=TRUE, echo=TRUE, tidy=FALSE, cache=TRUE}
binomial_lfm_m3_txt <- "
functions
{
  matrix transpose_array_of_vectors_to_mat(array [] vector vecs) 
  {
    int Ncol = size(vecs);           
    int Nrow = rows(vecs[1]);        
    matrix [Nrow,Ncol] ans;

    for (j in 1:Ncol) 
    {
      ans[:,j] = vecs[j];
    }
    return ans;
  }
  
  matrix lf_get_ltr_loadings_matrix( int L,
                                     int Q,
                                     vector loadings_flat
                                     )
  {
    matrix [L,Q] loadings_mat = rep_matrix(0.0, L, Q);
    for(l in 1:L)
    {
      loadings_mat[l,l:Q] 
        = 
        loadings_flat[ ((l-1)*Q - ((l-1)*(l-2)) %/% 2 + 1):(l*Q - (l*(l-1)) %/% 2) ]';
    }
    return loadings_mat;
  }
  
  vector lf_get_eta_Ldim( array [] real beta_baseline, // P x 1
                          array [] vector beta_questions, // P x Q
                          array [] matrix loadings_questions, // P x L x Q
                          array [] int group_of_obs, // N x 1
                          array [] int question_of_obs, // N x 1
                          array [] int unit_of_obs, // N x 1
                          matrix latent_factor_baseline, // P x L 
                          row_vector latent_factor_unit_sd, // 1 x L
                          matrix latent_factor_unit // U x L
                      )
  {
    int N = num_elements(unit_of_obs);
    vector [N] eta;
    for(n in 1:N)
    {
      eta[n] 
        = 
        beta_baseline[group_of_obs[n]] +
        beta_questions[group_of_obs[n]][question_of_obs[n]] + 
        ( latent_factor_baseline[group_of_obs[n],:] + latent_factor_unit_sd .* latent_factor_unit[unit_of_obs[n],:] ) * 
        loadings_questions[group_of_obs[n]][:, question_of_obs[n]]; 
    }
    return eta;
  }
}
data
{
  int<lower=1> Nbin; // number of binomial observations
  int<lower=2> Qbin; // number of binomial questions
  int<lower=1> NbinMax; // maximum number of success
  int<lower=2> P; // number of groups
  int<lower=1> U; // number of units
  int<lower=1, upper=Qbin> L; // number of latent factors
  array [Nbin] int<lower=0, upper=NbinMax> ybin; // observations
  array [Nbin] int<lower=1,upper=U> unit_of_bin_obs;
  array [Nbin] int<lower=1,upper=P> group_of_bin_obs;
  array [Nbin] int<lower=1,upper=Qbin> question_of_bin_obs;
}
transformed data
{
    real s2z_sd_groups;
    real s2z_sd_unit;
    real s2z_sd_bin_questions;
    int Nloadings_bin;
    row_vector<lower=0>[L] latent_factor_unit_sd;
  
    s2z_sd_groups = inv(sqrt(1. - inv(P)));
    s2z_sd_unit = inv(sqrt(1. - inv(U)));
    s2z_sd_bin_questions = inv(sqrt(1. - inv(Qbin)));
    Nloadings_bin = Qbin*L - (L*(L-1))%/%2;
    latent_factor_unit_sd = rep_row_vector(1.0, L);
}
parameters
{
  array [L] sum_to_zero_vector[P] latent_factor_group;
  array [L] sum_to_zero_vector[U] latent_factor_unit;
  
  
  array [P] real beta_bin_baseline;
  array [P] sum_to_zero_vector[Qbin] beta_bin_questions;
  array [P] vector<lower=0>[Nloadings_bin] loadings_bin_questions_flat;
}
transformed parameters
{
  array [P] matrix<lower=0>[L,Qbin] loadings_bin_questions;
  vector [Nbin] bin_eta;
  
  for (p in 1:P)
  {
    loadings_bin_questions[p] 
      = 
      lf_get_ltr_loadings_matrix( L,
                                  Qbin,
                                  loadings_bin_questions_flat[p]
                                  );
  }
  
  bin_eta
        =
        lf_get_eta_Ldim( beta_bin_baseline,
                         beta_bin_questions, 
                         loadings_bin_questions,
                         group_of_bin_obs,
                         question_of_bin_obs,
                         unit_of_bin_obs,
                         transpose_array_of_vectors_to_mat(latent_factor_group), // turn LxP into PxL
                         latent_factor_unit_sd,
                         transpose_array_of_vectors_to_mat(latent_factor_unit) // turn LxU into UxL
                         );
}
model
{
  // binomial likelihood for binomial obs
  ybin ~ binomial_logit(NbinMax, bin_eta);

  // priors for latent factors
  for(l in 1:L)
  {
    latent_factor_group[l] ~ normal(0, s2z_sd_groups); 
    latent_factor_unit[l] ~ normal(0, s2z_sd_unit); 
  }
  
  // priors for bin loadings
  beta_bin_baseline ~ normal(0, 3.5);
  for (p in 1:P)
  {
    beta_bin_questions[p] ~ normal(0, s2z_sd_bin_questions); 
    loadings_bin_questions_flat[p] ~ lognormal(0,1);
  }
}
generated quantities
{
  array [L] vector[U] latent_factor_unit_re;
  vector[Nbin] bin_prob_by_obs;
  array [Nbin] int<lower=0> bin_ypred;
  array [Nbin] real log_lik;
  
  for(l in 1:L)
  {
    latent_factor_unit_re[l] = latent_factor_unit_sd[l] * latent_factor_unit[l];
  }
  
  bin_prob_by_obs = inv_logit(bin_eta);
  bin_ypred = binomial_rng(NbinMax, inv_logit(bin_eta));
  
  for(n in 1:Nbin)
  {
    log_lik[ n ] = binomial_logit_lpmf( ybin[n] | NbinMax, bin_eta[n] );
  }
}
"
# compile the model
binomial_lfm_m3_filename <- cmdstanr::write_stan_file(
  gsub('\t',' ', binomial_lfm_m3_txt),
  dir = dir.out,
  basename = NULL,
  force_overwrite = FALSE,
  hash_salt = ""
)

# compile Stan model
binomial_lfm_m3_compiled <- cmdstanr::cmdstan_model(binomial_lfm_m3_filename)
```
```{r mlf3_prepare data, include=TRUE, eval=TRUE, echo=TRUE, message=FALSE, tidy=FALSE, cache=TRUE}
file.prefix <- 'cg_vio_mlf_3_'
dcat3 <- subset(dp1, grepl('CG-VIO|CG-MONITOR-CHI',item_label))
dcat3[, time := time + 1L]
dcat3[, y_stan := y]
tmp <- data.table(item_label = sort(unique(dcat3$item_label)))
tmp[, item_type := ifelse(grepl('CG-MH',item_label), 'categorical', 'binomial')]
tmp <- tmp[, list(item_label = item_label, item_id = 1:length(item_label)), by = 'item_type']
dcat3 <- merge(dcat3, tmp, by = 'item_label')
setkey(dcat3, pid, time, item_label)
dcat3[, oid := 1:nrow(dcat3)]
tmp <- dcat3[, list(oid = oid, oidt = 1:length(y_stan)), by = 'item_type' ]
dcat3 <- merge(dcat3, tmp, by  = c('item_type','oid'))
setkey(dcat3, pid, time, item_label)
```

```{r mlf3_run_model, include=TRUE, eval=TRUE, echo=TRUE, message=FALSE, tidy=FALSE, cache=TRUE}
stan_data <- list()
stan_data$P <- max(dcat3$time)
stan_data$U <- max(dcat3$pid)
stan_data$L <- 3L
stan_data$Nbin <- nrow(dcat3[item_type == 'binomial',])
stan_data$Qbin <- max(dcat3[item_type == 'binomial', item_id])
stan_data$NbinMax <- 7L
stan_data$ybin <- dcat3[item_type == 'binomial', y_stan]
stan_data$group_of_bin_obs <- dcat3[item_type == 'binomial', time]
stan_data$question_of_bin_obs <- dcat3[item_type == 'binomial', item_id]
stan_data$unit_of_bin_obs <- dcat3[item_type == 'binomial', pid]

# sample
binomial_lfm_m3_fit <- binomial_lfm_m3_compiled$sample(
  data = stan_data,
  seed = 123,
  chains = 2,
  parallel_chains = 2,
  iter_warmup = 5e2,
  iter_sampling = 15e2,
  refresh = 500,
  save_warmup = TRUE
)

# save output to RDS
binomial_lfm_m3_fit$save_object(file = file.path(dir.out, paste0(file.prefix,"stan.rds")))
```


```{r mlf3_load_model, include=FALSE, eval=TRUE, echo=FALSE, tidy=FALSE, cache=TRUE}
binomial_lfm_m3_fit <- readRDS(file = file.path(dir.out, paste0(file.prefix,'stan.rds')))
```

```{r mlf3_check_mixing_convergence, include=FALSE, eval=TRUE, echo=FALSE, tidy=FALSE, cache=TRUE}
#	check convergence and check mixing
#	get 95% credible intervals
tmp <- binomial_lfm_m3_fit$summary(
  variables = c('latent_factor_group','latent_factor_unit',
                'beta_bin_baseline','beta_bin_questions','loadings_bin_questions_flat'
                ),
  posterior::default_summary_measures(),
  posterior::default_convergence_measures(),
  extra_quantiles = ~posterior::quantile2(., probs = c(.0275, .975))
)
tmp <- as.data.table(tmp)
tmp[order(ess_bulk),]
tmp[order(ess_bulk)[1:10],]

  
# parameter with lowest ess_bulk
worst_var <- tmp$variable[ which.min(tmp$ess_bulk) ]

# extract samples
po <- binomial_lfm_m3_fit$draws(
  variables = c("lp__", worst_var),
  inc_warmup = TRUE,
  format = "draws_array"
  )

# make worst trace plot
p <- bayesplot:::mcmc_trace(po,  
                            pars = c("lp__",worst_var), 
                            n_warmup = 500,
                            facet_args = list(nrow = 2)
                            )
p <- p + theme_bw()
ggsave(file = file.path(dir.out, paste0(file.prefix,'worsttrace.png')), 
       plot = p, 
       h = 10, 
       w = 8
       )

# make intervals plot
po <- binomial_lfm_m3_fit$draws(
  variables = c('latent_factor_group','latent_factor_unit',
                'beta_bin_baseline','beta_bin_questions','loadings_bin_questions_flat'
                ),
  inc_warmup = FALSE,
  format = "draws_array"
  )

color_scheme_set("teal")
p <- bayesplot::mcmc_intervals(po, prob = 0.5, prob_outer = 0.95, outer_size = 1, point_size = 2) +
  theme_bw()
ggsave(file = file.path(dir.out, paste0(file.prefix,'intervals.png')), 
       plot = p, 
       h = 100, 
       w = 8, 
       limitsize = FALSE
       )
```

```{r mlf3_extract_class_probs_by_question, include=FALSE, eval=TRUE, echo=FALSE, tidy=FALSE, cache=TRUE}
po <- binomial_lfm_m3_fit$draws(
  variables = c('bin_prob_by_obs'),
  inc_warmup = FALSE,
  format = "draws_df"
  )
po <- as.data.table(po)
po <- data.table::melt(po, id.vars = c('.draw','.chain','.iteration'), value.name = 'prob')
po[, oidt := as.integer(gsub('([a-z_]+)\\[([0-9]+)\\]','\\2',variable)) ]
set(po, NULL, 'variable', NULL)
tmp <- unique(subset(dcat3[item_type == 'binomial',], select = c(oidt, pid, item_id, time )))
po <- merge(po, tmp, by = c('oidt'))
po <- po[, 
         list(prob = dbinom(0:stan_data$NbinMax, stan_data$NbinMax, mean(prob)),
              y = 0:stan_data$NbinMax), 
         by = c('.draw','time','item_id')
         ]
pos <- po[, 
          list( summary_value = quantile(prob, prob = c(0.025, 0.25, 0.5, 0.75, 0.975)),
                summary_name = c('q_lower','iqr_lower','median','iqr_upper','q_upper') 
                ), 
          by = c('time','item_id','y')
          ]
pos <- data.table::dcast(pos, time + item_id + y ~ summary_name, value.var = 'summary_value')
tmp <- unique( subset(dcat3[item_type == 'binomial',], select = c(time, time_label)) )
pos <- merge(pos, tmp, by = c('time'))
tmp <- unique( subset(dcat3[item_type == 'binomial',], select = c(item_id, item_label)) )
pos <- merge(pos, tmp, by = c('item_id'))
tmp <- unique( subset(dcat3[item_type == 'binomial',], select = c(y, y_label)) )
pos <- merge(pos, tmp, by = c('y'))
tmp <- dcat3[item_type == 'binomial', list(n = length(pid)), by = c('time_label','item_label','y_label')]
tmp2 <- tmp[, list(total = sum(n)), by = c('time_label','item_label')]
tmp <- merge(tmp, tmp2, by = c('time_label','item_label'))
tmp[, p_emp := n/total]
pos <- merge(pos, subset(tmp, select = -c(n, total)), by = c('time_label','item_label','y_label'), all.x = TRUE)
set(pos, pos[, which(is.na(p_emp))], 'p_emp', 0.)
pos.bin <- copy(pos)

pos[, group_label := gsub('([^_]+)_([^_]+)','\\1',item_label)]

pal <- colorRampPalette(ggsci::pal_futurama("planetexpress")(12))(pos[, length(unique(item_label))])


p <- ggplot(pos, aes(x = y_label, group = interaction(item_label, y_label))) +
  geom_col(aes(fill = item_label, y = p_emp), 
           position = position_dodge(width = 0.9, preserve = "single"), 
           alpha = 0.8,
           width = 0.8 ) +
  geom_boxplot( aes(ymin = q_lower, lower = iqr_lower, middle = median, upper = iqr_upper, ymax = q_upper), 
                position = position_dodge(0.9, preserve = "single"),
                stat = 'identity',
                alpha = 0,
                width = 0.3) +
  scale_y_continuous(labels = scales::percent) +
  scale_fill_manual(values = pal) +
  facet_wrap(group_label ~ time_label, scales = "free_x", ncol = 2) +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 45,vjust = 1,hjust = 1),
        legend.position = 'bottom'
        ) +
  labs(x = '', y = 'proportion of outcomes', fill = 'survey items')
ggsave(file = file.path(dir.out, paste0(file.prefix,'probs_barplot_v2.png')), 
       plot = p, 
       h = 12, 
       w = 12
       )
```

## Fourth model: treat out of 7 as category

Here is the Stan code:

```{r mlf4_stan_code, include=TRUE, eval=TRUE, echo=TRUE, tidy=FALSE, cache=TRUE}
ocat_lfm_m4_txt <- "
functions
{
  vector lf_get_eta( array [] vector beta_questions, 
                     array [] vector loadings_questions,
                     array [] int group_of_obs,
                     array [] int question_of_obs,
                     array [] int unit_of_obs,
                     vector latent_factor_baseline,
                     real latent_factor_unit_sd,
                     vector latent_factor_unit
                   )
  {
    int N = num_elements(unit_of_obs);
    vector [N] eta;
    eta = latent_factor_unit_sd * latent_factor_unit[unit_of_obs];
    for(n in 1:N)
    {
      eta[n] 
        = 
        beta_questions[group_of_obs[n]][question_of_obs[n]] + 
        loadings_questions[group_of_obs[n]][question_of_obs[n]] * 
        (
          latent_factor_baseline[group_of_obs[n]] + eta[n]
        );
    }
    return eta;
  }
}
data
{
  int<lower=1> Ncat; // number of ordered categorial observations
  int<lower=2> Qcat; // number of ordered categorial questions
  int<lower=3> Kcat; // number of categories
  int<lower=1> Ncat2; 
  int<lower=2> Qcat2; 
  int<lower=3> Kcat2; 
  int<lower=2> P; // number of groups
  int<lower=1> U; // number of units
  array [Ncat] int<lower=1, upper=Kcat> ycat; // observations
  array [Ncat] int<lower=1,upper=U> unit_of_cat_obs;
  array [Ncat] int<lower=1,upper=P> group_of_cat_obs;
  array [Ncat] int<lower=1,upper=Qcat> question_of_cat_obs;
  array [Ncat2] int<lower=1, upper=Kcat2> ycat2; // observations
  array [Ncat2] int<lower=1,upper=U> unit_of_cat2_obs;
  array [Ncat2] int<lower=1,upper=P> group_of_cat2_obs;
  array [Ncat2] int<lower=1,upper=Qcat2> question_of_cat2_obs;
}
transformed data
{
    real s2z_sd_groups;
    real s2z_sd_unit;
    real s2z_sd_cat_questions;
    real s2z_sd_cat2_questions;
    s2z_sd_groups = inv(sqrt(1. - inv(P)));
    s2z_sd_unit = inv(sqrt(1. - inv(U)));
    s2z_sd_cat_questions = inv(sqrt(1. - inv(Qcat)));
    s2z_sd_cat2_questions = inv(sqrt(1. - inv(Qcat2)));
}
parameters
{
  sum_to_zero_vector[P] latent_factor_group;
  sum_to_zero_vector[U] latent_factor_unit;
  real<lower=0> latent_factor_unit_sd;
  array [P] sum_to_zero_vector[Qcat] beta_cat_questions;
  array [P] vector<lower=0>[Qcat-1] loadings_cat_questions_m1;
  real cat_cut_point_1;
  matrix<lower=0>[Kcat - 2,P] cat_cut_point_gaps_groups;
  array [P] sum_to_zero_vector[Qcat2] beta_cat2_questions;
  array [P] vector<lower=0>[Qcat2-1] loadings_cat2_questions_m1;
  real cat2_cut_point_1;
  matrix<lower=0>[Kcat2 - 2,P] cat2_cut_point_gaps_groups;
}
transformed parameters
{
  array [P] ordered[Kcat-1] cat_cut_points;
  array [P] vector<lower=0>[Qcat] loadings_cat_questions;
  array [P] ordered[Kcat2-1] cat2_cut_points;
  array [P] vector<lower=0>[Qcat2] loadings_cat2_questions;
  vector [Ncat] cat_eta;
  vector [Ncat2] cat2_eta;
  
  for (p in 1:P)
  {
    cat_cut_points[p] = 
      append_row(
        rep_vector(cat_cut_point_1, 1), 
        rep_vector(cat_cut_point_1, Kcat-2) + cumulative_sum(cat_cut_point_gaps_groups[,p])
        );
    loadings_cat_questions[p] = append_row(1.0, loadings_cat_questions_m1[p]);
    cat2_cut_points[p] = 
      append_row(
        rep_vector(cat2_cut_point_1, 1), 
        rep_vector(cat2_cut_point_1, Kcat2-2) + cumulative_sum(cat2_cut_point_gaps_groups[,p])
        );
        
    loadings_cat2_questions[p] = append_row(1.0, loadings_cat2_questions_m1[p]);
  }
  
  cat_eta
        = 
        lf_get_eta( beta_cat_questions, 
                    loadings_cat_questions,
                    group_of_cat_obs,
                    question_of_cat_obs,
                    unit_of_cat_obs,
                    latent_factor_group,
                    latent_factor_unit_sd,
                    latent_factor_unit
                    );
  cat2_eta
        = 
        lf_get_eta( beta_cat2_questions, 
                    loadings_cat2_questions,
                    group_of_cat2_obs,
                    question_of_cat2_obs,
                    unit_of_cat2_obs,
                    latent_factor_group,
                    latent_factor_unit_sd,
                    latent_factor_unit
                    );
  
}
model
{
  // ordered cat likelihood, cannot be vectorized, for cat obs
  for (n in 1:Ncat) 
  {
    ycat[n] ~ ordered_logistic(cat_eta[n], cat_cut_points[group_of_cat_obs[n]]);
  }
  for (n in 1:Ncat2) 
  {
    ycat2[n] ~ ordered_logistic(cat2_eta[n], cat2_cut_points[group_of_cat2_obs[n]]);
  }
  
  // priors for latent factors
  latent_factor_group ~ normal(0, s2z_sd_groups); 
  latent_factor_unit ~ normal(0, s2z_sd_unit); 
  latent_factor_unit_sd ~ exponential(2); 
  
  // priors for cat loadings
  cat_cut_point_1 ~ normal(0, 3.5); 
  to_vector(cat_cut_point_gaps_groups) ~ normal(0, 2.5); 
  for (p in 1:P)
  {
    beta_cat_questions[p] ~ normal(0, s2z_sd_cat_questions); 
    loadings_cat_questions_m1[p] ~ lognormal(0,1);
  }
  cat2_cut_point_1 ~ normal(0, 3.5); 
  to_vector(cat2_cut_point_gaps_groups) ~ normal(0, 2.5); 
  for (p in 1:P)
  {
    beta_cat2_questions[p] ~ normal(0, s2z_sd_cat2_questions); 
    loadings_cat2_questions_m1[p] ~ lognormal(0,1);
  }
}
generated quantities
{
  vector[U] latent_factor_unit_re;
  array [P,Qcat+Qcat2] real mean_eta;
  array [Kcat,Ncat] real<lower=0, upper=1> cat_ordered_prob_by_obs;
  array [Kcat2,Ncat2] real<lower=0, upper=1> cat2_ordered_prob_by_obs;
  array [Ncat] int<lower=0> cat_ypred;
  array [Ncat2] int<lower=0> cat2_ypred;
  array [Ncat+Ncat2] real log_lik;
  
  latent_factor_unit_re = latent_factor_unit_sd * latent_factor_unit;
  
  for (p in 1:P) 
  {
    for (q in 1:Qcat) 
    {
      mean_eta[p,q] = beta_cat_questions[p][q] + loadings_cat_questions[p][q] * latent_factor_group[p];
    }
    for (q in 1:Qcat2) 
    {
      mean_eta[p,Qcat+q] = beta_cat2_questions[p][q] + loadings_cat2_questions[p][q] * latent_factor_group[p];
    }
  }
  
  for (n in 1:Ncat) 
  {
    cat_ordered_prob_by_obs[1,n]          = 1 - inv_logit( cat_eta[n] - cat_cut_points[group_of_cat_obs[n]][1] );
    cat_ordered_prob_by_obs[2:(Kcat-1),n] = to_array_1d( inv_logit( cat_eta[n] - cat_cut_points[group_of_cat_obs[n]][1:(Kcat-2)] ) 
                                                         - 
                                                         inv_logit( cat_eta[n] - cat_cut_points[group_of_cat_obs[n]][2:(Kcat-1)] )
                                                         );
    cat_ordered_prob_by_obs[Kcat,n]       = inv_logit( cat_eta[n] - cat_cut_points[group_of_cat_obs[n]][Kcat-1] );
  }
  
  for (n in 1:Ncat2) 
  {
    cat2_ordered_prob_by_obs[1,n]           = 1 - inv_logit( cat2_eta[n] - cat2_cut_points[group_of_cat2_obs[n]][1] );
    cat2_ordered_prob_by_obs[2:(Kcat2-1),n] = to_array_1d( inv_logit( cat2_eta[n] - cat2_cut_points[group_of_cat2_obs[n]][1:(Kcat2-2)] ) 
                                                           - 
                                                           inv_logit( cat2_eta[n] - cat2_cut_points[group_of_cat2_obs[n]][2:(Kcat2-1)] )
                                                           );
    cat2_ordered_prob_by_obs[Kcat2,n]       = inv_logit( cat2_eta[n] - cat2_cut_points[group_of_cat2_obs[n]][Kcat2-1] );
  }
  
  for( n in 1:Ncat ) 
  {
    cat_ypred[n] = ordered_logistic_rng( cat_eta[n], cat_cut_points[group_of_cat_obs[n]] ); 
  }
  for( n in 1:Ncat2 ) 
  {
    cat2_ypred[n] = ordered_logistic_rng( cat2_eta[n], cat2_cut_points[group_of_cat2_obs[n]] ); 
  }
  
  for(n in 1:Ncat)
  {
    log_lik[n] =  ordered_logistic_lpmf( ycat[n] | cat_eta[n], cat_cut_points[group_of_cat_obs[n]] );
  }
  for(n in 1:Ncat2)
  {
    log_lik[ Ncat+n ] = ordered_logistic_lpmf( ycat2[n] | cat2_eta[n], cat2_cut_points[group_of_cat2_obs[n]] );
  }
}
"
# compile the model
ocat_lfm_m4_filename <- cmdstanr::write_stan_file(
  gsub('\t',' ', ocat_lfm_m4_txt),
  dir = dir.out,
  basename = NULL,
  force_overwrite = FALSE,
  hash_salt = ""
)

# compile Stan model
ocat_lfm_m4_compiled <- cmdstanr::cmdstan_model(ocat_lfm_m4_filename)
```
Let's fit the model!

```{r mlf4_prepare data, include=TRUE, eval=TRUE, echo=TRUE, message=FALSE, tidy=FALSE, cache=TRUE}
file.prefix <- 'cgmh_vio_monitor_mlf_4_'
dcat4 <- subset(dp1, grepl('CG-MH|CG-VIO|CG-MONITOR-CHI',item_label))
dcat4[, time := time + 1L]
dcat4[, y_stan := y + 1L]
tmp <- data.table(item_label = sort(unique(dcat4$item_label)))
tmp[, item_type := ifelse(grepl('CG-MH',item_label), 'categorical', 'out-of-7')]
tmp <- tmp[, list(item_label = item_label, item_id = 1:length(item_label)), by = 'item_type']
dcat4 <- merge(dcat4, tmp, by = 'item_label')
setkey(dcat4, pid, time, item_label)
dcat4[, oid := 1:nrow(dcat4)]
tmp <- dcat4[, list(oid = oid, oidt = 1:length(y_stan)), by = 'item_type' ]
dcat4 <- merge(dcat4, tmp, by  = c('item_type','oid'))
setkey(dcat4, pid, time, item_label)
```

```{r mlf4_run_model, include=TRUE, eval=TRUE, echo=TRUE, message=FALSE, tidy=FALSE, cache=TRUE}
# define data in format needed for model specification
stan_data <- list()
stan_data$P <- max(dcat4$time)
stan_data$U <- max(dcat4$pid)
stan_data$Ncat <- nrow(dcat4[item_type == 'categorical',])
stan_data$Qcat <- max(dcat4[item_type == 'categorical', item_id])
stan_data$Kcat <- length(unique(dcat4[item_type == 'categorical', y_stan]))
stan_data$ycat <- dcat4[item_type == 'categorical', y_stan]
stan_data$group_of_cat_obs <- dcat4[item_type == 'categorical', time]
stan_data$question_of_cat_obs <- dcat4[item_type == 'categorical', item_id]
stan_data$unit_of_cat_obs <- dcat4[item_type == 'categorical', pid]
stan_data$Ncat2 <- nrow(dcat4[item_type == 'out-of-7',])
stan_data$Qcat2 <- max(dcat4[item_type == 'out-of-7', item_id])
stan_data$Kcat2 <- length(unique(dcat4[item_type == 'out-of-7', y_stan]))
stan_data$ycat2 <- dcat4[item_type == 'out-of-7', y_stan]
stan_data$group_of_cat2_obs <- dcat4[item_type == 'out-of-7', time]
stan_data$question_of_cat2_obs <- dcat4[item_type == 'out-of-7', item_id]
stan_data$unit_of_cat2_obs <- dcat4[item_type == 'out-of-7', pid]

# sample
ocat_lfm_m4_fit <- ocat_lfm_m4_compiled$sample(
  data = stan_data,
  seed = 123,
  chains = 2,
  parallel_chains = 2,
  iter_warmup = 5e2,
  iter_sampling = 15e2,
  refresh = 500,
  save_warmup = TRUE
)

# save output to RDS
ocat_lfm_m4_fit$save_object(file = file.path(dir.out, paste0(file.prefix,"stan.rds")))
```


```{r mlf4_load_model, include=FALSE, eval=TRUE, echo=FALSE, tidy=FALSE, cache=TRUE}
ocat_lfm_m4_fit <- readRDS(file = file.path(dir.out, paste0(file.prefix,'stan.rds')))
```

```{r mlf4_check_mixing_convergence, include=FALSE, eval=TRUE, echo=FALSE, tidy=FALSE, cache=TRUE}
#	check convergence and check mixing
#	get 95% credible intervals
tmp <- ocat_lfm_m4_fit$summary(
  variables = c('latent_factor_group','latent_factor_unit','latent_factor_unit_sd',
                'beta_cat_questions','loadings_cat_questions_m1','cat_cut_point_1','cat_cut_point_gaps_groups',
                'beta_cat2_questions','loadings_cat2_questions_m1','cat2_cut_point_1','cat2_cut_point_gaps_groups'
                ),
  posterior::default_summary_measures(),
  posterior::default_convergence_measures(),
  extra_quantiles = ~posterior::quantile2(., probs = c(.0275, .975))
)
tmp <- as.data.table(tmp)
tmp[order(ess_bulk),]
  
# parameter with lowest ess_bulk
worst_var <- tmp$variable[ which.min(tmp$ess_bulk) ]

# extract samples
po <- ocat_lfm_m4_fit$draws(
  variables = c("lp__", worst_var),
  inc_warmup = TRUE,
  format = "draws_array"
  )

# make worst trace plot
p <- bayesplot:::mcmc_trace(po,  
                            pars = c("lp__",worst_var), 
                            n_warmup = 500,
                            facet_args = list(nrow = 2)
                            )
p <- p + theme_bw()
ggsave(file = file.path(dir.out, paste0(file.prefix,'worsttrace.png')), 
       plot = p, 
       h = 10, 
       w = 8
       )

# make intervals plot
po <- ocat_lfm_m4_fit$draws(
  variables = c('latent_factor_group','latent_factor_unit','latent_factor_unit_sd',
                'beta_cat_questions','loadings_cat_questions_m1','cat_cut_point_1','cat_cut_point_gaps_groups',
                'beta_cat2_questions','loadings_cat2_questions_m1','cat2_cut_point_1','cat2_cut_point_gaps_groups'
                ),
  inc_warmup = FALSE,
  format = "draws_array"
  )

color_scheme_set("teal")
p <- bayesplot::mcmc_intervals(po, prob = 0.5, prob_outer = 0.95, outer_size = 1, point_size = 2) +
  theme_bw()
ggsave(file = file.path(dir.out, paste0(file.prefix,'intervals.png')), 
       plot = p, 
       h = 50, 
       w = 8, 
       limitsize = FALSE
       )


# make posterior predictive check

# create median and 95\% credible intervals
po <- ocat_lfm_m4_fit$draws(
  variables = c('cat_ypred','cat2_ypred'),
  inc_warmup = FALSE,
  format = "draws_df"
  )
po <- as.data.table(po)
po <- data.table::melt(po, id.vars = c('.draw','.chain','.iteration'), value.name = 'ypred')
po[, item_type := gsub('([a-z0-9_]+)\\[([0-9]+)\\]','\\1',variable) ]
po[, oidt := as.integer(gsub('([a-z0-9_]+)\\[([0-9]+)\\]','\\2',variable)) ]
tmp <- po[, which(grepl('cat2',item_type))]
set(po, tmp, 'item_type', 'out-of-7')
set(po, po[, which(grepl('ypred',item_type))], 'item_type', 'categorical')  

pos <- 
  po[,
     list( summary_value = quantile(ypred, prob = c(0.025, 0.25, 0.5, 0.75, 0.975)),
           summary_name = c('q_lower','iqr_lower','median','iqr_upper','q_upper') 
           ),
     by = c('item_type','oidt')
     ]
pos <- 
  data.table::dcast(pos,
                    item_type + oidt ~ summary_name, 
                    value.var = 'summary_value'
                    )

pos <- merge(pos, dcat4, by = c('item_type','oidt'))
pos[, IN_PPI := y_stan >= q_lower & y_stan <= q_upper]
pos[, mean(IN_PPI)]

# plot posterior predictive check
p <- ggplot(pos, aes(x = oid, group = oid)) + 
  geom_boxplot( aes( ymin = q_lower,
                     lower = iqr_lower,
                     middle = median,
                     upper = iqr_upper,
                     ymax = q_upper),
                stat = 'identity') +
  geom_point( aes(y = y_stan, colour = IN_PPI ) ) +
  facet_grid(item_label ~ time_label, scales = 'free') +
  scale_x_discrete() +
  scale_y_continuous() +
  ggsci::scale_color_npg() +
  labs(x = '', 
       y = 'outcome', 
       colour = 'within\n95% posterior\nprediction\ninterval') +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 45,vjust = 1,hjust = 1))
ggsave(file = file.path(dir.out, paste0(file.prefix,'ppcheck.png')), 
       p, 
       w = 20, 
       h = 20)
```

```{r mlf4_extract_class_probs_by_question, include=FALSE, eval=TRUE, echo=FALSE, tidy=FALSE, cache=TRUE}
po <- ocat_lfm_m4_fit$draws(
  variables = c('cat_ordered_prob_by_obs'),
  inc_warmup = FALSE,
  format = "draws_df"
  )
po <- as.data.table(po)
po <- data.table::melt(po, id.vars = c('.draw','.chain','.iteration'), value.name = 'prob')
po[, y_stan := as.integer(gsub('([a-z_]+)\\[([0-9]),([0-9]+)\\]','\\2',variable)) ]
po[, oidt := as.integer(gsub('([a-z_]+)\\[([0-9]),([0-9]+)\\]','\\3',variable)) ]
set(po, NULL, 'variable', NULL)
tmp <- unique(subset(dcat4[item_type == 'categorical',], select = c(oidt, pid, item_id, time )))
po <- merge(po, tmp, by = c('oidt'))
po <- po[, list(prob = mean(prob)), by = c('.draw','time','item_id','y_stan')]
pos <- po[, 
          list( summary_value = quantile(prob, prob = c(0.025, 0.25, 0.5, 0.75, 0.975)),
                summary_name = c('q_lower','iqr_lower','median','iqr_upper','q_upper') 
                ), 
          by = c('time','item_id','y_stan')
          ]
pos <- data.table::dcast(pos, time + item_id + y_stan ~ summary_name, value.var = 'summary_value')
tmp <- unique( subset(dcat4[item_type == 'categorical',], select = c(time, time_label)) )
pos <- merge(pos, tmp, by = c('time'))
tmp <- unique( subset(dcat4[item_type == 'categorical',], select = c(item_id, item_label)) )
pos <- merge(pos, tmp, by = c('item_id'))
tmp <- unique( subset(dcat4[item_type == 'categorical',], select = c(y, y_stan, y_label)) )
pos <- merge(pos, tmp, by = c('y_stan'))
tmp <- dcat4[item_type == 'categorical', list(n = length(pid)), by = c('time_label','item_label','y_label')]
tmp2 <- tmp[, list(total = sum(n)), by = c('time_label','item_label')]
tmp <- merge(tmp, tmp2, by = c('time_label','item_label'))
tmp[, p_emp := n/total]
pos <- merge(pos, subset(tmp, select = -c(n, total)), by = c('time_label','item_label','y_label'), all.x = TRUE)
set(pos, pos[, which(is.na(p_emp))], 'p_emp', 0.)
pos[, y_stan := NULL]
pos.cat <- copy(pos)


po <- ocat_lfm_m4_fit$draws(
  variables = c('cat2_ordered_prob_by_obs'),
  inc_warmup = FALSE,
  format = "draws_df"
  )
po <- as.data.table(po)
po <- data.table::melt(po, id.vars = c('.draw','.chain','.iteration'), value.name = 'prob')
po[, y_stan := as.integer(gsub('([a-z0-9_]+)\\[([0-9]),([0-9]+)\\]','\\2',variable)) ]
po[, oidt := as.integer(gsub('([a-z0-9_]+)\\[([0-9]),([0-9]+)\\]','\\3',variable)) ]
set(po, NULL, 'variable', NULL)
tmp <- unique(subset(dcat4[item_type == 'out-of-7',], select = c(oidt, pid, item_id, time )))
po <- merge(po, tmp, by = c('oidt'))
po <- po[, list(prob = mean(prob)), by = c('.draw','time','item_id','y_stan')]
pos <- po[, 
          list( summary_value = quantile(prob, prob = c(0.025, 0.25, 0.5, 0.75, 0.975)),
                summary_name = c('q_lower','iqr_lower','median','iqr_upper','q_upper') 
                ), 
          by = c('time','item_id','y_stan')
          ]
pos <- data.table::dcast(pos, time + item_id + y_stan ~ summary_name, value.var = 'summary_value')
tmp <- unique( subset(dcat4[item_type == 'out-of-7',], select = c(time, time_label)) )
pos <- merge(pos, tmp, by = c('time'))
tmp <- unique( subset(dcat4[item_type == 'out-of-7',], select = c(item_id, item_label)) )
pos <- merge(pos, tmp, by = c('item_id'))
tmp <- unique( subset(dcat4[item_type == 'out-of-7',], select = c(y, y_stan, y_label)) )
pos <- merge(pos, tmp, by = c('y_stan'))
tmp <- dcat4[item_type == 'out-of-7', list(n = length(pid)), by = c('time_label','item_label','y_label')]
tmp2 <- tmp[, list(total = sum(n)), by = c('time_label','item_label')]
tmp <- merge(tmp, tmp2, by = c('time_label','item_label'))
tmp[, p_emp := n/total]
pos <- merge(pos, subset(tmp, select = -c(n, total)), by = c('time_label','item_label','y_label'), all.x = TRUE)
set(pos, pos[, which(is.na(p_emp))], 'p_emp', 0.)
pos[, y_stan := NULL]
pos.cat2 <- copy(pos)

pos <- rbind(pos.cat, pos.cat2)
pos[, group_label := gsub('([^_]+)_([^_]+)','\\1',item_label)]

pal <- colorRampPalette(ggsci::pal_futurama("planetexpress")(12))(pos[, length(unique(item_label))])


p <- ggplot(pos, aes(x = y_label, group = interaction(item_label, y_label))) +
  geom_col(aes(fill = item_label, y = p_emp), 
           position = position_dodge(width = 0.9, preserve = "single"), 
           alpha = 0.8,
           width = 0.8 ) +
  geom_boxplot( aes(ymin = q_lower, lower = iqr_lower, middle = median, upper = iqr_upper, ymax = q_upper), 
                position = position_dodge(0.9, preserve = "single"),
                stat = 'identity',
                alpha = 0,
                width = 0.3) +
  scale_y_continuous(labels = scales::percent) +
  scale_fill_manual(values = pal) +
  facet_wrap(group_label ~ time_label, scales = "free_x", ncol = 2) +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 45,vjust = 1,hjust = 1),
        legend.position = 'bottom'
        ) +
  labs(x = '', y = 'proportion of outcomes', fill = 'survey items')
ggsave(file = file.path(dir.out, paste0(file.prefix,'probs_barplot_v2.png')), 
       plot = p, 
       h = 12, 
       w = 12
       )
```

## Fourth model: use all data


```{r mlf4all_prepare data, include=TRUE, eval=TRUE, echo=TRUE, message=FALSE, tidy=FALSE, cache=TRUE}
file.prefix <- 'cg_chi_all_lf1_'
dcat5 <- copy(dp1)
dcat5[, time := time + 1L]
dcat5[, y_stan := y + 1L]
tmp <- data.table(item_label = sort(unique(dcat5$item_label)))
tmp[, item_type := ifelse(grepl('CG-MH',item_label), 'categorical', 'out-of-7')]
tmp <- tmp[, list(item_label = item_label, item_id = 1:length(item_label)), by = 'item_type']
dcat5 <- merge(dcat5, tmp, by = 'item_label')
setkey(dcat5, pid, time, item_label)
dcat5[, oid := 1:nrow(dcat5)]
tmp <- dcat5[, list(oid = oid, oidt = 1:length(y_stan)), by = 'item_type' ]
dcat5 <- merge(dcat5, tmp, by  = c('item_type','oid'))
setkey(dcat5, pid, time, item_label)
```

```{r mlf4all_run_model, include=TRUE, eval=TRUE, echo=TRUE, message=FALSE, tidy=FALSE, cache=TRUE}
# define data in format needed for model specification
stan_data <- list()
stan_data$P <- max(dcat5$time)
stan_data$U <- max(dcat5$pid)
stan_data$Ncat <- nrow(dcat5[item_type == 'categorical',])
stan_data$Qcat <- max(dcat5[item_type == 'categorical', item_id])
stan_data$Kcat <- length(unique(dcat5[item_type == 'categorical', y_stan]))
stan_data$ycat <- dcat5[item_type == 'categorical', y_stan]
stan_data$group_of_cat_obs <- dcat5[item_type == 'categorical', time]
stan_data$question_of_cat_obs <- dcat5[item_type == 'categorical', item_id]
stan_data$unit_of_cat_obs <- dcat5[item_type == 'categorical', pid]
stan_data$Ncat2 <- nrow(dcat5[item_type == 'out-of-7',])
stan_data$Qcat2 <- max(dcat5[item_type == 'out-of-7', item_id])
stan_data$Kcat2 <- length(unique(dcat5[item_type == 'out-of-7', y_stan]))
stan_data$ycat2 <- dcat5[item_type == 'out-of-7', y_stan]
stan_data$group_of_cat2_obs <- dcat5[item_type == 'out-of-7', time]
stan_data$question_of_cat2_obs <- dcat5[item_type == 'out-of-7', item_id]
stan_data$unit_of_cat2_obs <- dcat5[item_type == 'out-of-7', pid]

# sample
ocat_lfm_m4all_fit <- ocat_lfm_m4_compiled$sample(
  data = stan_data,
  seed = 123,
  chains = 2,
  parallel_chains = 2,
  iter_warmup = 5e2,
  iter_sampling = 15e2,
  refresh = 500,
  save_warmup = TRUE
)

# save output to RDS
ocat_lfm_m4all_fit$save_object(file = file.path(dir.out, paste0(file.prefix,"stan.rds")))
```


```{r mlf4all_load_model, include=FALSE, eval=TRUE, echo=FALSE, tidy=FALSE, cache=TRUE}
ocat_lfm_m4all_fit <- readRDS(file = file.path(dir.out, paste0(file.prefix,'stan.rds')))
```

```{r mlf4all_check_mixing_convergence, include=FALSE, eval=TRUE, echo=FALSE, tidy=FALSE, cache=TRUE}
#	check convergence and check mixing
#	get 95% credible intervals
tmp <- ocat_lfm_m4all_fit$summary(
  variables = c('latent_factor_group','latent_factor_unit','latent_factor_unit_sd',
                'beta_cat_questions','loadings_cat_questions_m1','cat_cut_point_1','cat_cut_point_gaps_groups',
                'beta_cat2_questions','loadings_cat2_questions_m1','cat2_cut_point_1','cat2_cut_point_gaps_groups'
                ),
  posterior::default_summary_measures(),
  posterior::default_convergence_measures(),
  extra_quantiles = ~posterior::quantile2(., probs = c(.0275, .975))
)
tmp <- as.data.table(tmp)
tmp <- tmp[order(ess_bulk),]
tmp
  
# worst 14 parameters with lowest ess_bulk
worst_var <- tmp$variable[ 1:14 ]

# extract samples
po <- ocat_lfm_m4all_fit$draws(
  variables = c("lp__", worst_var),
  inc_warmup = TRUE,
  format = "draws_array"
  )

# make worst trace plot
p <- bayesplot:::mcmc_trace(po,  
                            pars = c("lp__",worst_var), 
                            n_warmup = 500,
                            facet_args = list(nrow = 3)
                            )
p <- p + theme_bw()
ggsave(file = file.path(dir.out, paste0(file.prefix,'worsttrace.png')), 
       plot = p, 
       h = 10, 
       w = 30
       )

# make intervals/areas plot
po <- ocat_lfm_m4all_fit$draws(
  variables = c('latent_factor_group','latent_factor_unit','latent_factor_unit_sd',
                'beta_cat_questions','loadings_cat_questions_m1','cat_cut_point_1','cat_cut_point_gaps_groups',
                'beta_cat2_questions','loadings_cat2_questions_m1','cat2_cut_point_1','cat2_cut_point_gaps_groups'
                ),
  inc_warmup = FALSE,
  format = "draws_array"
  )

color_scheme_set("teal")
p <- bayesplot::mcmc_intervals(po, prob = 0.5, prob_outer = 0.95, outer_size = 1, point_size = 2) +
  theme_bw()
ggsave(file = file.path(dir.out, paste0(file.prefix,'intervals.png')), 
       plot = p, 
       h = 50, 
       w = 8, 
       limitsize = FALSE
       )

  
p <- bayesplot::mcmc_areas(po, prob = 0.5, prob_outer = 0.95, point_est = 'median') +
  theme_bw()
ggsave(file = file.path(dir.out, paste0(file.prefix,'areas.png')), 
       plot = p, 
       h = 150, 
       w = 8, 
       limitsize = FALSE
       )


# make posterior predictive check

# create median and 95\% credible intervals
po <- ocat_lfm_m4all_fit$draws(
  variables = c('cat_ypred','cat2_ypred'),
  inc_warmup = FALSE,
  format = "draws_df"
  )
po <- as.data.table(po)
po <- data.table::melt(po, id.vars = c('.draw','.chain','.iteration'), value.name = 'ypred')
po[, item_type := gsub('([a-z0-9_]+)\\[([0-9]+)\\]','\\1',variable) ]
po[, oidt := as.integer(gsub('([a-z0-9_]+)\\[([0-9]+)\\]','\\2',variable)) ]
tmp <- po[, which(grepl('cat2',item_type))]
set(po, tmp, 'item_type', 'out-of-7')
set(po, po[, which(grepl('ypred',item_type))], 'item_type', 'categorical')  

pos <- 
  po[,
     list( summary_value = quantile(ypred, prob = c(0.025, 0.25, 0.5, 0.75, 0.975)),
           summary_name = c('q_lower','iqr_lower','median','iqr_upper','q_upper') 
           ),
     by = c('item_type','oidt')
     ]
pos <- 
  data.table::dcast(pos,
                    item_type + oidt ~ summary_name, 
                    value.var = 'summary_value'
                    )

pos <- merge(pos, dcat5, by = c('item_type','oidt'))
pos[, IN_PPI := y_stan >= q_lower & y_stan <= q_upper]
pos[, mean(IN_PPI)]

# plot posterior predictive check
p <- ggplot(pos, aes(x = oid, group = oid)) + 
  geom_boxplot( aes( ymin = q_lower,
                     lower = iqr_lower,
                     middle = median,
                     upper = iqr_upper,
                     ymax = q_upper),
                stat = 'identity') +
  geom_point( aes(y = y_stan, colour = IN_PPI ) ) +
  facet_grid(item_label ~ time_label, scales = 'free') +
  scale_x_discrete() +
  scale_y_continuous() +
  ggsci::scale_color_npg() +
  labs(x = '', 
       y = 'outcome', 
       colour = 'within\n95% posterior\nprediction\ninterval') +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 45,vjust = 1,hjust = 1))
ggsave(file = file.path(dir.out, paste0(file.prefix,'ppcheck.png')), 
       p, 
       w = 20, 
       h = 20)
```

```{r mlf4all_extract_class_probs_by_question, include=FALSE, eval=TRUE, echo=FALSE, tidy=FALSE, cache=TRUE}
po <- ocat_lfm_m4all_fit$draws(
  variables = c('cat_ordered_prob_by_obs'),
  inc_warmup = FALSE,
  format = "draws_df"
  )
po <- as.data.table(po)
po <- data.table::melt(po, id.vars = c('.draw','.chain','.iteration'), value.name = 'prob')
po[, y_stan := as.integer(gsub('([a-z_]+)\\[([0-9]),([0-9]+)\\]','\\2',variable)) ]
po[, oidt := as.integer(gsub('([a-z_]+)\\[([0-9]),([0-9]+)\\]','\\3',variable)) ]
set(po, NULL, 'variable', NULL)
tmp <- unique(subset(dcat5[item_type == 'categorical',], select = c(oidt, pid, item_id, time )))
po <- merge(po, tmp, by = c('oidt'))
po <- po[, list(prob = mean(prob)), by = c('.draw','time','item_id','y_stan')]
pos <- po[, 
          list( summary_value = quantile(prob, prob = c(0.025, 0.25, 0.5, 0.75, 0.975)),
                summary_name = c('q_lower','iqr_lower','median','iqr_upper','q_upper') 
                ), 
          by = c('time','item_id','y_stan')
          ]
pos <- data.table::dcast(pos, time + item_id + y_stan ~ summary_name, value.var = 'summary_value')
tmp <- unique( subset(dcat5[item_type == 'categorical',], select = c(time, time_label)) )
pos <- merge(pos, tmp, by = c('time'))
tmp <- unique( subset(dcat5[item_type == 'categorical',], select = c(item_id, item_label)) )
pos <- merge(pos, tmp, by = c('item_id'))
tmp <- unique( subset(dcat5[item_type == 'categorical',], select = c(y, y_stan, y_label)) )
pos <- merge(pos, tmp, by = c('y_stan'))
tmp <- dcat5[item_type == 'categorical', list(n = length(pid)), by = c('time_label','item_label','y_label')]
tmp2 <- tmp[, list(total = sum(n)), by = c('time_label','item_label')]
tmp <- merge(tmp, tmp2, by = c('time_label','item_label'))
tmp[, p_emp := n/total]
pos <- merge(pos, subset(tmp, select = -c(n, total)), by = c('time_label','item_label','y_label'), all.x = TRUE)
set(pos, pos[, which(is.na(p_emp))], 'p_emp', 0.)
pos[, y_stan := NULL]
pos.cat <- copy(pos)


po <- ocat_lfm_m4all_fit$draws(
  variables = c('cat2_ordered_prob_by_obs'),
  inc_warmup = FALSE,
  format = "draws_df"
  )
po <- as.data.table(po)
po <- data.table::melt(po, id.vars = c('.draw','.chain','.iteration'), value.name = 'prob')
po[, y_stan := as.integer(gsub('([a-z0-9_]+)\\[([0-9]),([0-9]+)\\]','\\2',variable)) ]
po[, oidt := as.integer(gsub('([a-z0-9_]+)\\[([0-9]),([0-9]+)\\]','\\3',variable)) ]
set(po, NULL, 'variable', NULL)
tmp <- unique(subset(dcat5[item_type == 'out-of-7',], select = c(oidt, pid, item_id, time )))
po <- merge(po, tmp, by = c('oidt'))
po <- po[, list(prob = mean(prob)), by = c('.draw','time','item_id','y_stan')]
pos <- po[, 
          list( summary_value = quantile(prob, prob = c(0.025, 0.25, 0.5, 0.75, 0.975)),
                summary_name = c('q_lower','iqr_lower','median','iqr_upper','q_upper') 
                ), 
          by = c('time','item_id','y_stan')
          ]
pos <- data.table::dcast(pos, time + item_id + y_stan ~ summary_name, value.var = 'summary_value')
tmp <- unique( subset(dcat5[item_type == 'out-of-7',], select = c(time, time_label)) )
pos <- merge(pos, tmp, by = c('time'))
tmp <- unique( subset(dcat5[item_type == 'out-of-7',], select = c(item_id, item_label)) )
pos <- merge(pos, tmp, by = c('item_id'))
tmp <- unique( subset(dcat5[item_type == 'out-of-7',], select = c(y, y_stan, y_label)) )
pos <- merge(pos, tmp, by = c('y_stan'))
tmp <- dcat5[item_type == 'out-of-7', list(n = length(pid)), by = c('time_label','item_label','y_label')]
tmp2 <- tmp[, list(total = sum(n)), by = c('time_label','item_label')]
tmp <- merge(tmp, tmp2, by = c('time_label','item_label'))
tmp[, p_emp := n/total]
pos <- merge(pos, subset(tmp, select = -c(n, total)), by = c('time_label','item_label','y_label'), all.x = TRUE)
set(pos, pos[, which(is.na(p_emp))], 'p_emp', 0.)
pos[, y_stan := NULL]
pos.cat2 <- copy(pos)

pos <- rbind(pos.cat, pos.cat2)
pos[, group_label := gsub('([^_]+)_([^_]+)','\\1',item_label)]

pal <- colorRampPalette(ggsci::pal_futurama("planetexpress")(12))(pos[, length(unique(item_label))])


p <- ggplot(pos, aes(x = y_label, group = interaction(item_label, y_label))) +
  geom_col(aes(fill = item_label, y = p_emp), 
           position = position_dodge(width = 0.9, preserve = "single"), 
           alpha = 0.8,
           width = 0.8 ) +
  geom_boxplot( aes(ymin = q_lower, lower = iqr_lower, middle = median, upper = iqr_upper, ymax = q_upper), 
                position = position_dodge(0.9, preserve = "single"),
                stat = 'identity',
                alpha = 0,
                width = 0.3) +
  scale_y_continuous(labels = scales::percent) +
  scale_fill_manual(values = pal) +
  facet_wrap(group_label ~ time_label, scales = "free_x", ncol = 2) +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 45,vjust = 1,hjust = 1),
        legend.position = 'bottom'
        ) +
  labs(x = '', y = 'proportion of outcomes', fill = 'survey items')
ggsave(file = file.path(dir.out, paste0(file.prefix,'probs_barplot_v2.png')), 
       plot = p, 
       h = 40, 
       w = 12
       )
```

## Fifth model: remove factor baselines due to sign switching

Here is the Stan code:

```{r mlf5all_stan_code, include=TRUE, eval=TRUE, echo=TRUE, tidy=FALSE, cache=TRUE}
ocat_lfm_m5_txt <- "
functions
{
  vector lf_get_eta( array [] vector beta_questions, 
                     array [] vector loadings_questions,
                     array [] int group_of_obs,
                     array [] int question_of_obs,
                     array [] int unit_of_obs,
                     real latent_factor_unit_sd,
                     vector latent_factor_unit
                   )
  {
    int N = num_elements(unit_of_obs);
    vector [N] eta;
    eta = latent_factor_unit_sd * latent_factor_unit[unit_of_obs];
    for(n in 1:N)
    {
      eta[n] 
        = 
        beta_questions[group_of_obs[n]][question_of_obs[n]] + 
        loadings_questions[group_of_obs[n]][question_of_obs[n]] * eta[n];
    }
    return eta;
  }
}
data
{
  int<lower=1> Ncat; // number of ordered categorial observations
  int<lower=2> Qcat; // number of ordered categorial questions
  int<lower=3> Kcat; // number of categories
  int<lower=1> Ncat2; 
  int<lower=2> Qcat2; 
  int<lower=3> Kcat2; 
  int<lower=2> P; // number of groups
  int<lower=1> U; // number of units
  array [Ncat] int<lower=1, upper=Kcat> ycat; // observations
  array [Ncat] int<lower=1,upper=U> unit_of_cat_obs;
  array [Ncat] int<lower=1,upper=P> group_of_cat_obs;
  array [Ncat] int<lower=1,upper=Qcat> question_of_cat_obs;
  array [Ncat2] int<lower=1, upper=Kcat2> ycat2; // observations
  array [Ncat2] int<lower=1,upper=U> unit_of_cat2_obs;
  array [Ncat2] int<lower=1,upper=P> group_of_cat2_obs;
  array [Ncat2] int<lower=1,upper=Qcat2> question_of_cat2_obs;
}
transformed data
{
    real s2z_sd_unit;
    real s2z_sd_cat_questions;
    real s2z_sd_cat2_questions;
    s2z_sd_unit = inv(sqrt(1. - inv(U)));
    s2z_sd_cat_questions = inv(sqrt(1. - inv(Qcat)));
    s2z_sd_cat2_questions = inv(sqrt(1. - inv(Qcat2)));
}
parameters
{
  sum_to_zero_vector[U] latent_factor_unit;
  real<lower=0> latent_factor_unit_sd;
  array [P] sum_to_zero_vector[Qcat] beta_cat_questions;
  array [P] vector<lower=0>[Qcat-1] loadings_cat_questions_m1;
  real cat_cut_point_1;
  matrix<lower=0>[Kcat - 2,P] cat_cut_point_gaps_groups;
  array [P] sum_to_zero_vector[Qcat2] beta_cat2_questions;
  array [P] vector<lower=0>[Qcat2-1] loadings_cat2_questions_m1;
  real cat2_cut_point_1;
  matrix<lower=0>[Kcat2 - 2,P] cat2_cut_point_gaps_groups;
}
transformed parameters
{
  array [P] ordered[Kcat-1] cat_cut_points;
  array [P] vector<lower=0>[Qcat] loadings_cat_questions;
  array [P] ordered[Kcat2-1] cat2_cut_points;
  array [P] vector<lower=0>[Qcat2] loadings_cat2_questions;
  vector [Ncat] cat_eta;
  vector [Ncat2] cat2_eta;
  
  for (p in 1:P)
  {
    cat_cut_points[p] = 
      append_row(
        rep_vector(cat_cut_point_1, 1), 
        rep_vector(cat_cut_point_1, Kcat-2) + cumulative_sum(cat_cut_point_gaps_groups[,p])
        );
    loadings_cat_questions[p] = append_row(1.0, loadings_cat_questions_m1[p]);
    cat2_cut_points[p] = 
      append_row(
        rep_vector(cat2_cut_point_1, 1), 
        rep_vector(cat2_cut_point_1, Kcat2-2) + cumulative_sum(cat2_cut_point_gaps_groups[,p])
        );
        
    loadings_cat2_questions[p] = append_row(1.0, loadings_cat2_questions_m1[p]);
  }
  
  cat_eta
        = 
        lf_get_eta( beta_cat_questions, 
                    loadings_cat_questions,
                    group_of_cat_obs,
                    question_of_cat_obs,
                    unit_of_cat_obs,
                    latent_factor_unit_sd,
                    latent_factor_unit
                    );
  cat2_eta
        = 
        lf_get_eta( beta_cat2_questions, 
                    loadings_cat2_questions,
                    group_of_cat2_obs,
                    question_of_cat2_obs,
                    unit_of_cat2_obs,
                    latent_factor_unit_sd,
                    latent_factor_unit
                    );
  
}
model
{
  // ordered cat likelihood, cannot be vectorized, for cat obs
  for (n in 1:Ncat) 
  {
    ycat[n] ~ ordered_logistic(cat_eta[n], cat_cut_points[group_of_cat_obs[n]]);
  }
  for (n in 1:Ncat2) 
  {
    ycat2[n] ~ ordered_logistic(cat2_eta[n], cat2_cut_points[group_of_cat2_obs[n]]);
  }
  
  // priors for latent factors
  latent_factor_unit ~ normal(0, s2z_sd_unit); 
  latent_factor_unit_sd ~ exponential(2); 
  
  // priors for cat loadings
  cat_cut_point_1 ~ normal(0, 3.5); 
  to_vector(cat_cut_point_gaps_groups) ~ normal(0, 2.5); 
  for (p in 1:P)
  {
    beta_cat_questions[p] ~ normal(0, s2z_sd_cat_questions); 
    loadings_cat_questions_m1[p] ~ lognormal(0,1);
  }
  cat2_cut_point_1 ~ normal(0, 3.5); 
  to_vector(cat2_cut_point_gaps_groups) ~ normal(0, 2.5); 
  for (p in 1:P)
  {
    beta_cat2_questions[p] ~ normal(0, s2z_sd_cat2_questions); 
    loadings_cat2_questions_m1[p] ~ lognormal(0,1);
  }
}
generated quantities
{
  vector[U] latent_factor_unit_re;
  array [Kcat,Ncat] real<lower=0, upper=1> cat_ordered_prob_by_obs;
  array [Kcat2,Ncat2] real<lower=0, upper=1> cat2_ordered_prob_by_obs;
  array [Ncat] int<lower=0> cat_ypred;
  array [Ncat2] int<lower=0> cat2_ypred;
  array [Ncat+Ncat2] real log_lik;
  
  latent_factor_unit_re = latent_factor_unit_sd * latent_factor_unit;
  
  for (n in 1:Ncat) 
  {
    cat_ordered_prob_by_obs[1,n]          = 1 - inv_logit( cat_eta[n] - cat_cut_points[group_of_cat_obs[n]][1] );
    cat_ordered_prob_by_obs[2:(Kcat-1),n] = to_array_1d( inv_logit( cat_eta[n] - cat_cut_points[group_of_cat_obs[n]][1:(Kcat-2)] ) 
                                                         - 
                                                         inv_logit( cat_eta[n] - cat_cut_points[group_of_cat_obs[n]][2:(Kcat-1)] )
                                                         );
    cat_ordered_prob_by_obs[Kcat,n]       = inv_logit( cat_eta[n] - cat_cut_points[group_of_cat_obs[n]][Kcat-1] );
  }
  
  for (n in 1:Ncat2) 
  {
    cat2_ordered_prob_by_obs[1,n]           = 1 - inv_logit( cat2_eta[n] - cat2_cut_points[group_of_cat2_obs[n]][1] );
    cat2_ordered_prob_by_obs[2:(Kcat2-1),n] = to_array_1d( inv_logit( cat2_eta[n] - cat2_cut_points[group_of_cat2_obs[n]][1:(Kcat2-2)] ) 
                                                           - 
                                                           inv_logit( cat2_eta[n] - cat2_cut_points[group_of_cat2_obs[n]][2:(Kcat2-1)] )
                                                           );
    cat2_ordered_prob_by_obs[Kcat2,n]       = inv_logit( cat2_eta[n] - cat2_cut_points[group_of_cat2_obs[n]][Kcat2-1] );
  }
  
  for( n in 1:Ncat ) 
  {
    cat_ypred[n] = ordered_logistic_rng( cat_eta[n], cat_cut_points[group_of_cat_obs[n]] ); 
  }
  for( n in 1:Ncat2 ) 
  {
    cat2_ypred[n] = ordered_logistic_rng( cat2_eta[n], cat2_cut_points[group_of_cat2_obs[n]] ); 
  }
  
  for(n in 1:Ncat)
  {
    log_lik[n] =  ordered_logistic_lpmf( ycat[n] | cat_eta[n], cat_cut_points[group_of_cat_obs[n]] );
  }
  for(n in 1:Ncat2)
  {
    log_lik[ Ncat+n ] = ordered_logistic_lpmf( ycat2[n] | cat2_eta[n], cat2_cut_points[group_of_cat2_obs[n]] );
  }
}
"
# compile the model
ocat_lfm_m5_filename <- cmdstanr::write_stan_file(
  gsub('\t',' ', ocat_lfm_m5_txt),
  dir = dir.out,
  basename = NULL,
  force_overwrite = FALSE,
  hash_salt = ""
)

# compile Stan model
ocat_lfm_m5_compiled <- cmdstanr::cmdstan_model(ocat_lfm_m5_filename)
```
```{r mlf5all_prepare data, include=TRUE, eval=TRUE, echo=TRUE, message=FALSE, tidy=FALSE, cache=TRUE}
file.prefix <- 'cg_chi_all_lf5_'
dcat5 <- copy(dp1)
dcat5[, time := time + 1L]
dcat5[, y_stan := y + 1L]
tmp <- data.table(item_label = sort(unique(dcat5$item_label)))
tmp[, item_type := ifelse(grepl('CG-MH',item_label), 'categorical', 'out-of-7')]
tmp <- tmp[, list(item_label = item_label, item_id = 1:length(item_label)), by = 'item_type']
dcat5 <- merge(dcat5, tmp, by = 'item_label')
setkey(dcat5, pid, time, item_label)
dcat5[, oid := 1:nrow(dcat5)]
tmp <- dcat5[, list(oid = oid, oidt = 1:length(y_stan)), by = 'item_type' ]
dcat5 <- merge(dcat5, tmp, by  = c('item_type','oid'))
setkey(dcat5, pid, time, item_label)
```

```{r mlf5all_run_model, include=TRUE, eval=TRUE, echo=TRUE, message=FALSE, tidy=FALSE, cache=TRUE}
# define data in format needed for model specification
stan_data <- list()
stan_data$P <- max(dcat5$time)
stan_data$U <- max(dcat5$pid)
stan_data$Ncat <- nrow(dcat5[item_type == 'categorical',])
stan_data$Qcat <- max(dcat5[item_type == 'categorical', item_id])
stan_data$Kcat <- length(unique(dcat5[item_type == 'categorical', y_stan]))
stan_data$ycat <- dcat5[item_type == 'categorical', y_stan]
stan_data$group_of_cat_obs <- dcat5[item_type == 'categorical', time]
stan_data$question_of_cat_obs <- dcat5[item_type == 'categorical', item_id]
stan_data$unit_of_cat_obs <- dcat5[item_type == 'categorical', pid]
stan_data$Ncat2 <- nrow(dcat5[item_type == 'out-of-7',])
stan_data$Qcat2 <- max(dcat5[item_type == 'out-of-7', item_id])
stan_data$Kcat2 <- length(unique(dcat5[item_type == 'out-of-7', y_stan]))
stan_data$ycat2 <- dcat5[item_type == 'out-of-7', y_stan]
stan_data$group_of_cat2_obs <- dcat5[item_type == 'out-of-7', time]
stan_data$question_of_cat2_obs <- dcat5[item_type == 'out-of-7', item_id]
stan_data$unit_of_cat2_obs <- dcat5[item_type == 'out-of-7', pid]

# sample
ocat_lfm_m5all_fit <- ocat_lfm_m5_compiled$sample(
  data = stan_data,
  seed = 123,
  chains = 2,
  parallel_chains = 2,
  iter_warmup = 5e2,
  iter_sampling = 15e2,
  refresh = 500,
  save_warmup = TRUE
)

# save output to RDS
ocat_lfm_m5all_fit$save_object(file = file.path(dir.out, paste0(file.prefix,"stan.rds")))
```


```{r mlf5all_load_model, include=FALSE, eval=TRUE, echo=FALSE, tidy=FALSE, cache=TRUE}
ocat_lfm_m5all_fit <- readRDS(file = file.path(dir.out, paste0(file.prefix,'stan.rds')))
```

```{r mlf5all_check_mixing_convergence, include=FALSE, eval=TRUE, echo=FALSE, tidy=FALSE, cache=TRUE}
#	check convergence and check mixing
#	get 95% credible intervals
tmp <- ocat_lfm_m5all_fit$summary(
  variables = c('latent_factor_unit','latent_factor_unit_sd',
                'beta_cat_questions','loadings_cat_questions_m1','cat_cut_point_1','cat_cut_point_gaps_groups',
                'beta_cat2_questions','loadings_cat2_questions_m1','cat2_cut_point_1','cat2_cut_point_gaps_groups'
                ),
  posterior::default_summary_measures(),
  posterior::default_convergence_measures(),
  extra_quantiles = ~posterior::quantile2(., probs = c(.0275, .975))
)
tmp <- as.data.table(tmp)
tmp <- tmp[order(ess_bulk),]
tmp
  
# worst 14 parameters with lowest ess_bulk
worst_var <- tmp$variable[ 1:14 ]

# extract samples
po <- ocat_lfm_m4all_fit$draws(
  variables = c("lp__", worst_var),
  inc_warmup = TRUE,
  format = "draws_array"
  )

# make worst trace plot
p <- bayesplot:::mcmc_trace(po,  
                            pars = c("lp__",worst_var), 
                            n_warmup = 500,
                            facet_args = list(nrow = 3)
                            )
p <- p + theme_bw()
ggsave(file = file.path(dir.out, paste0(file.prefix,'worsttrace.png')), 
       plot = p, 
       h = 10, 
       w = 30
       )

# make intervals/areas plot
po <- ocat_lfm_m5all_fit$draws(
  variables = c('latent_factor_unit','latent_factor_unit_sd',
                'beta_cat_questions','loadings_cat_questions_m1','cat_cut_point_1','cat_cut_point_gaps_groups',
                'beta_cat2_questions','loadings_cat2_questions_m1','cat2_cut_point_1','cat2_cut_point_gaps_groups'
                ),
  inc_warmup = FALSE,
  format = "draws_array"
  )

color_scheme_set("teal")
p <- bayesplot::mcmc_intervals(po, prob = 0.5, prob_outer = 0.95, outer_size = 1, point_size = 2) +
  theme_bw()
ggsave(file = file.path(dir.out, paste0(file.prefix,'intervals.png')), 
       plot = p, 
       h = 50, 
       w = 8, 
       limitsize = FALSE
       )

  
p <- bayesplot::mcmc_areas(po, prob = 0.5, prob_outer = 0.95, point_est = 'median') +
  theme_bw()
ggsave(file = file.path(dir.out, paste0(file.prefix,'areas.png')), 
       plot = p, 
       h = 150, 
       w = 8, 
       limitsize = FALSE
       )


# make posterior predictive check

# create median and 95\% credible intervals
po <- ocat_lfm_m5all_fit$draws(
  variables = c('cat_ypred','cat2_ypred'),
  inc_warmup = FALSE,
  format = "draws_df"
  )
po <- as.data.table(po)
po <- data.table::melt(po, id.vars = c('.draw','.chain','.iteration'), value.name = 'ypred')
po[, item_type := gsub('([a-z0-9_]+)\\[([0-9]+)\\]','\\1',variable) ]
po[, oidt := as.integer(gsub('([a-z0-9_]+)\\[([0-9]+)\\]','\\2',variable)) ]
tmp <- po[, which(grepl('cat2',item_type))]
set(po, tmp, 'item_type', 'out-of-7')
set(po, po[, which(grepl('ypred',item_type))], 'item_type', 'categorical')  

pos <- 
  po[,
     list( summary_value = quantile(ypred, prob = c(0.025, 0.25, 0.5, 0.75, 0.975)),
           summary_name = c('q_lower','iqr_lower','median','iqr_upper','q_upper') 
           ),
     by = c('item_type','oidt')
     ]
pos <- 
  data.table::dcast(pos,
                    item_type + oidt ~ summary_name, 
                    value.var = 'summary_value'
                    )

pos <- merge(pos, dcat5, by = c('item_type','oidt'))
pos[, IN_PPI := y_stan >= q_lower & y_stan <= q_upper]
pos[, mean(IN_PPI)]

# plot posterior predictive check
p <- ggplot(pos, aes(x = oid, group = oid)) + 
  geom_boxplot( aes( ymin = q_lower,
                     lower = iqr_lower,
                     middle = median,
                     upper = iqr_upper,
                     ymax = q_upper),
                stat = 'identity') +
  geom_point( aes(y = y_stan, colour = IN_PPI ) ) +
  facet_grid(item_label ~ time_label, scales = 'free') +
  scale_x_discrete() +
  scale_y_continuous() +
  ggsci::scale_color_npg() +
  labs(x = '', 
       y = 'outcome', 
       colour = 'within\n95% posterior\nprediction\ninterval') +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 45,vjust = 1,hjust = 1))
ggsave(file = file.path(dir.out, paste0(file.prefix,'ppcheck.png')), 
       p, 
       w = 20, 
       h = 20)
```

```{r mlf5all_extract_class_probs_by_question, include=FALSE, eval=TRUE, echo=FALSE, tidy=FALSE, cache=TRUE}
po <- ocat_lfm_m5all_fit$draws(
  variables = c('cat_ordered_prob_by_obs'),
  inc_warmup = FALSE,
  format = "draws_df"
  )
po <- as.data.table(po)
po <- data.table::melt(po, id.vars = c('.draw','.chain','.iteration'), value.name = 'prob')
po[, y_stan := as.integer(gsub('([a-z_]+)\\[([0-9]),([0-9]+)\\]','\\2',variable)) ]
po[, oidt := as.integer(gsub('([a-z_]+)\\[([0-9]),([0-9]+)\\]','\\3',variable)) ]
set(po, NULL, 'variable', NULL)
tmp <- unique(subset(dcat5[item_type == 'categorical',], select = c(oidt, pid, item_id, time )))
po <- merge(po, tmp, by = c('oidt'))
po <- po[, list(prob = mean(prob)), by = c('.draw','time','item_id','y_stan')]
pos <- po[, 
          list( summary_value = quantile(prob, prob = c(0.025, 0.25, 0.5, 0.75, 0.975)),
                summary_name = c('q_lower','iqr_lower','median','iqr_upper','q_upper') 
                ), 
          by = c('time','item_id','y_stan')
          ]
pos <- data.table::dcast(pos, time + item_id + y_stan ~ summary_name, value.var = 'summary_value')
tmp <- unique( subset(dcat5[item_type == 'categorical',], select = c(time, time_label)) )
pos <- merge(pos, tmp, by = c('time'))
tmp <- unique( subset(dcat5[item_type == 'categorical',], select = c(item_id, item_label)) )
pos <- merge(pos, tmp, by = c('item_id'))
tmp <- unique( subset(dcat5[item_type == 'categorical',], select = c(y, y_stan, y_label)) )
pos <- merge(pos, tmp, by = c('y_stan'))
tmp <- dcat5[item_type == 'categorical', list(n = length(pid)), by = c('time_label','item_label','y_label')]
tmp2 <- tmp[, list(total = sum(n)), by = c('time_label','item_label')]
tmp <- merge(tmp, tmp2, by = c('time_label','item_label'))
tmp[, p_emp := n/total]
pos <- merge(pos, subset(tmp, select = -c(n, total)), by = c('time_label','item_label','y_label'), all.x = TRUE)
set(pos, pos[, which(is.na(p_emp))], 'p_emp', 0.)
pos[, y_stan := NULL]
pos.cat <- copy(pos)


po <- ocat_lfm_m5all_fit$draws(
  variables = c('cat2_ordered_prob_by_obs'),
  inc_warmup = FALSE,
  format = "draws_df"
  )
po <- as.data.table(po)
po <- data.table::melt(po, id.vars = c('.draw','.chain','.iteration'), value.name = 'prob')
po[, y_stan := as.integer(gsub('([a-z0-9_]+)\\[([0-9]),([0-9]+)\\]','\\2',variable)) ]
po[, oidt := as.integer(gsub('([a-z0-9_]+)\\[([0-9]),([0-9]+)\\]','\\3',variable)) ]
set(po, NULL, 'variable', NULL)
tmp <- unique(subset(dcat5[item_type == 'out-of-7',], select = c(oidt, pid, item_id, time )))
po <- merge(po, tmp, by = c('oidt'))
po <- po[, list(prob = mean(prob)), by = c('.draw','time','item_id','y_stan')]
pos <- po[, 
          list( summary_value = quantile(prob, prob = c(0.025, 0.25, 0.5, 0.75, 0.975)),
                summary_name = c('q_lower','iqr_lower','median','iqr_upper','q_upper') 
                ), 
          by = c('time','item_id','y_stan')
          ]
pos <- data.table::dcast(pos, time + item_id + y_stan ~ summary_name, value.var = 'summary_value')
tmp <- unique( subset(dcat5[item_type == 'out-of-7',], select = c(time, time_label)) )
pos <- merge(pos, tmp, by = c('time'))
tmp <- unique( subset(dcat5[item_type == 'out-of-7',], select = c(item_id, item_label)) )
pos <- merge(pos, tmp, by = c('item_id'))
tmp <- unique( subset(dcat5[item_type == 'out-of-7',], select = c(y, y_stan, y_label)) )
pos <- merge(pos, tmp, by = c('y_stan'))
tmp <- dcat5[item_type == 'out-of-7', list(n = length(pid)), by = c('time_label','item_label','y_label')]
tmp2 <- tmp[, list(total = sum(n)), by = c('time_label','item_label')]
tmp <- merge(tmp, tmp2, by = c('time_label','item_label'))
tmp[, p_emp := n/total]
pos <- merge(pos, subset(tmp, select = -c(n, total)), by = c('time_label','item_label','y_label'), all.x = TRUE)
set(pos, pos[, which(is.na(p_emp))], 'p_emp', 0.)
pos[, y_stan := NULL]
pos.cat2 <- copy(pos)

pos <- rbind(pos.cat, pos.cat2)
pos[, group_label := gsub('([^_]+)_([^_]+)','\\1',item_label)]

pal <- colorRampPalette(ggsci::pal_futurama("planetexpress")(12))(pos[, length(unique(item_label))])


p <- ggplot(pos, aes(x = y_label, group = interaction(item_label, y_label))) +
  geom_col(aes(fill = item_label, y = p_emp), 
           position = position_dodge(width = 0.9, preserve = "single"), 
           alpha = 0.8,
           width = 0.8 ) +
  geom_boxplot( aes(ymin = q_lower, lower = iqr_lower, middle = median, upper = iqr_upper, ymax = q_upper), 
                position = position_dodge(0.9, preserve = "single"),
                stat = 'identity',
                alpha = 0,
                width = 0.3) +
  scale_y_continuous(labels = scales::percent) +
  scale_fill_manual(values = pal) +
  facet_wrap(group_label ~ time_label, scales = "free_x", ncol = 2) +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 45,vjust = 1,hjust = 1),
        legend.position = 'bottom'
        ) +
  labs(x = '', y = 'proportion of outcomes', fill = 'survey items')
ggsave(file = file.path(dir.out, paste0(file.prefix,'probs_barplot_v2.png')), 
       plot = p, 
       h = 40, 
       w = 12
       )
```

## Sixth model: remove factor sd's due to over-parameterisation

Here is the Stan code:

```{r mlf6all_stan_code, include=TRUE, eval=TRUE, echo=TRUE, tidy=FALSE, cache=TRUE}
ocat_lfm_m6_txt <- "
functions
{
  vector lf_get_eta( array [] vector beta_questions, 
                     array [] vector loadings_questions,
                     array [] int group_of_obs,
                     array [] int question_of_obs,
                     array [] int unit_of_obs,
                     vector latent_factor_unit
                   )
  {
    int N = num_elements(unit_of_obs);
    vector [N] eta;
    eta = latent_factor_unit[unit_of_obs];
    for(n in 1:N)
    {
      eta[n] 
        = 
        beta_questions[group_of_obs[n]][question_of_obs[n]] + 
        loadings_questions[group_of_obs[n]][question_of_obs[n]] * eta[n];
    }
    return eta;
  }
}
data
{
  int<lower=1> Ncat; // number of ordered categorial observations
  int<lower=2> Qcat; // number of ordered categorial questions
  int<lower=3> Kcat; // number of categories
  int<lower=1> Ncat2; 
  int<lower=2> Qcat2; 
  int<lower=3> Kcat2; 
  int<lower=2> P; // number of groups
  int<lower=1> U; // number of units
  array [Ncat] int<lower=1, upper=Kcat> ycat; // observations
  array [Ncat] int<lower=1,upper=U> unit_of_cat_obs;
  array [Ncat] int<lower=1,upper=P> group_of_cat_obs;
  array [Ncat] int<lower=1,upper=Qcat> question_of_cat_obs;
  array [Ncat2] int<lower=1, upper=Kcat2> ycat2; // observations
  array [Ncat2] int<lower=1,upper=U> unit_of_cat2_obs;
  array [Ncat2] int<lower=1,upper=P> group_of_cat2_obs;
  array [Ncat2] int<lower=1,upper=Qcat2> question_of_cat2_obs;
}
transformed data
{
    real s2z_sd_unit;
    real s2z_sd_cat_questions;
    real s2z_sd_cat2_questions;
    s2z_sd_unit = inv(sqrt(1. - inv(U)));
    s2z_sd_cat_questions = inv(sqrt(1. - inv(Qcat)));
    s2z_sd_cat2_questions = inv(sqrt(1. - inv(Qcat2)));
}
parameters
{
  sum_to_zero_vector[U] latent_factor_unit;
  array [P] sum_to_zero_vector[Qcat] beta_cat_questions;
  array [P] vector<lower=0>[Qcat-1] loadings_cat_questions_m1;
  real cat_cut_point_1;
  matrix<lower=0>[Kcat - 2,P] cat_cut_point_gaps_groups;
  array [P] sum_to_zero_vector[Qcat2] beta_cat2_questions;
  array [P] vector<lower=0>[Qcat2-1] loadings_cat2_questions_m1;
  real cat2_cut_point_1;
  matrix<lower=0>[Kcat2 - 2,P] cat2_cut_point_gaps_groups;
}
transformed parameters
{
  array [P] ordered[Kcat-1] cat_cut_points;
  array [P] vector<lower=0>[Qcat] loadings_cat_questions;
  array [P] ordered[Kcat2-1] cat2_cut_points;
  array [P] vector<lower=0>[Qcat2] loadings_cat2_questions;
  vector [Ncat] cat_eta;
  vector [Ncat2] cat2_eta;
  
  for (p in 1:P)
  {
    cat_cut_points[p] = 
      append_row(
        rep_vector(cat_cut_point_1, 1), 
        rep_vector(cat_cut_point_1, Kcat-2) + cumulative_sum(cat_cut_point_gaps_groups[,p])
        );
    loadings_cat_questions[p] = append_row(1.0, loadings_cat_questions_m1[p]);
    cat2_cut_points[p] = 
      append_row(
        rep_vector(cat2_cut_point_1, 1), 
        rep_vector(cat2_cut_point_1, Kcat2-2) + cumulative_sum(cat2_cut_point_gaps_groups[,p])
        );
    loadings_cat2_questions[p] = append_row(1.0, loadings_cat2_questions_m1[p]);
  }
  cat_eta
        = 
        lf_get_eta( beta_cat_questions, 
                    loadings_cat_questions,
                    group_of_cat_obs,
                    question_of_cat_obs,
                    unit_of_cat_obs,
                    latent_factor_unit
                    );
  cat2_eta
        = 
        lf_get_eta( beta_cat2_questions, 
                    loadings_cat2_questions,
                    group_of_cat2_obs,
                    question_of_cat2_obs,
                    unit_of_cat2_obs,
                    latent_factor_unit
                    );
}
model
{
  // ordered cat likelihood, cannot be vectorized, for cat obs
  for (n in 1:Ncat) 
  {
    ycat[n] ~ ordered_logistic(cat_eta[n], cat_cut_points[group_of_cat_obs[n]]);
  }
  for (n in 1:Ncat2) 
  {
    ycat2[n] ~ ordered_logistic(cat2_eta[n], cat2_cut_points[group_of_cat2_obs[n]]);
  }
  
  // priors for latent factors
  latent_factor_unit ~ normal(0, s2z_sd_unit); 
  
  // priors for cat loadings
  cat_cut_point_1 ~ normal(0, 3.5); 
  to_vector(cat_cut_point_gaps_groups) ~ normal(0, 2.5); 
  for (p in 1:P)
  {
    beta_cat_questions[p] ~ normal(0, s2z_sd_cat_questions); 
    loadings_cat_questions_m1[p] ~ lognormal(0,1);
  }
  cat2_cut_point_1 ~ normal(0, 3.5); 
  to_vector(cat2_cut_point_gaps_groups) ~ normal(0, 2.5); 
  for (p in 1:P)
  {
    beta_cat2_questions[p] ~ normal(0, s2z_sd_cat2_questions); 
    loadings_cat2_questions_m1[p] ~ lognormal(0,1);
  }
}
generated quantities
{
  array [Kcat,Ncat] real<lower=0, upper=1> cat_ordered_prob_by_obs;
  array [Kcat2,Ncat2] real<lower=0, upper=1> cat2_ordered_prob_by_obs;
  array [Ncat] int<lower=0> cat_ypred;
  array [Ncat2] int<lower=0> cat2_ypred;
  array [Ncat+Ncat2] real log_lik;
  
  for (n in 1:Ncat) 
  {
    cat_ordered_prob_by_obs[1,n]          = 1 - inv_logit( cat_eta[n] - cat_cut_points[group_of_cat_obs[n]][1] );
    cat_ordered_prob_by_obs[2:(Kcat-1),n] = to_array_1d( inv_logit( cat_eta[n] - cat_cut_points[group_of_cat_obs[n]][1:(Kcat-2)] ) 
                                                         - 
                                                         inv_logit( cat_eta[n] - cat_cut_points[group_of_cat_obs[n]][2:(Kcat-1)] )
                                                         );
    cat_ordered_prob_by_obs[Kcat,n]       = inv_logit( cat_eta[n] - cat_cut_points[group_of_cat_obs[n]][Kcat-1] );
  }
  for (n in 1:Ncat2) 
  {
    cat2_ordered_prob_by_obs[1,n]           = 1 - inv_logit( cat2_eta[n] - cat2_cut_points[group_of_cat2_obs[n]][1] );
    cat2_ordered_prob_by_obs[2:(Kcat2-1),n] = to_array_1d( inv_logit( cat2_eta[n] - cat2_cut_points[group_of_cat2_obs[n]][1:(Kcat2-2)] ) 
                                                           - 
                                                           inv_logit( cat2_eta[n] - cat2_cut_points[group_of_cat2_obs[n]][2:(Kcat2-1)] )
                                                           );
    cat2_ordered_prob_by_obs[Kcat2,n]       = inv_logit( cat2_eta[n] - cat2_cut_points[group_of_cat2_obs[n]][Kcat2-1] );
  }
  for( n in 1:Ncat ) 
  {
    cat_ypred[n] = ordered_logistic_rng( cat_eta[n], cat_cut_points[group_of_cat_obs[n]] ); 
  }
  for( n in 1:Ncat2 ) 
  {
    cat2_ypred[n] = ordered_logistic_rng( cat2_eta[n], cat2_cut_points[group_of_cat2_obs[n]] ); 
  }
  for(n in 1:Ncat)
  {
    log_lik[n] =  ordered_logistic_lpmf( ycat[n] | cat_eta[n], cat_cut_points[group_of_cat_obs[n]] );
  }
  for(n in 1:Ncat2)
  {
    log_lik[ Ncat+n ] = ordered_logistic_lpmf( ycat2[n] | cat2_eta[n], cat2_cut_points[group_of_cat2_obs[n]] );
  }
}
"
# compile the model
ocat_lfm_m6_filename <- cmdstanr::write_stan_file(
  gsub('\t',' ', ocat_lfm_m6_txt),
  dir = dir.out,
  basename = NULL,
  force_overwrite = FALSE,
  hash_salt = ""
)

# compile Stan model
ocat_lfm_m6_compiled <- cmdstanr::cmdstan_model(ocat_lfm_m6_filename)
```
```{r mlf6all_prepare data, include=TRUE, eval=TRUE, echo=TRUE, message=FALSE, tidy=FALSE, cache=TRUE}
file.prefix <- 'cg_chi_all_lf6_'
dcat5 <- copy(dp1)
dcat5[, time := time + 1L]
dcat5[, y_stan := y + 1L]
tmp <- data.table(item_label = sort(unique(dcat5$item_label)))
tmp[, item_type := ifelse(grepl('CG-MH',item_label), 'categorical', 'out-of-7')]
tmp <- tmp[, list(item_label = item_label, item_id = 1:length(item_label)), by = 'item_type']
dcat5 <- merge(dcat5, tmp, by = 'item_label')
setkey(dcat5, pid, time, item_label)
dcat5[, oid := 1:nrow(dcat5)]
tmp <- dcat5[, list(oid = oid, oidt = 1:length(y_stan)), by = 'item_type' ]
dcat5 <- merge(dcat5, tmp, by  = c('item_type','oid'))
setkey(dcat5, pid, time, item_label)
```

```{r mlf6all_run_model, include=TRUE, eval=TRUE, echo=TRUE, message=FALSE, tidy=FALSE, cache=TRUE}
# define data in format needed for model specification
stan_data <- list()
stan_data$P <- max(dcat5$time)
stan_data$U <- max(dcat5$pid)
stan_data$Ncat <- nrow(dcat5[item_type == 'categorical',])
stan_data$Qcat <- max(dcat5[item_type == 'categorical', item_id])
stan_data$Kcat <- length(unique(dcat5[item_type == 'categorical', y_stan]))
stan_data$ycat <- dcat5[item_type == 'categorical', y_stan]
stan_data$group_of_cat_obs <- dcat5[item_type == 'categorical', time]
stan_data$question_of_cat_obs <- dcat5[item_type == 'categorical', item_id]
stan_data$unit_of_cat_obs <- dcat5[item_type == 'categorical', pid]
stan_data$Ncat2 <- nrow(dcat5[item_type == 'out-of-7',])
stan_data$Qcat2 <- max(dcat5[item_type == 'out-of-7', item_id])
stan_data$Kcat2 <- length(unique(dcat5[item_type == 'out-of-7', y_stan]))
stan_data$ycat2 <- dcat5[item_type == 'out-of-7', y_stan]
stan_data$group_of_cat2_obs <- dcat5[item_type == 'out-of-7', time]
stan_data$question_of_cat2_obs <- dcat5[item_type == 'out-of-7', item_id]
stan_data$unit_of_cat2_obs <- dcat5[item_type == 'out-of-7', pid]

# sample
ocat_lfm_m6all_fit <- ocat_lfm_m6_compiled$sample(
  data = stan_data,
  seed = 123,
  chains = 2,
  parallel_chains = 2,
  iter_warmup = 5e2,
  iter_sampling = 15e2,
  refresh = 500,
  save_warmup = TRUE
)

# save output to RDS
ocat_lfm_m6all_fit$save_object(file = file.path(dir.out, paste0(file.prefix,"stan.rds")))
```


```{r mlf6all_load_model, include=FALSE, eval=TRUE, echo=FALSE, tidy=FALSE, cache=TRUE}
ocat_lfm_m6all_fit <- readRDS(file = file.path(dir.out, paste0(file.prefix,'stan.rds')))
```

```{r mlf6all_check_mixing_convergence, include=FALSE, eval=TRUE, echo=FALSE, tidy=FALSE, cache=TRUE}
#	check convergence and check mixing
#	get 95% credible intervals
tmp <- ocat_lfm_m6all_fit$summary(
  variables = c('latent_factor_unit',
                'beta_cat_questions','loadings_cat_questions_m1','cat_cut_point_1','cat_cut_point_gaps_groups',
                'beta_cat2_questions','loadings_cat2_questions_m1','cat2_cut_point_1','cat2_cut_point_gaps_groups'
                ),
  posterior::default_summary_measures(),
  posterior::default_convergence_measures(),
  extra_quantiles = ~posterior::quantile2(., probs = c(.0275, .975))
)
tmp <- as.data.table(tmp)
tmp <- tmp[order(ess_bulk),]
tmp
  
# worst 14 parameters with lowest ess_bulk
worst_var <- tmp$variable[ 1:14 ]

# extract samples
po <- ocat_lfm_m6all_fit$draws(
  variables = c("lp__", worst_var),
  inc_warmup = TRUE,
  format = "draws_array"
  )

# make worst trace plot
p <- bayesplot:::mcmc_trace(po,  
                            pars = c("lp__",worst_var), 
                            n_warmup = 500,
                            facet_args = list(nrow = 3)
                            )
p <- p + theme_bw()
ggsave(file = file.path(dir.out, paste0(file.prefix,'worsttrace.png')), 
       plot = p, 
       h = 10, 
       w = 30
       )

# make intervals/areas plot
po <- ocat_lfm_m6all_fit$draws(
  variables = c('latent_factor_unit',
                'beta_cat_questions','loadings_cat_questions_m1','cat_cut_point_1','cat_cut_point_gaps_groups',
                'beta_cat2_questions','loadings_cat2_questions_m1','cat2_cut_point_1','cat2_cut_point_gaps_groups'
                ),
  inc_warmup = FALSE,
  format = "draws_array"
  )

color_scheme_set("teal")
p <- bayesplot::mcmc_intervals(po, prob = 0.5, prob_outer = 0.95, outer_size = 1, point_size = 2) +
  theme_bw()
ggsave(file = file.path(dir.out, paste0(file.prefix,'intervals.png')), 
       plot = p, 
       h = 50, 
       w = 8, 
       limitsize = FALSE
       )

  
p <- bayesplot::mcmc_areas(po, prob = 0.5, prob_outer = 0.95, point_est = 'median') +
  theme_bw()
ggsave(file = file.path(dir.out, paste0(file.prefix,'areas.png')), 
       plot = p, 
       h = 150, 
       w = 8, 
       limitsize = FALSE
       )


# make posterior predictive check

# create median and 95\% credible intervals
po <- ocat_lfm_m6all_fit$draws(
  variables = c('cat_ypred','cat2_ypred'),
  inc_warmup = FALSE,
  format = "draws_df"
  )
po <- as.data.table(po)
po <- data.table::melt(po, id.vars = c('.draw','.chain','.iteration'), value.name = 'ypred')
po[, item_type := gsub('([a-z0-9_]+)\\[([0-9]+)\\]','\\1',variable) ]
po[, oidt := as.integer(gsub('([a-z0-9_]+)\\[([0-9]+)\\]','\\2',variable)) ]
tmp <- po[, which(grepl('cat2',item_type))]
set(po, tmp, 'item_type', 'out-of-7')
set(po, po[, which(grepl('ypred',item_type))], 'item_type', 'categorical')  

pos <- 
  po[,
     list( summary_value = quantile(ypred, prob = c(0.025, 0.25, 0.5, 0.75, 0.975)),
           summary_name = c('q_lower','iqr_lower','median','iqr_upper','q_upper') 
           ),
     by = c('item_type','oidt')
     ]
pos <- 
  data.table::dcast(pos,
                    item_type + oidt ~ summary_name, 
                    value.var = 'summary_value'
                    )

pos <- merge(pos, dcat5, by = c('item_type','oidt'))
pos[, IN_PPI := y_stan >= q_lower & y_stan <= q_upper]
pos[, mean(IN_PPI)]

# plot posterior predictive check
p <- ggplot(pos, aes(x = oid, group = oid)) + 
  geom_boxplot( aes( ymin = q_lower,
                     lower = iqr_lower,
                     middle = median,
                     upper = iqr_upper,
                     ymax = q_upper),
                stat = 'identity') +
  geom_point( aes(y = y_stan, colour = IN_PPI ) ) +
  facet_grid(item_label ~ time_label, scales = 'free') +
  scale_x_discrete() +
  scale_y_continuous() +
  ggsci::scale_color_npg() +
  labs(x = '', 
       y = 'outcome', 
       colour = 'within\n95% posterior\nprediction\ninterval') +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 45,vjust = 1,hjust = 1))
ggsave(file = file.path(dir.out, paste0(file.prefix,'ppcheck.png')), 
       p, 
       w = 20, 
       h = 20)
```

```{r mlf6all_extract_class_probs_by_question, include=FALSE, eval=TRUE, echo=FALSE, tidy=FALSE, cache=TRUE}
po <- ocat_lfm_m6all_fit$draws(
  variables = c('cat_ordered_prob_by_obs'),
  inc_warmup = FALSE,
  format = "draws_df"
  )
po <- as.data.table(po)
po <- data.table::melt(po, id.vars = c('.draw','.chain','.iteration'), value.name = 'prob')
po[, y_stan := as.integer(gsub('([a-z_]+)\\[([0-9]),([0-9]+)\\]','\\2',variable)) ]
po[, oidt := as.integer(gsub('([a-z_]+)\\[([0-9]),([0-9]+)\\]','\\3',variable)) ]
set(po, NULL, 'variable', NULL)
tmp <- unique(subset(dcat5[item_type == 'categorical',], select = c(oidt, pid, item_id, time )))
po <- merge(po, tmp, by = c('oidt'))
po <- po[, list(prob = mean(prob)), by = c('.draw','time','item_id','y_stan')]
pos <- po[, 
          list( summary_value = quantile(prob, prob = c(0.025, 0.25, 0.5, 0.75, 0.975)),
                summary_name = c('q_lower','iqr_lower','median','iqr_upper','q_upper') 
                ), 
          by = c('time','item_id','y_stan')
          ]
pos <- data.table::dcast(pos, time + item_id + y_stan ~ summary_name, value.var = 'summary_value')
tmp <- unique( subset(dcat5[item_type == 'categorical',], select = c(time, time_label)) )
pos <- merge(pos, tmp, by = c('time'))
tmp <- unique( subset(dcat5[item_type == 'categorical',], select = c(item_id, item_label)) )
pos <- merge(pos, tmp, by = c('item_id'))
tmp <- unique( subset(dcat5[item_type == 'categorical',], select = c(y, y_stan, y_label)) )
pos <- merge(pos, tmp, by = c('y_stan'))
tmp <- dcat5[item_type == 'categorical', list(n = length(pid)), by = c('time_label','item_label','y_label')]
tmp2 <- tmp[, list(total = sum(n)), by = c('time_label','item_label')]
tmp <- merge(tmp, tmp2, by = c('time_label','item_label'))
tmp[, p_emp := n/total]
pos <- merge(pos, subset(tmp, select = -c(n, total)), by = c('time_label','item_label','y_label'), all.x = TRUE)
set(pos, pos[, which(is.na(p_emp))], 'p_emp', 0.)
pos[, y_stan := NULL]
pos.cat <- copy(pos)


po <- ocat_lfm_m6all_fit$draws(
  variables = c('cat2_ordered_prob_by_obs'),
  inc_warmup = FALSE,
  format = "draws_df"
  )
po <- as.data.table(po)
po <- data.table::melt(po, id.vars = c('.draw','.chain','.iteration'), value.name = 'prob')
po[, y_stan := as.integer(gsub('([a-z0-9_]+)\\[([0-9]),([0-9]+)\\]','\\2',variable)) ]
po[, oidt := as.integer(gsub('([a-z0-9_]+)\\[([0-9]),([0-9]+)\\]','\\3',variable)) ]
set(po, NULL, 'variable', NULL)
tmp <- unique(subset(dcat5[item_type == 'out-of-7',], select = c(oidt, pid, item_id, time )))
po <- merge(po, tmp, by = c('oidt'))
po <- po[, list(prob = mean(prob)), by = c('.draw','time','item_id','y_stan')]
pos <- po[, 
          list( summary_value = quantile(prob, prob = c(0.025, 0.25, 0.5, 0.75, 0.975)),
                summary_name = c('q_lower','iqr_lower','median','iqr_upper','q_upper') 
                ), 
          by = c('time','item_id','y_stan')
          ]
pos <- data.table::dcast(pos, time + item_id + y_stan ~ summary_name, value.var = 'summary_value')
tmp <- unique( subset(dcat5[item_type == 'out-of-7',], select = c(time, time_label)) )
pos <- merge(pos, tmp, by = c('time'))
tmp <- unique( subset(dcat5[item_type == 'out-of-7',], select = c(item_id, item_label)) )
pos <- merge(pos, tmp, by = c('item_id'))
tmp <- unique( subset(dcat5[item_type == 'out-of-7',], select = c(y, y_stan, y_label)) )
pos <- merge(pos, tmp, by = c('y_stan'))
tmp <- dcat5[item_type == 'out-of-7', list(n = length(pid)), by = c('time_label','item_label','y_label')]
tmp2 <- tmp[, list(total = sum(n)), by = c('time_label','item_label')]
tmp <- merge(tmp, tmp2, by = c('time_label','item_label'))
tmp[, p_emp := n/total]
pos <- merge(pos, subset(tmp, select = -c(n, total)), by = c('time_label','item_label','y_label'), all.x = TRUE)
set(pos, pos[, which(is.na(p_emp))], 'p_emp', 0.)
pos[, y_stan := NULL]
pos.cat2 <- copy(pos)

pos <- rbind(pos.cat, pos.cat2)
pos[, group_label := gsub('([^_]+)_([^_]+)','\\1',item_label)]

pal <- colorRampPalette(ggsci::pal_futurama("planetexpress")(12))(pos[, length(unique(item_label))])


p <- ggplot(pos, aes(x = y_label, group = interaction(item_label, y_label))) +
  geom_col(aes(fill = item_label, y = p_emp), 
           position = position_dodge(width = 0.9, preserve = "single"), 
           alpha = 0.8,
           width = 0.8 ) +
  geom_boxplot( aes(ymin = q_lower, lower = iqr_lower, middle = median, upper = iqr_upper, ymax = q_upper), 
                position = position_dodge(0.9, preserve = "single"),
                stat = 'identity',
                alpha = 0,
                width = 0.3) +
  scale_y_continuous(labels = scales::percent) +
  scale_fill_manual(values = pal) +
  facet_wrap(group_label ~ time_label, scales = "free_x", ncol = 2) +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 45,vjust = 1,hjust = 1),
        legend.position = 'bottom'
        ) +
  labs(x = '', y = 'proportion of outcomes', fill = 'survey items')
ggsave(file = file.path(dir.out, paste0(file.prefix,'probs_barplot_v2.png')), 
       plot = p, 
       h = 40, 
       w = 12
       )
```

## Seventh model: unstructured generalised partial credit model

Here is the Stan code:

```{r upgcm1all_stan_code, include=TRUE, eval=TRUE, echo=TRUE, tidy=FALSE, cache=TRUE}
ugpcm_m1_txt <- "
functions
{
  matrix upgcm_get_skill_thresholds( vector skill_thresholds_1,
                                     matrix skill_thresholds_incs
                                     )
  {
    int Q = size(skill_thresholds_1);
    int Km1 = cols(skill_thresholds_incs) + 1;
    matrix [Q, Km1] skill_thresholds;
    skill_thresholds = rep_matrix(skill_thresholds_1, Km1);
    for(j in 1:Q)
    { 
      skill_thresholds[j,2:Km1] += cumulative_sum( skill_thresholds_incs[j,:] );  
    }
    return skill_thresholds;
  }
  
  matrix upgcm_get_etas(  vector loadings_questions,
                          matrix skill_thresholds,
                          vector latent_factor_obs,
                          array [] int question_of_obs,
                          array [] int unit_of_obs
                          )
  {
    int N = num_elements(unit_of_obs);
    int Q = rows(skill_thresholds);
    int K = cols(skill_thresholds) + 1;
    
    matrix[N, K] etas;
    etas[:,1] = rep_vector(0., N);
    for(n in 1:N)
    {
      etas[n,2:K]  = loadings_questions[question_of_obs[n]] * 
                     ( 
                        latent_factor_obs[n] - skill_thresholds[question_of_obs[n],:]
                     );
    }
    return etas;
  }
  
  matrix upgcm_get_etas(  matrix loadings_questions,
                          matrix skill_thresholds,
                          vector latent_factor_obs,
                          array [] int question_of_obs,
                          array [] int unit_of_obs
                          )
  {
    int N = num_elements(unit_of_obs);
    int Q = rows(skill_thresholds);
    int K = cols(skill_thresholds) + 1;
    
    matrix[N, K] etas;
    etas[:,1] = rep_vector(0., N);
    for(n in 1:N)
    {
      etas[n,2:K]  = loadings_questions[question_of_obs[n], :] .* 
                     ( 
                        latent_factor_obs[n] - skill_thresholds[question_of_obs[n],:]
                     );
    }
    return etas;
  }
}
data
{
  int<lower=1> P; // number of predictors
  int<lower=1> U; // number of units
  
  int<lower=1> Ncat1; // number of ordered categorial observations
  int<lower=2> Qcat1; // number of ordered categorial questions
  int<lower=3> Kcat1; // number of categories
  array [Ncat1] int<lower=1, upper=Kcat1> cat1_y; // observations
  array [Ncat1] int<lower=1,upper=U> cat1_unit_of_obs;
  array [Ncat1] int<lower=1,upper=Qcat1> cat1_question_of_obs;
  matrix [Ncat1,P] cat1_X;
  
  int<lower=1> Ncat2; 
  int<lower=2> Qcat2; 
  int<lower=3> Kcat2; 
  array [Ncat2] int<lower=1, upper=Kcat2> cat2_y; // observations
  array [Ncat2] int<lower=1,upper=U> cat2_unit_of_obs;
  array [Ncat2] int<lower=1,upper=Qcat2> cat2_question_of_obs;
  matrix [Ncat2,P] cat2_X;
}
transformed data
{
    real s2z_sd_unit;
    s2z_sd_unit = inv(sqrt(1. - inv(U)));
}
parameters
{
  sum_to_zero_vector[U] latent_factor_unit;
  vector [P] latent_factor_beta;
  
  vector [Qcat1] cat1_skill_thresholds_1;
  matrix [Qcat1, Kcat1 - 2] cat1_skill_thresholds_incs;
  vector<lower=0>[Qcat1] cat1_loadings_questions;
  
  vector [Qcat2] cat2_skill_thresholds_1;
  matrix [Qcat2, Kcat2 - 2] cat2_skill_thresholds_incs;
  vector<lower=0>[Qcat2] cat2_loadings_questions;
}
transformed parameters
{
  matrix [Ncat1, Kcat1] cat1_eta;
  matrix [Ncat2, Kcat2] cat2_eta;
  
  {
    matrix [Qcat1, Kcat1 - 1] cat1_skill_thresholds;
    matrix [Qcat2, Kcat2 - 1] cat2_skill_thresholds;
    
    cat1_skill_thresholds 
      =
      upgcm_get_skill_thresholds( cat1_skill_thresholds_1,
                                  cat1_skill_thresholds_incs
                                  );
    cat2_skill_thresholds 
      =
      upgcm_get_skill_thresholds( cat2_skill_thresholds_1,
                                  cat2_skill_thresholds_incs
                                  );
  
    cat1_eta
        = 
        upgcm_get_etas( cat1_loadings_questions,
                        cat1_skill_thresholds,
                        latent_factor_unit[cat1_unit_of_obs] + cat1_X * latent_factor_beta,
                        cat1_question_of_obs,
                        cat1_unit_of_obs
                        );
    cat2_eta
        = 
        upgcm_get_etas( cat2_loadings_questions,
                        cat2_skill_thresholds,
                        latent_factor_unit[cat2_unit_of_obs] + cat2_X * latent_factor_beta,
                        cat2_question_of_obs,
                        cat2_unit_of_obs
                        );
  }
}
model
{
  // likelihood under unsummed generalised partial credits model
  for (n in 1:Ncat1) 
  {
    target += categorical_logit_lupmf( cat1_y[n] | cat1_eta[n]' );
  }
  for (n in 1:Ncat2) 
  {
    target += categorical_logit_lupmf( cat2_y[n] | cat2_eta[n]' );
  }
  
  // priors for latent factors
  target += normal_lupdf( latent_factor_unit | 0, s2z_sd_unit ); 
  target += std_normal_lupdf(latent_factor_beta);
  
  // priors for skill thresholds
  target += normal_lupdf( cat1_skill_thresholds_1 | 0, 3.5 ); 
  target += normal_lupdf( to_vector(cat1_skill_thresholds_incs) | 0, 2.5); 
  target += normal_lupdf( cat2_skill_thresholds_1 | 0, 3.5 ); 
  target += normal_lupdf( to_vector(cat2_skill_thresholds_incs) | 0, 2.5); 
  
  // priors for loadings
  target += student_t_lupdf(cat1_loadings_questions | 3, 0, 1);
  target += student_t_lupdf(cat2_loadings_questions | 3, 0, 1);
}
generated quantities
{
  matrix [Ncat1,Kcat1] cat1_ordered_prob_by_obs;
  matrix [Ncat2,Kcat2] cat2_ordered_prob_by_obs;
  array [Ncat1] int<lower=0> cat1_ypred;
  array [Ncat2] int<lower=0> cat2_ypred;
  array [Ncat1+Ncat2] real log_lik;
  
  for (n in 1:Ncat1) 
  {
    cat1_ordered_prob_by_obs[n,:] = softmax( cat1_eta[n]' )';
    cat1_ypred[n] = categorical_logit_rng( cat1_eta[n]' );
  }
  for (n in 1:Ncat2) 
  {
    cat2_ordered_prob_by_obs[n,:] = softmax( cat2_eta[n]' )';
    cat2_ypred[n] = categorical_logit_rng( cat2_eta[n]' ); 
  }
  for(n in 1:Ncat1)
  {
    log_lik[n] =  categorical_logit_lpmf( cat1_y[n] | cat1_eta[n]' );
  }
  for(n in 1:Ncat2)
  {
    log_lik[ Ncat1+n ] = categorical_logit_lpmf( cat2_y[n] | cat2_eta[n]' );
  }
}
"
# compile the model
ugpcm_m1_filename <- cmdstanr::write_stan_file(
  gsub('\t',' ', ugpcm_m1_txt),
  dir = dir.out,
  basename = NULL,
  force_overwrite = FALSE,
  hash_salt = ""
)

# compile Stan model
ugpcm_m1_compiled <- cmdstanr::cmdstan_model(ugpcm_m1_filename)
```

```{r upgcm1all_prepare data, include=TRUE, eval=TRUE, echo=TRUE, message=FALSE, tidy=FALSE, cache=TRUE}
file.prefix <- 'cg_chi_all_upgcm1_'
dcat5 <- copy(dp1)
dcat5[, time := time + 1L]
dcat5[, y_stan := y + 1L]
tmp <- data.table(item_label = sort(unique(dcat5$item_label)))
tmp[, item_type := ifelse(grepl('CG-MH',item_label), 'categorical', 'out-of-7')]
tmp <- tmp[, list(item_label = item_label, item_id = 1:length(item_label)), by = 'item_type']
dcat5 <- merge(dcat5, tmp, by = 'item_label')
setkey(dcat5, pid, time, item_label)
dcat5[, oid := 1:nrow(dcat5)]
tmp <- dcat5[, list(oid = oid, oidt = 1:length(y_stan)), by = 'item_type' ]
dcat5 <- merge(dcat5, tmp, by  = c('item_type','oid'))
setkey(dcat5, pid, time, item_label)
```

```{r upgcm1all_run_model, include=TRUE, eval=TRUE, echo=TRUE, message=FALSE, tidy=FALSE, cache=TRUE}
# define data in format needed for model specification
stan_data <- list()
stan_data$P <- 1L
stan_data$U <- max(dcat5$pid)
stan_data$Ncat1 <- nrow(dcat5[item_type == 'categorical',])
stan_data$Qcat1 <- max(dcat5[item_type == 'categorical', item_id])
stan_data$Kcat1 <- length(unique(dcat5[item_type == 'categorical', y_stan]))
stan_data$cat1_y <- dcat5[item_type == 'categorical', y_stan]
stan_data$cat1_question_of_obs <- dcat5[item_type == 'categorical', item_id]
stan_data$cat1_unit_of_obs <- dcat5[item_type == 'categorical', pid]
stan_data$cat1_X <- as.matrix(dcat5[item_type == 'categorical', time - 1L], ncol = 1)
stan_data$Ncat2 <- nrow(dcat5[item_type == 'out-of-7',])
stan_data$Qcat2 <- max(dcat5[item_type == 'out-of-7', item_id])
stan_data$Kcat2 <- length(unique(dcat5[item_type == 'out-of-7', y_stan]))
stan_data$cat2_y <- dcat5[item_type == 'out-of-7', y_stan]
stan_data$cat2_question_of_obs <- dcat5[item_type == 'out-of-7', item_id]
stan_data$cat2_unit_of_obs <- dcat5[item_type == 'out-of-7', pid]
stan_data$cat2_X <- as.matrix(dcat5[item_type == 'out-of-7', time - 1L], ncol = 1)

# sample
upgcm_m1_all_fit <- ugpcm_m1_compiled$sample(
  data = stan_data,
  seed = 123,
  chains = 2,
  parallel_chains = 2,
  iter_warmup = 5e2,
  iter_sampling = 15e2,
  refresh = 500,
  save_warmup = TRUE
)

# save output to RDS
upgcm_m1_all_fit$save_object(file = file.path(dir.out, paste0(file.prefix,"stan.rds")))
```

```{r upgcm1all_load_model, include=FALSE, eval=TRUE, echo=FALSE, tidy=FALSE, cache=TRUE}
upgcm_m1_all_fit <- readRDS(file = file.path(dir.out, paste0(file.prefix,'stan.rds')))
```

```{r upgcm1all_check_mixing_convergence, include=FALSE, eval=TRUE, echo=FALSE, tidy=FALSE, cache=TRUE}
#	check convergence and check mixing
#	get 95% credible intervals
tmp <- upgcm_m1_all_fit$summary(
  variables = c('latent_factor_unit', 'latent_factor_beta',
                'cat1_skill_thresholds_1','cat1_skill_thresholds_incs','cat1_loadings_questions',
                'cat2_skill_thresholds_1','cat2_skill_thresholds_incs','cat2_loadings_questions'
                ),
  posterior::default_summary_measures(),
  posterior::default_convergence_measures(),
  extra_quantiles = ~posterior::quantile2(., probs = c(.0275, .975))
)
tmp <- as.data.table(tmp)
tmp <- tmp[order(ess_bulk),]
tmp
  
# worst 14 parameters with lowest ess_bulk
worst_var <- tmp$variable[ 1:9 ]

# extract samples
po <- upgcm_m1_all_fit$draws(
  variables = c("lp__", worst_var),
  inc_warmup = TRUE,
  format = "draws_array"
  )

# make worst trace plot
p <- bayesplot:::mcmc_trace(po,  
                            pars = c("lp__",worst_var), 
                            n_warmup = 500,
                            facet_args = list(nrow = 2)
                            )
p <- p + theme_bw()
ggsave(file = file.path(dir.out, paste0(file.prefix,'worsttrace.png')), 
       plot = p, 
       h = 10, 
       w = 20
       )

# make intervals/areas plot
po <- upgcm_m1_all_fit$draws(
  variables = c('latent_factor_unit', 'latent_factor_beta',
                'cat1_skill_thresholds_1','cat1_skill_thresholds_incs','cat1_loadings_questions',
                'cat2_skill_thresholds_1','cat2_skill_thresholds_incs','cat2_loadings_questions'
                ),
  inc_warmup = FALSE,
  format = "draws_array"
  )

color_scheme_set("teal")
p <- bayesplot::mcmc_intervals(po, prob = 0.5, prob_outer = 0.95, outer_size = 1, point_size = 2) +
  theme_bw()
ggsave(file = file.path(dir.out, paste0(file.prefix,'intervals.png')), 
       plot = p, 
       h = 50, 
       w = 8, 
       limitsize = FALSE
       )

  
p <- bayesplot::mcmc_areas(po, prob = 0.5, prob_outer = 0.95, point_est = 'median') +
  theme_bw()
ggsave(file = file.path(dir.out, paste0(file.prefix,'areas.png')), 
       plot = p, 
       h = 150, 
       w = 8, 
       limitsize = FALSE
       )


# make posterior predictive check

# create median and 95\% credible intervals
po <- upgcm_m1_all_fit$draws(
  variables = c('cat1_ypred','cat2_ypred'),
  inc_warmup = FALSE,
  format = "draws_df"
  )
po <- as.data.table(po)
po <- data.table::melt(po, id.vars = c('.draw','.chain','.iteration'), value.name = 'ypred')
po[, item_type := gsub('([a-z0-9_]+)\\[([0-9]+)\\]','\\1',variable) ]
po[, oidt := as.integer(gsub('([a-z0-9_]+)\\[([0-9]+)\\]','\\2',variable)) ]
tmp <- po[, which(grepl('cat2',item_type))]
set(po, tmp, 'item_type', 'out-of-7')
set(po, po[, which(grepl('ypred',item_type))], 'item_type', 'categorical')  

pos <- 
  po[,
     list( summary_value = quantile(ypred, prob = c(0.025, 0.25, 0.5, 0.75, 0.975)),
           summary_name = c('q_lower','iqr_lower','median','iqr_upper','q_upper') 
           ),
     by = c('item_type','oidt')
     ]
pos <- 
  data.table::dcast(pos,
                    item_type + oidt ~ summary_name, 
                    value.var = 'summary_value'
                    )

pos <- merge(pos, dcat5, by = c('item_type','oidt'))
pos[, IN_PPI := y_stan >= q_lower & y_stan <= q_upper]
pos[, mean(IN_PPI)]

# plot posterior predictive check
p <- ggplot(pos, aes(x = oid, group = oid)) + 
  geom_boxplot( aes( ymin = q_lower,
                     lower = iqr_lower,
                     middle = median,
                     upper = iqr_upper,
                     ymax = q_upper),
                stat = 'identity') +
  geom_point( aes(y = y_stan, colour = IN_PPI ) ) +
  facet_grid(item_label ~ time_label, scales = 'free') +
  scale_x_discrete() +
  scale_y_continuous() +
  ggsci::scale_color_npg() +
  labs(x = '', 
       y = 'outcome', 
       colour = 'within\n95% posterior\nprediction\ninterval') +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 45,vjust = 1,hjust = 1))
ggsave(file = file.path(dir.out, paste0(file.prefix,'ppcheck.png')), 
       p, 
       w = 20, 
       h = 20)
```

```{r upgcm1all_extract_class_probs_by_question, include=FALSE, eval=TRUE, echo=FALSE, tidy=FALSE, cache=TRUE}
po <- upgcm_m1_all_fit$draws(
  variables = c('cat1_ordered_prob_by_obs'),
  inc_warmup = FALSE,
  format = "draws_df"
  )
po <- as.data.table(po)
po <- data.table::melt(po, id.vars = c('.draw','.chain','.iteration'), value.name = 'prob')
po[, y_stan := as.integer(gsub('([a-z0-9_]+)\\[([0-9]+),([0-9]+)\\]','\\3',variable)) ]
po[, oidt := as.integer(gsub('([a-z0-9_]+)\\[([0-9]+),([0-9]+)\\]','\\2',variable)) ]
set(po, NULL, 'variable', NULL)
tmp <- unique(subset(dcat5[item_type == 'categorical',], select = c(oidt, pid, item_id, time )))
po <- merge(po, tmp, by = c('oidt'))
po <- po[, list(prob = mean(prob)), by = c('.draw','time','item_id','y_stan')]
pos <- po[, 
          list( summary_value = quantile(prob, prob = c(0.025, 0.25, 0.5, 0.75, 0.975)),
                summary_name = c('q_lower','iqr_lower','median','iqr_upper','q_upper') 
                ), 
          by = c('time','item_id','y_stan')
          ]
pos <- data.table::dcast(pos, time + item_id + y_stan ~ summary_name, value.var = 'summary_value')
tmp <- unique( subset(dcat5[item_type == 'categorical',], select = c(time, time_label)) )
pos <- merge(pos, tmp, by = c('time'))
tmp <- unique( subset(dcat5[item_type == 'categorical',], select = c(item_id, item_label)) )
pos <- merge(pos, tmp, by = c('item_id'))
tmp <- unique( subset(dcat5[item_type == 'categorical',], select = c(y, y_stan, y_label)) )
pos <- merge(pos, tmp, by = c('y_stan'))
tmp <- dcat5[item_type == 'categorical', list(n = length(pid)), by = c('time_label','item_label','y_label')]
tmp2 <- tmp[, list(total = sum(n)), by = c('time_label','item_label')]
tmp <- merge(tmp, tmp2, by = c('time_label','item_label'))
tmp[, p_emp := n/total]
pos <- merge(pos, subset(tmp, select = -c(n, total)), by = c('time_label','item_label','y_label'), all.x = TRUE)
set(pos, pos[, which(is.na(p_emp))], 'p_emp', 0.)
pos[, y_stan := NULL]
pos.cat <- copy(pos)


po <- upgcm_m1_all_fit$draws(
  variables = c('cat2_ordered_prob_by_obs'),
  inc_warmup = FALSE,
  format = "draws_df"
  )
po <- as.data.table(po)
po <- data.table::melt(po, id.vars = c('.draw','.chain','.iteration'), value.name = 'prob')
po[, y_stan := as.integer(gsub('([a-z0-9_]+)\\[([0-9]+),([0-9]+)\\]','\\3',variable)) ]
po[, oidt := as.integer(gsub('([a-z0-9_]+)\\[([0-9]+),([0-9]+)\\]','\\2',variable)) ]
set(po, NULL, 'variable', NULL)
tmp <- unique(subset(dcat5[item_type == 'out-of-7',], select = c(oidt, pid, item_id, time )))
po <- merge(po, tmp, by = c('oidt'))
po <- po[, list(prob = mean(prob)), by = c('.draw','time','item_id','y_stan')]
pos <- po[, 
          list( summary_value = quantile(prob, prob = c(0.025, 0.25, 0.5, 0.75, 0.975)),
                summary_name = c('q_lower','iqr_lower','median','iqr_upper','q_upper') 
                ), 
          by = c('time','item_id','y_stan')
          ]
pos <- data.table::dcast(pos, time + item_id + y_stan ~ summary_name, value.var = 'summary_value')
tmp <- unique( subset(dcat5[item_type == 'out-of-7',], select = c(time, time_label)) )
pos <- merge(pos, tmp, by = c('time'))
tmp <- unique( subset(dcat5[item_type == 'out-of-7',], select = c(item_id, item_label)) )
pos <- merge(pos, tmp, by = c('item_id'))
tmp <- unique( subset(dcat5[item_type == 'out-of-7',], select = c(y, y_stan, y_label)) )
pos <- merge(pos, tmp, by = c('y_stan'))
tmp <- dcat5[item_type == 'out-of-7', list(n = length(pid)), by = c('time_label','item_label','y_label')]
tmp2 <- tmp[, list(total = sum(n)), by = c('time_label','item_label')]
tmp <- merge(tmp, tmp2, by = c('time_label','item_label'))
tmp[, p_emp := n/total]
pos <- merge(pos, subset(tmp, select = -c(n, total)), by = c('time_label','item_label','y_label'), all.x = TRUE)
set(pos, pos[, which(is.na(p_emp))], 'p_emp', 0.)
pos[, y_stan := NULL]
pos.cat2 <- copy(pos)

pos <- rbind(pos.cat, pos.cat2)
pos[, group_label := gsub('([^_]+)_([^_]+)','\\1',item_label)]

pal <- colorRampPalette(ggsci::pal_futurama("planetexpress")(12))(pos[, length(unique(item_label))])


p <- ggplot(pos, aes(x = y_label, group = interaction(item_label, y_label))) +
  geom_col(aes(fill = item_label, y = p_emp), 
           position = position_dodge(width = 0.9, preserve = "single"), 
           alpha = 0.8,
           width = 0.8 ) +
  geom_boxplot( aes(ymin = q_lower, lower = iqr_lower, middle = median, upper = iqr_upper, ymax = q_upper), 
                position = position_dodge(0.9, preserve = "single"),
                stat = 'identity',
                alpha = 0,
                width = 0.3) +
  scale_y_continuous(labels = scales::percent) +
  scale_fill_manual(values = pal) +
  facet_wrap(group_label ~ time_label, scales = "free_x", ncol = 2) +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 45,vjust = 1,hjust = 1),
        legend.position = 'bottom'
        ) +
  labs(x = '', y = 'proportion of outcomes', fill = 'survey items')
ggsave(file = file.path(dir.out, paste0(file.prefix,'probs_barplot_v2.png')), 
       plot = p, 
       h = 40, 
       w = 12
       )
```

## Apply UGPCM to question x time

```{r upgcm1all2_prepare data, include=TRUE, eval=TRUE, echo=TRUE, message=FALSE, tidy=FALSE, cache=TRUE}
file.prefix <- 'cg_chi_all2_upgcm1_'
dcat6 <- copy(dp1)
dcat6[, time := time + 1L]
dcat6[, y_stan := y + 1L]
tmp <- CJ(item_label = sort(unique(dcat6$item_label)),
          time = unique(dcat6$time)
          )
tmp[, item_type := ifelse(grepl('CG-MH',item_label), 'categorical', 'out-of-7')]
tmp <- tmp[, list(item_label = item_label, time = time, item_time_id = 1:length(item_label)), by = 'item_type']
dcat6 <- merge(dcat6, tmp, by = c('item_label','time'))
setkey(dcat6, pid, time, item_label)
dcat6[, oid := 1:nrow(dcat6)]
tmp <- dcat6[, list(oid = oid, oidt = 1:length(y_stan)), by = 'item_type' ]
dcat6 <- merge(dcat6, tmp, by  = c('item_type','oid'))
setkey(dcat6, pid, time, item_label)
```

```{r upgcm1all_run_model, include=TRUE, eval=TRUE, echo=TRUE, message=FALSE, tidy=FALSE, cache=TRUE}
# define data in format needed for model specification
stan_data <- list()
stan_data$P <- 1L
stan_data$U <- max(dcat6$pid)
stan_data$Ncat1 <- nrow(dcat6[item_type == 'categorical',])
stan_data$Qcat1 <- max(dcat6[item_type == 'categorical', item_time_id])
stan_data$Kcat1 <- length(unique(dcat6[item_type == 'categorical', y_stan]))
stan_data$cat1_y <- dcat6[item_type == 'categorical', y_stan]
stan_data$cat1_question_of_obs <- dcat6[item_type == 'categorical', item_time_id]
stan_data$cat1_unit_of_obs <- dcat6[item_type == 'categorical', pid]
stan_data$cat1_X <- as.matrix(dcat6[item_type == 'categorical', time - 1L], ncol = 1)
stan_data$Ncat2 <- nrow(dcat6[item_type == 'out-of-7',])
stan_data$Qcat2 <- max(dcat6[item_type == 'out-of-7', item_time_id])
stan_data$Kcat2 <- length(unique(dcat6[item_type == 'out-of-7', y_stan]))
stan_data$cat2_y <- dcat6[item_type == 'out-of-7', y_stan]
stan_data$cat2_question_of_obs <- dcat6[item_type == 'out-of-7', item_time_id]
stan_data$cat2_unit_of_obs <- dcat6[item_type == 'out-of-7', pid]
stan_data$cat2_X <- as.matrix(dcat6[item_type == 'out-of-7', time - 1L], ncol = 1)

# sample
upgcm_m1_all2_fit <- ugpcm_m1_compiled$sample(
  data = stan_data,
  seed = 123,
  chains = 2,
  parallel_chains = 2,
  iter_warmup = 5e2,
  iter_sampling = 15e2,
  refresh = 500,
  save_warmup = TRUE
)

# save output to RDS
upgcm_m1_all2_fit$save_object(file = file.path(dir.out, paste0(file.prefix,"stan.rds")))
```

```{r upgcm1all2_load_model, include=FALSE, eval=TRUE, echo=FALSE, tidy=FALSE, cache=TRUE}
upgcm_m1_all2_fit <- readRDS(file = file.path(dir.out, paste0(file.prefix,'stan.rds')))
```


```{r upgcm1all2_check_mixing_convergence, include=FALSE, eval=TRUE, echo=FALSE, tidy=FALSE, cache=TRUE}
#	check convergence and check mixing
#	get 95% credible intervals
tmp <- upgcm_m1_all2_fit$summary(
  variables = c('latent_factor_unit', 'latent_factor_beta',
                'cat1_skill_thresholds_1','cat1_skill_thresholds_incs','cat1_loadings_questions',
                'cat2_skill_thresholds_1','cat2_skill_thresholds_incs','cat2_loadings_questions'
                ),
  posterior::default_summary_measures(),
  posterior::default_convergence_measures(),
  extra_quantiles = ~posterior::quantile2(., probs = c(.0275, .975))
)
tmp <- as.data.table(tmp)
tmp <- tmp[order(ess_bulk),]
tmp
  
# worst 14 parameters with lowest ess_bulk
worst_var <- tmp$variable[ 1:9 ]

# extract samples
po <- upgcm_m1_all2_fit$draws(
  variables = c("lp__", worst_var),
  inc_warmup = TRUE,
  format = "draws_array"
  )

# make worst trace plot
p <- bayesplot:::mcmc_trace(po,  
                            pars = c("lp__",worst_var), 
                            n_warmup = 500,
                            facet_args = list(nrow = 2)
                            )
p <- p + theme_bw()
ggsave(file = file.path(dir.out, paste0(file.prefix,'worsttrace.png')), 
       plot = p, 
       h = 10, 
       w = 20
       )

# make intervals/areas plot
po <- upgcm_m1_all2_fit$draws(
  variables = c('latent_factor_unit', 'latent_factor_beta',
                'cat1_skill_thresholds_1','cat1_skill_thresholds_incs','cat1_loadings_questions',
                'cat2_skill_thresholds_1','cat2_skill_thresholds_incs','cat2_loadings_questions'
                ),
  inc_warmup = FALSE,
  format = "draws_array"
  )

color_scheme_set("teal")
p <- bayesplot::mcmc_intervals(po, prob = 0.5, prob_outer = 0.95, outer_size = 1, point_size = 2) +
  theme_bw()
ggsave(file = file.path(dir.out, paste0(file.prefix,'intervals.png')), 
       plot = p, 
       h = 50, 
       w = 8, 
       limitsize = FALSE
       )

  
p <- bayesplot::mcmc_areas(po, prob = 0.5, prob_outer = 0.95, point_est = 'median') +
  theme_bw()
ggsave(file = file.path(dir.out, paste0(file.prefix,'areas.png')), 
       plot = p, 
       h = 150, 
       w = 8, 
       limitsize = FALSE
       )


# make posterior predictive check

# create median and 95\% credible intervals
po <- upgcm_m1_all2_fit$draws(
  variables = c('cat1_ypred','cat2_ypred'),
  inc_warmup = FALSE,
  format = "draws_df"
  )
po <- as.data.table(po)
po <- data.table::melt(po, id.vars = c('.draw','.chain','.iteration'), value.name = 'ypred')
po[, item_type := gsub('([a-z0-9_]+)\\[([0-9]+)\\]','\\1',variable) ]
po[, oidt := as.integer(gsub('([a-z0-9_]+)\\[([0-9]+)\\]','\\2',variable)) ]
tmp <- po[, which(grepl('cat2',item_type))]
set(po, tmp, 'item_type', 'out-of-7')
set(po, po[, which(grepl('ypred',item_type))], 'item_type', 'categorical')  

pos <- 
  po[,
     list( summary_value = quantile(ypred, prob = c(0.025, 0.25, 0.5, 0.75, 0.975)),
           summary_name = c('q_lower','iqr_lower','median','iqr_upper','q_upper') 
           ),
     by = c('item_type','oidt')
     ]
pos <- 
  data.table::dcast(pos,
                    item_type + oidt ~ summary_name, 
                    value.var = 'summary_value'
                    )

pos <- merge(pos, dcat6, by = c('item_type','oidt'))
pos[, IN_PPI := y_stan >= q_lower & y_stan <= q_upper]
pos[, mean(IN_PPI)]

# plot posterior predictive check
p <- ggplot(pos, aes(x = oid, group = oid)) + 
  geom_boxplot( aes( ymin = q_lower,
                     lower = iqr_lower,
                     middle = median,
                     upper = iqr_upper,
                     ymax = q_upper),
                stat = 'identity') +
  geom_point( aes(y = y_stan, colour = IN_PPI ) ) +
  facet_grid(item_label ~ time_label, scales = 'free') +
  scale_x_discrete() +
  scale_y_continuous() +
  ggsci::scale_color_npg() +
  labs(x = '', 
       y = 'outcome', 
       colour = 'within\n95% posterior\nprediction\ninterval') +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 45,vjust = 1,hjust = 1))
ggsave(file = file.path(dir.out, paste0(file.prefix,'ppcheck.png')), 
       p, 
       w = 20, 
       h = 20)
```

```{r upgcm1all2_extract_class_probs_by_question, include=FALSE, eval=TRUE, echo=FALSE, tidy=FALSE, cache=TRUE}
po <- upgcm_m1_all2_fit$draws(
  variables = c('cat1_ordered_prob_by_obs'),
  inc_warmup = FALSE,
  format = "draws_df"
  )
po <- as.data.table(po)
po <- data.table::melt(po, id.vars = c('.draw','.chain','.iteration'), value.name = 'prob')
po[, y_stan := as.integer(gsub('([a-z0-9_]+)\\[([0-9]+),([0-9]+)\\]','\\3',variable)) ]
po[, oidt := as.integer(gsub('([a-z0-9_]+)\\[([0-9]+),([0-9]+)\\]','\\2',variable)) ]
set(po, NULL, 'variable', NULL)
tmp <- unique(subset(dcat6[item_type == 'categorical',], select = c(oidt, pid, item_time_id, time )))
po <- merge(po, tmp, by = c('oidt'))
po <- po[, list(prob = mean(prob)), by = c('.draw','time','item_time_id','y_stan')]
pos <- po[, 
          list( summary_value = quantile(prob, prob = c(0.025, 0.25, 0.5, 0.75, 0.975)),
                summary_name = c('q_lower','iqr_lower','median','iqr_upper','q_upper') 
                ), 
          by = c('time','item_time_id','y_stan')
          ]
pos <- data.table::dcast(pos, time + item_time_id + y_stan ~ summary_name, value.var = 'summary_value')
tmp <- unique( subset(dcat6[item_type == 'categorical',], select = c(time, time_label)) )
pos <- merge(pos, tmp, by = c('time'))
tmp <- unique( subset(dcat6[item_type == 'categorical',], select = c(item_time_id, item_label)) )
pos <- merge(pos, tmp, by = c('item_time_id'))
tmp <- unique( subset(dcat6[item_type == 'categorical',], select = c(y, y_stan, y_label)) )
pos <- merge(pos, tmp, by = c('y_stan'))
tmp <- dcat6[item_type == 'categorical', list(n = length(pid)), by = c('time_label','item_label','y_label')]
tmp2 <- tmp[, list(total = sum(n)), by = c('time_label','item_label')]
tmp <- merge(tmp, tmp2, by = c('time_label','item_label'))
tmp[, p_emp := n/total]
pos <- merge(pos, subset(tmp, select = -c(n, total)), by = c('time_label','item_label','y_label'), all.x = TRUE)
set(pos, pos[, which(is.na(p_emp))], 'p_emp', 0.)
pos[, y_stan := NULL]
pos.cat <- copy(pos)


po <- upgcm_m1_all2_fit$draws(
  variables = c('cat2_ordered_prob_by_obs'),
  inc_warmup = FALSE,
  format = "draws_df"
  )
po <- as.data.table(po)
po <- data.table::melt(po, id.vars = c('.draw','.chain','.iteration'), value.name = 'prob')
po[, y_stan := as.integer(gsub('([a-z0-9_]+)\\[([0-9]+),([0-9]+)\\]','\\3',variable)) ]
po[, oidt := as.integer(gsub('([a-z0-9_]+)\\[([0-9]+),([0-9]+)\\]','\\2',variable)) ]
set(po, NULL, 'variable', NULL)
tmp <- unique(subset(dcat6[item_type == 'out-of-7',], select = c(oidt, pid, item_time_id, time )))
po <- merge(po, tmp, by = c('oidt'))
po <- po[, list(prob = mean(prob)), by = c('.draw','time','item_time_id','y_stan')]
pos <- po[, 
          list( summary_value = quantile(prob, prob = c(0.025, 0.25, 0.5, 0.75, 0.975)),
                summary_name = c('q_lower','iqr_lower','median','iqr_upper','q_upper') 
                ), 
          by = c('time','item_time_id','y_stan')
          ]
pos <- data.table::dcast(pos, time + item_time_id + y_stan ~ summary_name, value.var = 'summary_value')
tmp <- unique( subset(dcat6[item_type == 'out-of-7',], select = c(time, time_label)) )
pos <- merge(pos, tmp, by = c('time'))
tmp <- unique( subset(dcat6[item_type == 'out-of-7',], select = c(item_time_id, item_label)) )
pos <- merge(pos, tmp, by = c('item_time_id'))
tmp <- unique( subset(dcat6[item_type == 'out-of-7',], select = c(y, y_stan, y_label)) )
pos <- merge(pos, tmp, by = c('y_stan'))
tmp <- dcat6[item_type == 'out-of-7', list(n = length(pid)), by = c('time_label','item_label','y_label')]
tmp2 <- tmp[, list(total = sum(n)), by = c('time_label','item_label')]
tmp <- merge(tmp, tmp2, by = c('time_label','item_label'))
tmp[, p_emp := n/total]
pos <- merge(pos, subset(tmp, select = -c(n, total)), by = c('time_label','item_label','y_label'), all.x = TRUE)
set(pos, pos[, which(is.na(p_emp))], 'p_emp', 0.)
pos[, y_stan := NULL]
pos.cat2 <- copy(pos)

pos <- rbind(pos.cat, pos.cat2)
pos[, group_label := gsub('([^_]+)_([^_]+)','\\1',item_label)]

pal <- colorRampPalette(ggsci::pal_futurama("planetexpress")(12))(pos[, length(unique(item_label))])


p <- ggplot(pos, aes(x = y_label, group = interaction(item_label, y_label))) +
  geom_col(aes(fill = item_label, y = p_emp), 
           position = position_dodge(width = 0.9, preserve = "single"), 
           alpha = 0.8,
           width = 0.8 ) +
  geom_boxplot( aes(ymin = q_lower, lower = iqr_lower, middle = median, upper = iqr_upper, ymax = q_upper), 
                position = position_dodge(0.9, preserve = "single"),
                stat = 'identity',
                alpha = 0,
                width = 0.3) +
  scale_y_continuous(labels = scales::percent) +
  scale_fill_manual(values = pal) +
  facet_wrap(group_label ~ time_label, scales = "free_x", ncol = 2) +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 45,vjust = 1,hjust = 1),
        legend.position = 'bottom'
        ) +
  labs(x = '', y = 'proportion of outcomes', fill = 'survey items')
ggsave(file = file.path(dir.out, paste0(file.prefix,'probs_barplot_v2.png')), 
       plot = p, 
       h = 40, 
       w = 12
       )
```

## Eight model: participant-level endline effect

Here is the Stan code:

```{r upgcm2all_stan_code, include=TRUE, eval=TRUE, echo=TRUE, tidy=FALSE, cache=TRUE}
ugpcm_m2_txt <- "
functions
{
  matrix upgcm_get_skill_thresholds( vector skill_thresholds_1,
                                     matrix skill_thresholds_incs
                                     )
  {
    int Q = size(skill_thresholds_1);
    int Km1 = cols(skill_thresholds_incs) + 1;
    matrix [Q, Km1] skill_thresholds;
    skill_thresholds = rep_matrix(skill_thresholds_1, Km1);
    for(j in 1:Q)
    { 
      skill_thresholds[j,2:Km1] += cumulative_sum( skill_thresholds_incs[j,:] );  
    }
    return skill_thresholds;
  }
  
  matrix upgcm_get_etas(  vector loadings_questions,
                          matrix skill_thresholds,
                          vector latent_factor_obs,
                          array [] int question_of_obs,
                          array [] int unit_of_obs
                          )
  {
    int N = num_elements(unit_of_obs);
    int Q = rows(skill_thresholds);
    int K = cols(skill_thresholds) + 1;
    
    matrix[N, K] etas;
    etas[:,1] = rep_vector(0., N);
    for(n in 1:N)
    {
      etas[n,2:K]  = loadings_questions[question_of_obs[n]] * 
                     ( 
                        latent_factor_obs[n] - skill_thresholds[question_of_obs[n],:]
                     );
    }
    return etas;
  }
  
  matrix upgcm_get_etas(  matrix loadings_questions,
                          matrix skill_thresholds,
                          vector latent_factor_obs,
                          array [] int question_of_obs,
                          array [] int unit_of_obs
                          )
  {
    int N = num_elements(unit_of_obs);
    int Q = rows(skill_thresholds);
    int K = cols(skill_thresholds) + 1;
    
    matrix[N, K] etas;
    etas[:,1] = rep_vector(0., N);
    for(n in 1:N)
    {
      etas[n,2:K]  = loadings_questions[question_of_obs[n], :] .* 
                     ( 
                        latent_factor_obs[n] - skill_thresholds[question_of_obs[n],:]
                     );
    }
    return etas;
  }
}
data
{
  int<lower=1> U; // number of units
  
  int<lower=1> Ncat1; // number of ordered categorial observations
  int<lower=2> Qcat1; // number of ordered categorial questions
  int<lower=3> Kcat1; // number of categories
  array [Ncat1] int<lower=1, upper=Kcat1> cat1_y; // observations
  array [Ncat1] int<lower=1,upper=U> cat1_unit_of_obs;
  array [Ncat1] int<lower=1,upper=U+1> cat1_unit_of_endline_obs; // U+1 codes nothing to be added
  array [Ncat1] int<lower=1,upper=Qcat1> cat1_question_of_obs;
  
  int<lower=1> Ncat2; 
  int<lower=2> Qcat2; 
  int<lower=3> Kcat2; 
  array [Ncat2] int<lower=1, upper=Kcat2> cat2_y; // observations
  array [Ncat2] int<lower=1,upper=U> cat2_unit_of_obs;
  array [Ncat2] int<lower=1,upper=U+1> cat2_unit_of_endline_obs; // U+1 codes nothing to be added
  array [Ncat2] int<lower=1,upper=Qcat2> cat2_question_of_obs;
}
transformed data
{
    real s2z_sd_unit;
    s2z_sd_unit = inv(sqrt(1. - inv(U)));
}
parameters
{
  sum_to_zero_vector[U] latent_factor_unit;
  sum_to_zero_vector[U] latent_factor_unit_endline;
  
  vector [Qcat1] cat1_skill_thresholds_1;
  matrix [Qcat1, Kcat1 - 2] cat1_skill_thresholds_incs;
  vector<lower=0>[Qcat1] cat1_loadings_questions;
  
  vector [Qcat2] cat2_skill_thresholds_1;
  matrix [Qcat2, Kcat2 - 2] cat2_skill_thresholds_incs;
  vector<lower=0>[Qcat2] cat2_loadings_questions;
}
transformed parameters
{
  matrix [Ncat1, Kcat1] cat1_eta;
  matrix [Ncat2, Kcat2] cat2_eta;
  
  {
    matrix [Qcat1, Kcat1 - 1] cat1_skill_thresholds;
    matrix [Qcat2, Kcat2 - 1] cat2_skill_thresholds;
    
    cat1_skill_thresholds 
      =
      upgcm_get_skill_thresholds( cat1_skill_thresholds_1,
                                  cat1_skill_thresholds_incs
                                  );
    cat2_skill_thresholds 
      =
      upgcm_get_skill_thresholds( cat2_skill_thresholds_1,
                                  cat2_skill_thresholds_incs
                                  );
                                  
    cat1_eta
        = 
        upgcm_get_etas( cat1_loadings_questions,
                        cat1_skill_thresholds,
                        latent_factor_unit[cat1_unit_of_obs] + 
                          append_row(latent_factor_unit_endline, 0.)[cat1_unit_of_endline_obs],
                        cat1_question_of_obs,
                        cat1_unit_of_obs
                        );
    cat2_eta
        = 
        upgcm_get_etas( cat2_loadings_questions,
                        cat2_skill_thresholds,
                        latent_factor_unit[cat2_unit_of_obs] + 
                          append_row(latent_factor_unit_endline, 0.)[cat2_unit_of_endline_obs],
                        cat2_question_of_obs,
                        cat2_unit_of_obs
                        );
  }
}
model
{
  // likelihood under unsummed generalised partial credits model
  for (n in 1:Ncat1) 
  {
    target += categorical_logit_lupmf( cat1_y[n] | cat1_eta[n]' );
  }
  for (n in 1:Ncat2) 
  {
    target += categorical_logit_lupmf( cat2_y[n] | cat2_eta[n]' );
  }
  
  // priors for latent factors
  target += normal_lupdf( latent_factor_unit | 0, s2z_sd_unit ); 
  target += normal_lupdf( latent_factor_unit_endline | 0, s2z_sd_unit ); 
  
  // priors for skill thresholds
  target += normal_lupdf( cat1_skill_thresholds_1 | 0, 3.5 ); 
  target += normal_lupdf( to_vector(cat1_skill_thresholds_incs) | 0, 2.5); 
  target += normal_lupdf( cat2_skill_thresholds_1 | 0, 3.5 ); 
  target += normal_lupdf( to_vector(cat2_skill_thresholds_incs) | 0, 2.5); 
  
  // priors for loadings
  target += student_t_lupdf(cat1_loadings_questions | 3, 0, 1);
  target += student_t_lupdf(cat2_loadings_questions | 3, 0, 1);
}
generated quantities
{
  matrix [Ncat1,Kcat1] cat1_ordered_prob_by_obs;
  matrix [Ncat2,Kcat2] cat2_ordered_prob_by_obs;
  array [Ncat1] int<lower=0> cat1_ypred;
  array [Ncat2] int<lower=0> cat2_ypred;
  array [Ncat1+Ncat2] real log_lik;
  
  for (n in 1:Ncat1) 
  {
    cat1_ordered_prob_by_obs[n,:] = softmax( cat1_eta[n]' )';
    cat1_ypred[n] = categorical_logit_rng( cat1_eta[n]' );
  }
  for (n in 1:Ncat2) 
  {
    cat2_ordered_prob_by_obs[n,:] = softmax( cat2_eta[n]' )';
    cat2_ypred[n] = categorical_logit_rng( cat2_eta[n]' ); 
  }
  for(n in 1:Ncat1)
  {
    log_lik[n] =  categorical_logit_lpmf( cat1_y[n] | cat1_eta[n]' );
  }
  for(n in 1:Ncat2)
  {
    log_lik[ Ncat1+n ] = categorical_logit_lpmf( cat2_y[n] | cat2_eta[n]' );
  }
}
"
# compile the model
ugpcm_m2_filename <- cmdstanr::write_stan_file(
  gsub('\t',' ', ugpcm_m2_txt),
  dir = dir.out,
  basename = NULL,
  force_overwrite = FALSE,
  hash_salt = ""
)

# compile Stan model
ugpcm_m2_compiled <- cmdstanr::cmdstan_model(ugpcm_m2_filename)
```
```{r upgcm2all_prepare data, include=TRUE, eval=TRUE, echo=TRUE, message=FALSE, tidy=FALSE, cache=TRUE}
file.prefix <- 'cg_chi_all_upgcm2_'
dcat5 <- copy(dp1)
dcat5[, time := time + 1L]
dcat5[, y_stan := y + 1L]
tmp <- data.table(item_label = sort(unique(dcat5$item_label)))
tmp[, item_type := ifelse(grepl('CG-MH',item_label), 'categorical', 'out-of-7')]
tmp <- tmp[, list(item_label = item_label, item_id = 1:length(item_label)), by = 'item_type']
dcat5 <- merge(dcat5, tmp, by = 'item_label')
setkey(dcat5, pid, time, item_label)
dcat5[, oid := 1:nrow(dcat5)]
tmp <- dcat5[, list(oid = oid, oidt = 1:length(y_stan)), by = 'item_type' ]
dcat5 <- merge(dcat5, tmp, by  = c('item_type','oid'))
dcat5[, pid_endline := pid]
set(dcat5, dcat5[, which(time_label == 'Baseline')], 'pid_endline', max(dcat5$pid) + 1L)
setkey(dcat5, pid, time, item_label)
```

```{r upgcm1all_run_model, include=TRUE, eval=TRUE, echo=TRUE, message=FALSE, tidy=FALSE, cache=TRUE}
# define data in format needed for model specification
stan_data <- list()
stan_data$U <- max(dcat5$pid)
stan_data$Ncat1 <- nrow(dcat5[item_type == 'categorical',])
stan_data$Qcat1 <- max(dcat5[item_type == 'categorical', item_id])
stan_data$Kcat1 <- length(unique(dcat5[item_type == 'categorical', y_stan]))
stan_data$cat1_y <- dcat5[item_type == 'categorical', y_stan]
stan_data$cat1_question_of_obs <- dcat5[item_type == 'categorical', item_id]
stan_data$cat1_unit_of_obs <- dcat5[item_type == 'categorical', pid]
stan_data$cat1_unit_of_endline_obs <- dcat5[item_type == 'categorical', pid_endline]
stan_data$Ncat2 <- nrow(dcat5[item_type == 'out-of-7',])
stan_data$Qcat2 <- max(dcat5[item_type == 'out-of-7', item_id])
stan_data$Kcat2 <- length(unique(dcat5[item_type == 'out-of-7', y_stan]))
stan_data$cat2_y <- dcat5[item_type == 'out-of-7', y_stan]
stan_data$cat2_question_of_obs <- dcat5[item_type == 'out-of-7', item_id]
stan_data$cat2_unit_of_endline_obs <- dcat5[item_type == 'out-of-7', pid_endline]
stan_data$cat2_unit_of_obs <- dcat5[item_type == 'out-of-7', pid]


# sample
upgcm_m2_all_fit <- ugpcm_m2_compiled$sample(
  data = stan_data,
  seed = 123,
  chains = 2,
  parallel_chains = 2,
  iter_warmup = 5e2,
  iter_sampling = 15e2,
  refresh = 500,
  save_warmup = TRUE
)

# save output to RDS
upgcm_m2_all_fit$save_object(file = file.path(dir.out, paste0(file.prefix,"stan.rds")))
```

```{r upgcm2all_load_model, include=FALSE, eval=TRUE, echo=FALSE, tidy=FALSE, cache=TRUE}
upgcm_m2_all_fit <- readRDS(file = file.path(dir.out, paste0(file.prefix,'stan.rds')))
```

```{r upgcm2all_check_mixing_convergence, include=FALSE, eval=TRUE, echo=FALSE, tidy=FALSE, cache=TRUE}
#	check convergence and check mixing
#	get 95% credible intervals
tmp <- upgcm_m2_all_fit$summary(
  variables = c('latent_factor_unit', 'latent_factor_unit_endline',
                'cat1_skill_thresholds_1','cat1_skill_thresholds_incs','cat1_loadings_questions',
                'cat2_skill_thresholds_1','cat2_skill_thresholds_incs','cat2_loadings_questions'
                ),
  posterior::default_summary_measures(),
  posterior::default_convergence_measures(),
  extra_quantiles = ~posterior::quantile2(., probs = c(.0275, .975))
)
tmp <- as.data.table(tmp)
tmp <- tmp[order(ess_bulk),]
tmp
  
# worst 14 parameters with lowest ess_bulk
worst_var <- tmp$variable[ 1:9 ]

# extract samples
po <- upgcm_m2_all_fit$draws(
  variables = c("lp__", worst_var),
  inc_warmup = TRUE,
  format = "draws_array"
  )

# make worst trace plot
p <- bayesplot:::mcmc_trace(po,  
                            pars = c("lp__",worst_var), 
                            n_warmup = 500,
                            facet_args = list(nrow = 2)
                            )
p <- p + theme_bw()
ggsave(file = file.path(dir.out, paste0(file.prefix,'worsttrace.png')), 
       plot = p, 
       h = 10, 
       w = 20
       )

# make intervals/areas plot
po <- upgcm_m2_all_fit$draws(
  variables = c('latent_factor_unit', 'latent_factor_unit_endline',
                'cat1_skill_thresholds_1','cat1_skill_thresholds_incs','cat1_loadings_questions',
                'cat2_skill_thresholds_1','cat2_skill_thresholds_incs','cat2_loadings_questions'
                ),
  inc_warmup = FALSE,
  format = "draws_array"
  )

color_scheme_set("teal")
p <- bayesplot::mcmc_intervals(po, prob = 0.5, prob_outer = 0.95, outer_size = 1, point_size = 2) +
  theme_bw()
ggsave(file = file.path(dir.out, paste0(file.prefix,'intervals.png')), 
       plot = p, 
       h = 50, 
       w = 8, 
       limitsize = FALSE
       )

  
p <- bayesplot::mcmc_areas(po, prob = 0.5, prob_outer = 0.95, point_est = 'median') +
  theme_bw()
ggsave(file = file.path(dir.out, paste0(file.prefix,'areas.png')), 
       plot = p, 
       h = 150, 
       w = 8, 
       limitsize = FALSE
       )


# make posterior predictive check

# create median and 95\% credible intervals
po <- upgcm_m2_all_fit$draws(
  variables = c('cat1_ypred','cat2_ypred'),
  inc_warmup = FALSE,
  format = "draws_df"
  )
po <- as.data.table(po)
po <- data.table::melt(po, id.vars = c('.draw','.chain','.iteration'), value.name = 'ypred')
po[, item_type := gsub('([a-z0-9_]+)\\[([0-9]+)\\]','\\1',variable) ]
po[, oidt := as.integer(gsub('([a-z0-9_]+)\\[([0-9]+)\\]','\\2',variable)) ]
tmp <- po[, which(grepl('cat2',item_type))]
set(po, tmp, 'item_type', 'out-of-7')
set(po, po[, which(grepl('ypred',item_type))], 'item_type', 'categorical')  

pos <- 
  po[,
     list( summary_value = quantile(ypred, prob = c(0.025, 0.25, 0.5, 0.75, 0.975)),
           summary_name = c('q_lower','iqr_lower','median','iqr_upper','q_upper') 
           ),
     by = c('item_type','oidt')
     ]
pos <- 
  data.table::dcast(pos,
                    item_type + oidt ~ summary_name, 
                    value.var = 'summary_value'
                    )

pos <- merge(pos, dcat6, by = c('item_type','oidt'))
pos[, IN_PPI := y_stan >= q_lower & y_stan <= q_upper]
pos[, mean(IN_PPI)]

# plot posterior predictive check
p <- ggplot(pos, aes(x = oid, group = oid)) + 
  geom_boxplot( aes( ymin = q_lower,
                     lower = iqr_lower,
                     middle = median,
                     upper = iqr_upper,
                     ymax = q_upper),
                stat = 'identity') +
  geom_point( aes(y = y_stan, colour = IN_PPI ) ) +
  facet_grid(item_label ~ time_label, scales = 'free') +
  scale_x_discrete() +
  scale_y_continuous() +
  ggsci::scale_color_npg() +
  labs(x = '', 
       y = 'outcome', 
       colour = 'within\n95% posterior\nprediction\ninterval') +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 45,vjust = 1,hjust = 1))
ggsave(file = file.path(dir.out, paste0(file.prefix,'ppcheck.png')), 
       p, 
       w = 20, 
       h = 20)
```

```{r upgcm2all_extract_class_probs_by_question, include=FALSE, eval=TRUE, echo=FALSE, tidy=FALSE, cache=TRUE}
po <- upgcm_m2_all_fit$draws(
  variables = c('cat1_ordered_prob_by_obs'),
  inc_warmup = FALSE,
  format = "draws_df"
  )
po <- as.data.table(po)
po <- data.table::melt(po, id.vars = c('.draw','.chain','.iteration'), value.name = 'prob')
po[, y_stan := as.integer(gsub('([a-z0-9_]+)\\[([0-9]+),([0-9]+)\\]','\\3',variable)) ]
po[, oidt := as.integer(gsub('([a-z0-9_]+)\\[([0-9]+),([0-9]+)\\]','\\2',variable)) ]
set(po, NULL, 'variable', NULL)
tmp <- unique(subset(dcat5[item_type == 'categorical',], select = c(oidt, pid, item_id, time )))
po <- merge(po, tmp, by = c('oidt'))
po <- po[, list(prob = mean(prob)), by = c('.draw','time','item_id','y_stan')]
pos <- po[, 
          list( summary_value = quantile(prob, prob = c(0.025, 0.25, 0.5, 0.75, 0.975)),
                summary_name = c('q_lower','iqr_lower','median','iqr_upper','q_upper') 
                ), 
          by = c('time','item_id','y_stan')
          ]
pos <- data.table::dcast(pos, time + item_id + y_stan ~ summary_name, value.var = 'summary_value')
tmp <- unique( subset(dcat5[item_type == 'categorical',], select = c(time, time_label)) )
pos <- merge(pos, tmp, by = c('time'))
tmp <- unique( subset(dcat5[item_type == 'categorical',], select = c(item_id, item_label)) )
pos <- merge(pos, tmp, by = c('item_id'))
tmp <- unique( subset(dcat5[item_type == 'categorical',], select = c(y, y_stan, y_label)) )
pos <- merge(pos, tmp, by = c('y_stan'))
tmp <- dcat5[item_type == 'categorical', list(n = length(pid)), by = c('time_label','item_label','y_label')]
tmp2 <- tmp[, list(total = sum(n)), by = c('time_label','item_label')]
tmp <- merge(tmp, tmp2, by = c('time_label','item_label'))
tmp[, p_emp := n/total]
pos <- merge(pos, subset(tmp, select = -c(n, total)), by = c('time_label','item_label','y_label'), all.x = TRUE)
set(pos, pos[, which(is.na(p_emp))], 'p_emp', 0.)
pos[, y_stan := NULL]
pos.cat <- copy(pos)


po <- upgcm_m2_all_fit$draws(
  variables = c('cat2_ordered_prob_by_obs'),
  inc_warmup = FALSE,
  format = "draws_df"
  )
po <- as.data.table(po)
po <- data.table::melt(po, id.vars = c('.draw','.chain','.iteration'), value.name = 'prob')
po[, y_stan := as.integer(gsub('([a-z0-9_]+)\\[([0-9]+),([0-9]+)\\]','\\3',variable)) ]
po[, oidt := as.integer(gsub('([a-z0-9_]+)\\[([0-9]+),([0-9]+)\\]','\\2',variable)) ]
set(po, NULL, 'variable', NULL)
tmp <- unique(subset(dcat5[item_type == 'out-of-7',], select = c(oidt, pid, item_id, time )))
po <- merge(po, tmp, by = c('oidt'))
po <- po[, list(prob = mean(prob)), by = c('.draw','time','item_id','y_stan')]
pos <- po[, 
          list( summary_value = quantile(prob, prob = c(0.025, 0.25, 0.5, 0.75, 0.975)),
                summary_name = c('q_lower','iqr_lower','median','iqr_upper','q_upper') 
                ), 
          by = c('time','item_id','y_stan')
          ]
pos <- data.table::dcast(pos, time + item_id + y_stan ~ summary_name, value.var = 'summary_value')
tmp <- unique( subset(dcat5[item_type == 'out-of-7',], select = c(time, time_label)) )
pos <- merge(pos, tmp, by = c('time'))
tmp <- unique( subset(dcat5[item_type == 'out-of-7',], select = c(item_id, item_label)) )
pos <- merge(pos, tmp, by = c('item_id'))
tmp <- unique( subset(dcat5[item_type == 'out-of-7',], select = c(y, y_stan, y_label)) )
pos <- merge(pos, tmp, by = c('y_stan'))
tmp <- dcat5[item_type == 'out-of-7', list(n = length(pid)), by = c('time_label','item_label','y_label')]
tmp2 <- tmp[, list(total = sum(n)), by = c('time_label','item_label')]
tmp <- merge(tmp, tmp2, by = c('time_label','item_label'))
tmp[, p_emp := n/total]
pos <- merge(pos, subset(tmp, select = -c(n, total)), by = c('time_label','item_label','y_label'), all.x = TRUE)
set(pos, pos[, which(is.na(p_emp))], 'p_emp', 0.)
pos[, y_stan := NULL]
pos.cat2 <- copy(pos)

pos <- rbind(pos.cat, pos.cat2)
pos[, group_label := gsub('([^_]+)_([^_]+)','\\1',item_label)]

pal <- colorRampPalette(ggsci::pal_futurama("planetexpress")(12))(pos[, length(unique(item_label))])


p <- ggplot(pos, aes(x = y_label, group = interaction(item_label, y_label))) +
  geom_col(aes(fill = item_label, y = p_emp), 
           position = position_dodge(width = 0.9, preserve = "single"), 
           alpha = 0.8,
           width = 0.8 ) +
  geom_boxplot( aes(ymin = q_lower, lower = iqr_lower, middle = median, upper = iqr_upper, ymax = q_upper), 
                position = position_dodge(0.9, preserve = "single"),
                stat = 'identity',
                alpha = 0,
                width = 0.3) +
  scale_y_continuous(labels = scales::percent) +
  scale_fill_manual(values = pal) +
  facet_wrap(group_label ~ time_label, scales = "free_x", ncol = 2) +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 45,vjust = 1,hjust = 1),
        legend.position = 'bottom'
        ) +
  labs(x = '', y = 'proportion of outcomes', fill = 'survey items')
ggsave(file = file.path(dir.out, paste0(file.prefix,'probs_barplot_v2.png')), 
       plot = p, 
       h = 40, 
       w = 12
       )
```

## Compare models

```{r colombia_Compare Bayesian LOO approximations, include=FALSE, eval=TRUE, echo=FALSE, message=FALSE, tidy=FALSE, cache=TRUE}
ocat_lfm_m6all_fit <- readRDS(file = file.path(dir.out, 'cg_chi_all_lf6_stan.rds'))
upgcm_m1_all_fit <- readRDS(file = file.path(dir.out, 'cg_chi_all_upgcm1_stan.rds'))
upgcm_m1_all2_fit <- readRDS(file = file.path(dir.out, 'cg_chi_all2_upgcm1_stan.rds'))
upgcm_m2_all_fit <- readRDS(file = file.path(dir.out, 'cg_chi_all_upgcm2_stan.rds'))

ocat_lfm_m6all_fit_loo <- ocat_lfm_m6all_fit$loo()
upgcm_m1_all_fit_loo <- upgcm_m1_all_fit$loo()
upgcm_m1_all2_fit_loo <- upgcm_m1_all2_fit$loo()
upgcm_m2_all_fit_loo <- upgcm_m2_all_fit$loo()
```

```{r colombia_Compare Bayesian LOO approximations table, include=TRUE, eval=TRUE, echo=TRUE, tidy=FALSE, cache=TRUE}
comp <- loo::loo_compare(ocat_lfm_m6all_fit_loo,
                         upgcm_m1_all_fit_loo,
                         upgcm_m1_all2_fit_loo, 
                         upgcm_m2_all_fit_loo
                         )

print(comp, simplify = FALSE)
```
