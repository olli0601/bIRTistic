---
title: "Jordan Hope Groups Analysis"
date: "2025-08-20"
output: 
  bookdown::html_document2:
    toc: TRUE
    toc_float: TRUE
    highlight: tango
  bookdown::pdf_book:
    keep_tex: yes
---

<style type="text/css">
h1{
  font-size: 24pt;
}
h2{
  font-size: 18pt;
}
body{
  font-size: 12pt;
}
</style>

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
# Data loading and pre-processing

```{r Load packages, include=TRUE, eval=TRUE, message=FALSE, echo=TRUE, warning=FALSE, tidy=TRUE}
#library(lmerTest)
#library(clubSandwich)
#library(rstanarm)
#library(rstan)
library(tidyverse)
library(data.table)
library(purrr)
require(ggplot2) # for plotting
require(ggsci) # for plotting colors
require(hexbin) # for plotting pair plots
require(bayesplot) # for plotting Stan outputs
require(knitr) # for Rmarkdown
require(kableExtra) # for Rmarkdown
require(cmdstanr) # for Stan
require(loo) # for Bayesian LOO
require(polycor) # esetimate polychoric correlations via ML
require(GGally) # plotting of pair plots
```

```{r Define data locations, include=TRUE, eval=TRUE, message=FALSE, echo=TRUE, warning=FALSE, tidy=TRUE}
dir.home <- '/Users/or105/Library/CloudStorage/OneDrive-ImperialCollegeLondon/OR_Work/2025/2025_project_Hope_Groups_Jordan'
dir.data <- file.path(dir.home, 'data')
dir.out <- file.path(dir.home, 'analysis-250814')
file.data <- file.path(dir.data,'PHQ_VAC_Jordan_v250812.csv')
set.seed(42L)
```

```{r Load and preprocess data, include=TRUE, eval=TRUE, message=FALSE, echo=TRUE, warning=FALSE, tidy=TRUE}
# Read in data
dp <- as.data.table(read.csv( file.data )) 
setnames(dp, 
         c('f_participants','facilitator_name','Arm','Timepoint'), 
         c('pid','fid','arm_label','time_label')
         )
dp[, arm := as.integer(arm_label == 'Intervention')]
dp[, time := as.integer(time_label == 'Endline')]

# Remove NA rows
dp <- subset(dp, !is.na(pid))

# Inspect participants with >2 rows and de-duplicate
dp[, table(pid)]
dp <- dp[!duplicated(dp, by = c('pid','arm','time')),]

# Remove participants that don't have both baseline and endline
tmp <- dp[, list(ntime = length(unique(time))) , by = 'pid']
dp <- merge(dp, subset(tmp, ntime == 2, select = pid), by = 'pid')

# Remove participant 13 with missing outcome
dp <- subset(dp, pid != 13)

# Selected data: 
dp[, table(pid)]

# Set pid to continuous ID
tmp <- data.table( pid = dp[, sort(unique(pid))] )
tmp[, pid_new := 1:nrow(tmp)]
dp <- merge(dp, tmp, by = 'pid')
setnames(dp, c('pid','pid_new'), c('pid_label','pid'))

# Bring table into long format
setnames(dp, 
         c('PHQ4_down_num','PHQ4_interest_num','PHQ4_nervous_num','PHQ4_worry_num',
           'VAC_physical_total','VAC_emotional_total','VAC_negative_total',
           'v_threat_num','v_refuse.speak_num','v_insult_num','v_shout_num','v_abandon_num',
           'v_object_num','v_push_num','v_hit_num','v_face_num','v_beat_num'),
         c('PHQ4_down','PHQ4_interest','PHQ4_nervous','PHQ4_worry',
           'VACP_total','VACE_total','VAC_total',
           'VACE_threat','VACE_refusespeak','VACE_insult','VACE_shout','VACE_abandon',
           'VACP_object','VACP_push','VACP_hit','VACP_face','VACP_beat')
         )
dp <- data.table::melt(dp, id.vars = c('time_label','arm_label','arm','time','pid','pid_label','fid'))

# Show value mins and maxs
dp[, list(value_min = min(value), value_max = max(value)), by = 'variable']

# get data into long format per survey item
tmp <- unique(subset(dp, select = 'variable'))
tmp[, variable_type := ifelse(grepl('total',tmp$variable),'agg_likert','likert')]
tmp[, vid := 1:nrow(tmp) ]
dp <- merge(dp, tmp, by = 'variable')
```

# PHQ4 - Ordered Cat model

I will start considering the PHQ4 data on the actual item scale as it was recorded. The distribution of survey responses in the Control and Intervention groups at the baseline and endline time points is as follows:

```{r PHQ4_data, include=TRUE, eval=TRUE, message=FALSE, echo=FALSE, warning=FALSE, tidy=TRUE}
# Select ordered categorical outcome
dcat1 <- subset(dp, vid < 6 & variable_type == 'likert')
file.prefix <- 'parentalmh_ordered_categorical_'

# Preprocessing
dcat1 <- dcat1[order(pid, arm, time, variable)]
dcat1[, oid := seq_len(nrow(dcat1))]
set(dcat1, NULL, 'value', dcat1[, value + 1L])
set(dcat1, NULL, 'arm', dcat1[, arm + 1L])
set(dcat1, NULL, 'time', dcat1[, time + 1L])
set(dcat1, NULL, 'group', dcat1[, max(arm) * (time - 1L) + arm])
set(dcat1, NULL, 'vid', dcat1[, vid - 1L])

tmp <- data.table(value = 1:4,
                  value_label = factor(1:4, 
                                       levels = 1:4, 
                                       labels = c('Not at all','Several days','More than half the days','Nearly every day')
                                       )
                  )
dcat1 <- merge(dcat1, tmp, by = 'value')
dcat1 <- dcat1[order(pid, arm, time, variable)]

# Frequency plot
tmp <- dcat1[, list(n = length(pid)), by = c('arm_label','time_label','variable','value_label', 'value')]
tmp2 <- tmp[, list(total = sum(n)), by = c('arm_label','time_label','variable')]
tmp <- merge(tmp, tmp2, by = c('arm_label','time_label','variable'))
tmp[, p := n/total]

p <- ggplot(tmp, aes(x = value_label, y = n/total)) +
  geom_col(aes(fill = variable), position = position_dodge()) +
  scale_fill_bmj() +
  facet_grid(arm_label ~ time_label) +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 45,vjust = 1,hjust = 1),
        legend.position = 'bottom'
        ) +
  labs(x = '', fill = 'Likert scale')
ggsave(file = file.path(dir.out, paste0(file.prefix,'frequencies.png')), 
       plot = p, 
       h = 10, 
       w = 8
       )
```

```{r PHQ4_data_plot, include=TRUE, eval=TRUE, echo=FALSE, tidy=FALSE, fig.align='center', out.width='80%'}
knitr::include_graphics( file.path(dir.out, paste0(file.prefix,'frequencies.png')) )
```

Let me next consider how participants respond across the four questions.

At each survey time point, participants tended to respond to the four questions targeting the same underlying latent behaviour (here parental mental health) in a similar way. For example, we see that most commonly, the responses to all other PHQ4 questions were the same as that for the 'PHQ4_down' question. We also see that participants did not respond exactly the same across all other questions. We also observe an ordering of the responses, for example participants who responded 'Nearly every day' to the 'PHQ4_down' question, responded most often 'Nearly every day', then 'More than half the days', then 'Several days' and then 'Not at all'. Here is a plot that illustrates this:


```{r PHQ4_data_explore_clustering1b, include=FALSE, eval=TRUE, echo=FALSE, message=FALSE, tidy=FALSE, cache=TRUE}
tmp <- data.table:::dcast( dcat1, time_label + arm_label + pid ~ variable, value.var = 'value_label')
tmp <- melt(tmp, id.vars = c('time_label','arm_label','pid','PHQ4_down'))

p <- ggplot(tmp, aes( x = value, fill = PHQ4_down, group = PHQ4_down)) +
  geom_bar(position = position_dodge()) +
  scale_fill_bmj() +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 45,vjust = 1,hjust = 1),
        legend.position = 'bottom'
        ) +
  labs(x = 'response to PHQ4_down question', y = 'responses to other PHQ4 questions', fill = 'responses to other PHQ4 questions') +
  facet_grid(arm_label ~ time_label)
ggsave(file = file.path(dir.out, paste0(file.prefix,'counts_correlations_across_PHQ_questions_same_arm-time.png')), 
       plot = p, 
       h = 8, 
       w = 8
       )
```
```{r PHQ4_data_explore_clustering1_plot, include=TRUE, eval=TRUE, echo=FALSE, tidy=FALSE, fig.align='center', out.width='80%'}
knitr::include_graphics( file.path(dir.out, paste0(file.prefix,'counts_correlations_across_PHQ_questions_same_arm-time.png')) )
```

These patterns are as expected: the survey deliberately asks multiple similar questions to elicit a latent underlying behavior, and we do not expect that a single question is sufficient to elicit the latent underlying behavior well. 

In statistical modelling, the latent underlying behavior is called a LATENT FACTOR. [A good paper on Bayesian latent factor models in psychology applications is here.](https://link.springer.com/article/10.3758/s13423-016-1016-7) Here, we will adopt simple versions of latent factor models, in that we consider ONE LATENT FACTOR for a particular set of item responses. For example, we consider four survey items, 'PHQ_down','PHQ_interest','PHQ_nervous','PHQ_worry', and suppose there is one underlying latent factor 'parental mental health'. 

In our application, we suppose there is a latent factor/trait that might be unique for each participants, or identical for all participants in a certain GROUP, that determines the outcomes for each of the survey items on the Likert scale. In our case, the group is specified by participants being in the same study arm (Control or Intervention) and at the same time (baseline or endline), so we have four groups. Statistical models for such data tend to be called MULTI-GROUP LATENT FACTOR MODELS.

Furthermore, if we suppose that participants each have unique latent factor/traits, then in general we consider the latent factors EXCHANGEABLE and model participant-level latent factors through RANDOM EFFECTS around the group mean latent factor.

The above plot illustrates similarities in item responses for the same participants. For categorical and ordinal data, the state-of-the-art approach to quantify these within-group similarities are POLYCHORIC CORRELATIONS. In this approach, we envisage a latent space of continuous variables that determine the outcomes, and measure the correlations between these continuously valued variables. [A good explanation and visualisation of polychoric correlations is here.](https://stats.stackexchange.com/questions/568756/how-to-interpret-a-polychoric-correlation) Visually, we found strong similarities across survey items, and across time for each item and can quantify the strength of the similarities in terms of polychoric correlations. We find that in any group (Control/Intervention and Baseline/Endline), the polychoric correlations are strongly significant:

```{r PHQ4_data_explore_clustering1, include=TRUE, eval=TRUE, echo=TRUE, message=FALSE, tidy=FALSE, cache=TRUE}
tmp <- data.table:::dcast( dcat1, time_label + arm_label + pid ~ variable, value.var = 'value')
tmp <- melt(tmp, id.vars = c('time_label','arm_label','pid','PHQ4_down'))
tmp[,
    {
      z <- polycor::polychor(PHQ4_down, value, std.err = TRUE, ML = FALSE)
      list(polchor = z$rho, polychor_sd = sqrt(as.numeric(z$var))) 
    }, 
    by = c('arm_label','time_label')
    ]
```

We also observe a clustering of reported outcomes across time. Participants who responded 'Not at all' to any of the survey questions at baseline also tended to respond 'Not at all', then 'Several days', then 'More than half the days' then 'Nearly every day' to the same question at endline. For some survey questions, polychoric correlations between baseline and endline tend to be significant, perhaps most notably for 'PHQ4_nervous' in the 'Control' group:

```{r PHQ4_data_explore_clustering2b, include=FALSE, eval=TRUE, echo=FALSE, message=FALSE, tidy=FALSE, cache=TRUE}
tmp <- data.table:::dcast( dcat1, variable + arm_label + pid ~ time_label, value.var = 'value_label')
p <- ggplot(tmp, aes( x = Baseline, fill = Endline, group = Endline)) +
  geom_bar(position = position_dodge()) +
  scale_fill_futurama() +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 45,vjust = 1,hjust = 1),
        legend.position = 'bottom'
        ) +
  labs(x = 'response at Baseline', y = 'response at Endline', fill = 'response at Endline') +
  facet_grid(arm_label ~ variable)
ggsave(file = file.path(dir.out, paste0(file.prefix,'counts_correlations_across_time_same_arm-question.png')), 
       plot = p, 
       h = 8, 
       w = 8
       )
```
```{r PHQ4_data_explore_clustering2_plot, include=TRUE, eval=TRUE, echo=FALSE, tidy=FALSE, fig.align='center', out.width='80%'}
knitr::include_graphics( file.path(dir.out, paste0(file.prefix,'counts_correlations_across_time_same_arm-question.png')) )
```

But I note that in our data, the temporal polychoric correlations are not as strong as the polychoric correlations across item responses at the same time point:

```{r PHQ4_data_explore_clustering2, include=TRUE, eval=TRUE, echo=TRUE, message=FALSE, tidy=FALSE, cache=TRUE}
tmp <- data.table:::dcast( dcat1, variable + arm_label + pid ~ time_label, value.var = 'value_label')
tmp[,
    {
      z <- polycor::polychor(Baseline, Endline, std.err = TRUE, ML = FALSE)
      list(polchor = z$rho, polychor_sd = sqrt(as.numeric(z$var))) 
    }, 
    by = c('arm_label','variable')
    ]
```

## Flexible Multi-Group Ordered Cat models without participant effects

We will first consider a standard, simple ordered categorical model for participants $i$ that provide survey item responses $Y_{ij} = \{1,\dotsc, K\}$ on a Likert scale with $K$ possible outcomes, without participant effects. 

\begin{align*}
& \text{Prob}(Y_{ij} = k) = \text{Ordered-Logistic}(k; \eta_{ij}, c) \\
& \eta_{ij}= \beta_{G(i)}\\
& c = [c1, c1 + \text{cumsum}(\delta_{1:{K-2}})] \\
& c_1 \sim \text{Normal}(0 , 3.5^2)\\
& \delta_{1:{K-2}} \sim \text{Normal}(0 , 2.5^2)\\
& \beta \sim \text{S2Z-Normal}(0, 1)
\end{align*}
where the probabilities of each response under the Ordered Logistic model are given by
\begin{equation*}
\text{Ordered-Logistic}(k;\eta_{ij}, c) = 
\begin{cases}
1 - \text{logit}^{-1}(\eta_{ij} - c_1) & \text{if k = 1}\\
\text{logit}^{-1}(\eta_{ij} - c_{k-1}) - \text{logit}^{-1}(\eta_{ij} - c_k) & \text{if 1 < k < K}\\
\text{logit}^{-1}(\eta_{ij} - c_{K-1}) - 0 & \text{if k = K.}
\end{cases}
\end{equation*}

The key points of this model are: we model the probabilities of the Likert outcomes in logit space, as one would do for $0/1$ binary outcomes. The logit space takes real values, which are transformed through the inverse logit function into probabilities in $[0,1]$. The cutpoints $c$ divide the logit-space and determine how the logit space is segmented to correspond to $K$ distinct probabilities. For $K$ categories, we need $K-1$ cutpoints and this explains why for a Bernoulli model we generally don't need any cutpoints. In the above model, the $G(i)$ is a function that maps participants to one of the four groups (Control/Intervention/Baseline/Endline), and $\beta_{G(i)}$ is the one latent factor that models the strength of the underlying parental mental health of all participants in the same group. In this simple model, all participants within the same group have the same identical latent factor, there are no participant effects. There is one more important conceptual point. Since $\eta_{ij}= \beta_{G(i)}$ does not depend on $j$, the probability that we observe $\{Y_{ij} = k \}$ is in this model independent of whatever we observe for all the other survey items of the same participant. This model cannot capture 'within-participant' clustering of item responses, and any predicted polychoric correlations will be zero under this model.

A few more technical points: putting a normal prior on the cut points themselves would mean that they 
tend to be pulled towards zero, and so one normally prefers to put a prior on the cut point differences. [This model is quite standard and readily available in Stan and other software packages.](https://mc-stan.org/docs/functions-reference/bounded_discrete_distributions.html#ordered-logistic-distribution)

Here is the Stan code for the above ordered categorical model without participant effects. This model is also straightforwardly available through the 'brms' package.

```{r ordered_logistic_no_unit_effects_stan_code_v0, include=TRUE, eval=TRUE, echo=TRUE, tidy=FALSE, cache=TRUE}
file.prefix <- 'parentalmh_ordered_categorical_'

# this is a textbook vanilla Orderec Categorical model, without participant effects
ordered_categorical_logit_txt_m0a <- "
data
{
  int<lower=1> N; // number of observations
  int<lower=3> K; // number of categories
  int<lower=2> P; // number of groups
  array[N] int<lower=1, upper=K> y; // observations
  array [N] int<lower=1,upper=P> group_of_obs;
}
transformed data
{
    real s2z_sd_groups;
    s2z_sd_groups = inv(sqrt(1. - inv(P)));
}
parameters
{
  sum_to_zero_vector[P] beta_group;
  
  // the cutpoints in logit space, using the 'ordered' variable type
  real cut_point_1;
  vector<lower=0>[K - 2] cut_point_gaps_groups;
}
transformed parameters
{
  ordered[K-1] cut_points;
  cut_points = 
      append_row(
        rep_vector(cut_point_1, 1), 
        rep_vector(cut_point_1, K-2) + cumulative_sum(cut_point_gaps_groups)
        );
}
model
{
  // likelihood, cannot be vectorized
  for (n in 1:N) {
    y[n] ~ ordered_logistic(
      beta_group[group_of_obs[n]], 
      cut_points);
  }
  
  // priors
  beta_group ~ normal(0, s2z_sd_groups); 
  cut_point_1 ~ normal(0, 3.5);
  cut_point_gaps_groups ~ normal(0, 2.5); 
}
generated quantities
{
  matrix [K,P] ordered_prob;
  array [N] int<lower=0> ypred;
  array [N] real log_lik;
  
  for (p in 1:P) {
    ordered_prob[1,p]       = 1 - inv_logit( beta_group[p] - cut_points[1] );
    ordered_prob[2:(K-1),p] = inv_logit( beta_group[p] - cut_points[1:(K-2)] ) 
                              - 
                              inv_logit( beta_group[p] - cut_points[2:(K-1)] );
    ordered_prob[K,p]       = inv_logit( beta_group[p] - cut_points[K-1] );
  }
    
  for( n in 1:N ) {
    ypred[n] = ordered_logistic_rng( 
                beta_group[group_of_obs[n]], 
                cut_points
                ); 
    log_lik[n] =  ordered_logistic_lpmf( y[n] |
                    beta_group[group_of_obs[n]], 
                    cut_points);
  }
}
"

# compile the model
ordered_categorical_logit_filename_m0a <- cmdstanr::write_stan_file(
  gsub('\t',' ', ordered_categorical_logit_txt_m0a),
  dir = dir.out,
  basename = NULL,
  force_overwrite = FALSE,
  hash_salt = ""
)

# compile Stan model
ordered_categorical_logit_compiled_m0a <- cmdstanr::cmdstan_model(ordered_categorical_logit_filename_m0a, force_recompile = TRUE)
```

```{r ordered_logistic_no_unit_effects_v0_run_model, include=TRUE, eval=TRUE, echo=TRUE, message=FALSE, tidy=FALSE, cache=TRUE}
# define data in format needed for model specification
stan_data <- list()
stan_data$N <- nrow(dcat1)
stan_data$P <- max(dcat1$group)
stan_data$K <- length(unique(dcat1$value))
stan_data$y <- dcat1$value
stan_data$group_of_obs <- dcat1$group

# sample
ordered_categorical_logit_fit_m0a <- ordered_categorical_logit_compiled_m0a$sample(
  data = stan_data,
  seed = 123,
  chains = 2,
  parallel_chains = 2,
  iter_warmup = 5e2,
  iter_sampling = 15e2,
  refresh = 500,
  save_warmup = TRUE
)

# save output to RDS
ordered_categorical_logit_fit_m0a$save_object(file = file.path(dir.out, paste0(file.prefix,"m0a_stan.rds")))
```
```{r ordered_logistic_no_unit_effects_v0_load_model, include=FALSE, eval=TRUE, echo=FALSE, tidy=FALSE, cache=TRUE}
ordered_categorical_logit_fit_m0a <- readRDS(file = file.path(dir.out, paste0(file.prefix,'m0a_stan.rds')))
```

```{r ordered_logistic_no_unit_effects_v0_check mixing convergence, include=FALSE, eval=TRUE, echo=FALSE, tidy=FALSE, cache=TRUE}
#	check convergence and check mixing
#	get 95% credible intervals
tmp <- ordered_categorical_logit_fit_m0a$summary(
  variables = c('beta_group','cut_point_1','cut_point_gaps_groups'),
  posterior::default_summary_measures(),
  posterior::default_convergence_measures(),
  extra_quantiles = ~posterior::quantile2(., probs = c(.0275, .975))
)
tmp <- as.data.table(tmp)
tmp
  
# parameter with lowest ess_bulk
worst_var <- tmp$variable[ which.min(tmp$ess_bulk) ]

# extract samples
po <- ordered_categorical_logit_fit_m0a$draws(
  variables = c("lp__", worst_var),
  inc_warmup = TRUE,
  format = "draws_array"
  )

# make worst trace plot
p <- bayesplot:::mcmc_trace(po,  
                            pars = c("lp__",worst_var), 
                            n_warmup = 500,
                            facet_args = list(nrow = 2)
                            )
p <- p + theme_bw()
ggsave(file = file.path(dir.out, paste0(file.prefix,'m0a_worsttrace.png')), 
       plot = p, 
       h = 10, 
       w = 8
       )

# make pairs plot
po <- ordered_categorical_logit_fit_m0a$draws(
  variables = c('beta_group','cut_point_1','cut_point_gaps_groups'),
  inc_warmup = FALSE,
  format = "draws_array"
  )

color_scheme_set("teal")
p <- bayesplot::mcmc_intervals(po, prob = 0.5, prob_outer = 0.95, outer_size = 1, point_size = 2) +
  theme_bw()
ggsave(file = file.path(dir.out, paste0(file.prefix,'m0a_intervals.png')), 
       plot = p, 
       h = 7, 
       w = 8, 
       limitsize = FALSE
       )

po <- ordered_categorical_logit_fit_m0a$draws(
  variables = c('lp__','ordered_prob[1,1]','ordered_prob[2,1]','ordered_prob[3,1]','ordered_prob[4,1]','beta_group[1]','cut_point_1','cut_point_gaps_groups'),
  inc_warmup = FALSE,
  format = "draws_array"
  )

bayesplot::color_scheme_set('viridisC')
p <- bayesplot::mcmc_pairs(po, 
                           diag_fun = "dens", 
                           off_diag_fun = "hex"
                           ) 
ggsave(file = file.path(dir.out, paste0(file.prefix,'m0a_pairsplot.png')), 
       plot = p, 
       h = 50, 
       w = 50, 
       limitsize = FALSE
       )
bayesplot::color_scheme_set('brewer-RdYlBu')

# make posterior predictive check

# create median and 95\% credible intervals
po <- ordered_categorical_logit_fit_m0a$draws(
  variables = c('ypred'),
  inc_warmup = FALSE,
  format = "draws_df"
  )
po <- as.data.table(po)
po <- data.table::melt(po, id.vars = c('.draw','.chain','.iteration'), value.name = 'ypred')
po[, oid := as.integer(gsub('ypred\\[([0-9]+)\\]','\\1',variable)) ]
pos <- 
  po[,
     list( summary_value = quantile(ypred, prob = c(0.025, 0.25, 0.5, 0.75, 0.975)),
           summary_name = c('q_lower','iqr_lower','median','iqr_upper','q_upper') 
           ),
     by = 'oid'
     ]
pos <- 
  data.table::dcast(pos,
                    oid ~ summary_name, 
                    value.var = 'summary_value'
                    )
pos <- merge(pos, dcat1, by = 'oid')
pos[, IN_PPI := value >= q_lower & value <= q_upper]
pos[, mean(IN_PPI)]

# plot posterior predictive check
p <- ggplot(pos, aes(x = oid, group = oid)) + 
  geom_boxplot( aes( ymin = q_lower,
                     lower = iqr_lower,
                     middle = median,
                     upper = iqr_upper,
                     ymax = q_upper),
                stat = 'identity') +
  geom_point( aes(y = value, colour = IN_PPI ) ) +
  facet_grid(variable ~ arm_label+time_label, scales = 'free') +
  scale_x_discrete() +
  scale_y_continuous() +
  ggsci::scale_color_npg() +
  labs(x = '', 
       y = 'outcome', 
       colour = 'within\n95% posterior\nprediction\ninterval') +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 45,vjust = 1,hjust = 1))
ggsave(file = file.path(dir.out, paste0(file.prefix,'m0a_ppcheck.png')), 
       p, 
       w = 12, 
       h = 12)
```

```{r ordered_logistic_no_unit_effects_v0_extract_class_probs, include=TRUE, eval=TRUE, echo=TRUE, tidy=FALSE, cache=TRUE}
# extract model probabilities
po <- ordered_categorical_logit_fit_m0a$draws(
  variables = c('ordered_prob'),
  inc_warmup = FALSE,
  format = "draws_df"
  )
po <- as.data.table(po)
setnames(po, colnames(po), gsub('\\]','',gsub('\\[','_',colnames(po))))
po <- data.table::melt(po, id.vars = c('.draw','.chain','.iteration'), value.name = 'prob')
po[, value := as.integer(gsub('ordered_prob_([0-9]),([0-9])','\\1',variable)) ]
po[, group := as.integer(gsub('ordered_prob_([0-9]),([0-9])','\\2',variable)) ]

tmp <- unique( subset(dcat1, select = c(group, arm, arm_label, time, time_label, value, value_label)) )
po <- merge(po, tmp, by = c('group','value'))
```

```{r ordered_logistic_no_unit_effects_v0_make_plots, include=FALSE, eval=TRUE, echo=FALSE, tidy=FALSE, cache=TRUE}
pos <- po[, 
          list( summary_value = quantile(prob, prob = c(0.025, 0.25, 0.5, 0.75, 0.975)),
                summary_name = c('q_lower','iqr_lower','median','iqr_upper','q_upper') 
                ), 
          by = c('group','arm','arm_label','time','time_label','value','value_label')
          ]
pos <- data.table::dcast(pos, group + arm + arm_label + time + time_label + value + value_label ~ summary_name, value.var = 'summary_value')

tmp <- dcat1[, list(n = length(pid)), by = c('arm_label','time_label','value_label','value')]
tmp2 <- tmp[, list(total = sum(n)), by = c('arm_label','time_label')]
tmp <- merge(tmp, tmp2, by = c('arm_label','time_label'))
tmp[, p_emp := n/total]
pos <- merge(pos, subset(tmp, select = -c(value, n, total)), by = c('arm_label','time_label','value_label'))

p <- ggplot(data = pos, aes( x = value_label, fill = interaction(arm_label,time_label))) +
  geom_bar(aes( y = p_emp), 
           stat = 'identity', 
           position = position_dodge()) +
  geom_boxplot( aes(ymin = q_lower, lower = iqr_lower, middle = median, upper = iqr_upper, ymax = q_upper), 
                 position = position_dodge(0.9),
                stat = 'identity',
                alpha = 0,
                width = 0.3,
                outliers = FALSE) +
  scale_fill_npg() +
  scale_y_continuous(labels = scales::percent) +
  labs(x = '\nParental mental health\nPHQ4 outcome', y = 'group-level\nposterior probability', fill = '') +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 45,vjust = 1,hjust = 1),
        legend.position = 'bottom'
        )
ggsave(file = file.path(dir.out, paste0(file.prefix,'m0a_probs_barplot.png')), 
       plot = p, 
       h = 8, 
       w = 8
       )
```


This is the model fit (in black) relative to the aggregated survey items. The model is not flexible enough to capture the proportions for each group exactly. 

```{r ordered_logistic_no_unit_effects_v0_plot, include=TRUE, eval=TRUE, echo=FALSE, tidy=FALSE, fig.align='center', out.width='80%'}
knitr::include_graphics( file.path(dir.out, paste0(file.prefix,'m0a_probs_barplot.png')) )
```

In the model above, there are $P-1$ free group parameters and $K-1$ cut point parameters, so $9$ in total. We aim to describe $4*4 = 16$ probabilities across the groups that must sum to $1$, so are free to determine $12$ values. This implies that the model above is UNDER-SPECIFIED: it aims to explain the data parsimoniously with fewer parameters compared to the complexity of our target statistics. 

The primary reason for the poor model fit is that there is only one way how the logit-space is segemented to create the categorical probabilities for each of the four groups. A straightforward way to increase the expressiveness of the model is to include cut points for each of the $4$ groups. That way, the logit space can be appropriately segmented for each group, enabling us to match the observed group-specific probabilities.

The updated model, still without participant effects, is as follows: 

\begin{align*}
& \text{Prob}(Y_{ij} = k) = \text{Ordered-Logistic}(k; \eta_{ij}, c_{G(i)}) \\
& \eta_{ij}= \beta_{G(i)}\\
& c_g = [c_{g1}, c_{g1} + \text{cumsum}(\delta_{g,1:{K-2}})] \\
& c_{g1} \sim \text{Normal}(0 , 3.5^2)\\
& \delta_{g,1:{K-2}} \sim \text{Normal}(0 , 2.5^2)\\
& \beta \sim \text{S2Z-Normal}(0, 1)
\end{align*}

The main difference to brms code is that for each group we have enough free parameters to describe the four categorical probabilities. That is, because they need to sum to 1, there are 3 free parameters for the 4 possible Likert outcomes per group, and 12 free parameters across the 4 groups in total.

```{r ordered_logistic_no_unit_effects_v0_make_plots_2, include=FALSE, eval=TRUE, echo=FALSE, tidy=FALSE, cache=TRUE}
# Frequency plot
tmp <- dcat1[, list(n = length(pid)), by = c('arm_label','time_label','variable','value_label', 'value')]
tmp2 <- tmp[, list(total = sum(n)), by = c('arm_label','time_label','variable')]
tmp <- merge(tmp, tmp2, by = c('arm_label','time_label','variable'))
tmp[, p_emp_by_variable := n/total]


p <- ggplot(tmp, aes(x = value_label)) +
  geom_col(aes(fill = variable, y = p_emp_by_variable), position = position_dodge()) +
  geom_point(data = pos, aes(y = median), pch = 18, size = 2, colour = 'black') +
  geom_linerange(data = pos, aes(ymin = q_lower, ymax = q_upper), colour = 'black') +
  scale_fill_bmj() +
  scale_y_continuous(labels = scales::percent) +
  facet_grid(arm_label ~ time_label) +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 45,vjust = 1,hjust = 1),
        legend.position = 'bottom'
        ) +
  labs(x = '', x = 'Likert scale', y = 'proportion of outcomes', fill = '')
ggsave(file = file.path(dir.out, paste0(file.prefix,'m0a_frequencies_v2.png')), 
       plot = p, 
       h = 10, 
       w = 8
       )
```

```{r ordered_logistic_no_unit_effects_stan_code, include=TRUE, eval=TRUE, echo=TRUE, tidy=FALSE, cache=TRUE}
file.prefix <- 'parentalmh_ordered_categorical_'
# this is a vanilla model without unit effects
ordered_categorical_logit_txt_m0 <- "
data
{
  int<lower=1> N; // number of observations
  int<lower=3> K; // number of categories
  int<lower=2> P; // number of groups
  array[N] int<lower=1, upper=K> y; // observations
  array [N] int<lower=1,upper=P> group_of_obs;
}
transformed data
{
    real s2z_sd_groups;
    s2z_sd_groups = inv(sqrt(1. - inv(P)));
}
parameters
{
  sum_to_zero_vector[P] beta_group;
  
  // the cutpoints in logit space, using the 'ordered' variable type
  real cut_point_1;
  matrix<lower=0>[K - 2,P] cut_point_gaps_groups;
}
transformed parameters
{
  array[P] ordered[K-1] cut_points;
  for (p in 1:P)
  {
    cut_points[p] = 
      append_row(
        rep_vector(cut_point_1, 1), 
        rep_vector(cut_point_1, K-2) + cumulative_sum(cut_point_gaps_groups[,p])
        );
  }
}
model
{
  // likelihood, cannot be vectorized
  for (n in 1:N) {
    y[n] ~ ordered_logistic(
      beta_group[group_of_obs[n]], 
      cut_points[group_of_obs[n]]);
  }
  
  // priors
  beta_group ~ normal(0, s2z_sd_groups); 
  cut_point_1 ~ normal(0, 3.5); // gtools::inv.logit(2*3.5) = 0.9991
  to_vector(cut_point_gaps_groups) ~ normal(0, 2.5); // gtools::inv.logit(2*2) = 0.98
}
generated quantities
{
  matrix [K,P] ordered_prob;
  array [N] int<lower=0> ypred;
  array [N] real log_lik;
  
  for (p in 1:P) {
    ordered_prob[1,p]       = 1 - inv_logit( beta_group[p] - cut_points[p][1] );
    ordered_prob[2:(K-1),p] = inv_logit( beta_group[p] - cut_points[p][1:(K-2)] ) 
                              - 
                              inv_logit( beta_group[p] - cut_points[p][2:(K-1)] );
    ordered_prob[K,p]       = inv_logit( beta_group[p] - cut_points[p][K-1] );
  }
    
  for( n in 1:N ) {
    ypred[n] = ordered_logistic_rng( 
                beta_group[group_of_obs[n]], 
                cut_points[group_of_obs[n]]
                ); 
    log_lik[n] =  ordered_logistic_lpmf( y[n] |
                    beta_group[group_of_obs[n]], 
                    cut_points[group_of_obs[n]]);
  }
}
"

# compile the model
ordered_categorical_logit_filename_m0 <- cmdstanr::write_stan_file(
  gsub('\t',' ', ordered_categorical_logit_txt_m0),
  dir = dir.out,
  basename = NULL,
  force_overwrite = FALSE,
  hash_salt = ""
)

# compile Stan model
ordered_categorical_logit_compiled_m0 <- cmdstanr::cmdstan_model(ordered_categorical_logit_filename_m0, force_recompile = TRUE)
```

Let us fit this model:

```{r ordered_logistic_no_unit_effects_run_model, include=TRUE, eval=TRUE, echo=TRUE, message=FALSE, tidy=FALSE, cache=TRUE}
# define data in format needed for model specification
stan_data <- list()
stan_data$N <- nrow(dcat1)
stan_data$P <- max(dcat1$group)
stan_data$K <- length(unique(dcat1$value))
stan_data$y <- dcat1$value
stan_data$group_of_obs <- dcat1$group

# sample
ordered_categorical_logit_fit_m0 <- ordered_categorical_logit_compiled_m0$sample(
  data = stan_data,
  seed = 123,
  chains = 2,
  parallel_chains = 2,
  iter_warmup = 5e2,
  iter_sampling = 15e2,
  refresh = 500,
  save_warmup = TRUE
)

# save output to RDS
ordered_categorical_logit_fit_m0$save_object(file = file.path(dir.out, paste0(file.prefix,"m0_stan.rds")))
```
```{r ordered_logistic_no_unit_effects_load_model, include=FALSE, eval=TRUE, echo=FALSE, tidy=FALSE, cache=TRUE}
ordered_categorical_logit_fit_m0 <- readRDS(file = file.path(dir.out, paste0(file.prefix,'m0_stan.rds')))
```

```{r ordered_logistic_no_unit_effects_check mixing convergence, include=FALSE, eval=TRUE, echo=FALSE, tidy=FALSE, cache=TRUE}
#	check convergence and check mixing
#	get 95% credible intervals
tmp <- ordered_categorical_logit_fit_m0$summary(
  variables = c('beta_group','cut_point_1','cut_point_gaps_groups'),
  posterior::default_summary_measures(),
  posterior::default_convergence_measures(),
  extra_quantiles = ~posterior::quantile2(., probs = c(.0275, .975))
)
tmp <- as.data.table(tmp)
tmp
  
# parameter with lowest ess_bulk
worst_var <- tmp$variable[ which.min(tmp$ess_bulk) ]

# extract samples
po <- ordered_categorical_logit_fit_m0$draws(
  variables = c("lp__", worst_var),
  inc_warmup = TRUE,
  format = "draws_array"
  )

# make worst trace plot
p <- bayesplot:::mcmc_trace(po,  
                            pars = c("lp__",worst_var), 
                            n_warmup = 500,
                            facet_args = list(nrow = 2)
                            )
p <- p + theme_bw()
ggsave(file = file.path(dir.out, paste0(file.prefix,'m0_worsttrace.png')), 
       plot = p, 
       h = 10, 
       w = 8
       )

# make pairs plot
po <- ordered_categorical_logit_fit_m0$draws(
  variables = c('beta_group','cut_point_1','cut_point_gaps_groups'),
  inc_warmup = FALSE,
  format = "draws_array"
  )

color_scheme_set("teal")
p <- bayesplot::mcmc_intervals(po, prob = 0.5, prob_outer = 0.95, outer_size = 1, point_size = 2) +
  theme_bw()
ggsave(file = file.path(dir.out, paste0(file.prefix,'m0_intervals.png')), 
       plot = p, 
       h = 7, 
       w = 8, 
       limitsize = FALSE
       )

po <- ordered_categorical_logit_fit_m0$draws(
  variables = c('lp__','ordered_prob[1,1]','ordered_prob[2,1]','ordered_prob[3,1]','ordered_prob[4,1]','beta_group[1]','cut_point_1','cut_point_gaps_groups'),
  inc_warmup = FALSE,
  format = "draws_array"
  )

bayesplot::color_scheme_set('viridisC')
p <- bayesplot::mcmc_pairs(po, 
                           diag_fun = "dens", 
                           off_diag_fun = "hex"
                           ) 
ggsave(file = file.path(dir.out, paste0(file.prefix,'m0_pairsplot.png')), 
       plot = p, 
       h = 50, 
       w = 50, 
       limitsize = FALSE
       )
bayesplot::color_scheme_set('brewer-RdYlBu')

# make posterior predictive check

# create median and 95\% credible intervals
po <- ordered_categorical_logit_fit_m0$draws(
  variables = c('ypred'),
  inc_warmup = FALSE,
  format = "draws_df"
  )
po <- as.data.table(po)
po <- data.table::melt(po, id.vars = c('.draw','.chain','.iteration'), value.name = 'ypred')
po[, oid := as.integer(gsub('ypred\\[([0-9]+)\\]','\\1',variable)) ]
pos <- 
  po[,
     list( summary_value = quantile(ypred, prob = c(0.025, 0.25, 0.5, 0.75, 0.975)),
           summary_name = c('q_lower','iqr_lower','median','iqr_upper','q_upper') 
           ),
     by = 'oid'
     ]
pos <- 
  data.table::dcast(pos,
                    oid ~ summary_name, 
                    value.var = 'summary_value'
                    )
pos <- merge(pos, dcat1, by = 'oid')
pos[, IN_PPI := value >= q_lower & value <= q_upper]
pos[, mean(IN_PPI)]

# plot posterior predictive check
p <- ggplot(pos, aes(x = oid, group = oid)) + 
  geom_boxplot( aes( ymin = q_lower,
                     lower = iqr_lower,
                     middle = median,
                     upper = iqr_upper,
                     ymax = q_upper),
                stat = 'identity') +
  geom_point( aes(y = value, colour = IN_PPI ) ) +
  facet_grid(variable ~ arm_label+time_label, scales = 'free') +
  scale_x_discrete() +
  scale_y_continuous() +
  ggsci::scale_color_npg() +
  labs(x = '', 
       y = 'outcome', 
       colour = 'within\n95% posterior\nprediction\ninterval') +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 45,vjust = 1,hjust = 1))
ggsave(file = file.path(dir.out, paste0(file.prefix,'m0_ppcheck.png')), 
       p, 
       w = 12, 
       h = 12)
```

```{r ordered_logistic_no_unit_effects_extract_class_probs, include=TRUE, eval=TRUE, echo=TRUE, tidy=FALSE, cache=TRUE}
# extract model probabilities
po <- ordered_categorical_logit_fit_m0$draws(
  variables = c('ordered_prob'),
  inc_warmup = FALSE,
  format = "draws_df"
  )
po <- as.data.table(po)
setnames(po, colnames(po), gsub('\\]','',gsub('\\[','_',colnames(po))))
po <- data.table::melt(po, id.vars = c('.draw','.chain','.iteration'), value.name = 'prob')
po[, value := as.integer(gsub('ordered_prob_([0-9]),([0-9])','\\1',variable)) ]
po[, group := as.integer(gsub('ordered_prob_([0-9]),([0-9])','\\2',variable)) ]

tmp <- unique( subset(dcat1, select = c(group, arm, arm_label, time, time_label, value, value_label)) )
po <- merge(po, tmp, by = c('group','value'))
```

```{r ordered_logistic_no_unit_effects_make_plots, include=FALSE, eval=TRUE, echo=FALSE, tidy=FALSE, cache=TRUE}
pos <- po[, 
          list( summary_value = quantile(prob, prob = c(0.025, 0.25, 0.5, 0.75, 0.975)),
                summary_name = c('q_lower','iqr_lower','median','iqr_upper','q_upper') 
                ), 
          by = c('group','arm','arm_label','time','time_label','value','value_label')
          ]
pos <- data.table::dcast(pos, group + arm + arm_label + time + time_label + value + value_label ~ summary_name, value.var = 'summary_value')

tmp <- dcat1[, list(n = length(pid)), by = c('arm_label','time_label','value_label','value')]
tmp2 <- tmp[, list(total = sum(n)), by = c('arm_label','time_label')]
tmp <- merge(tmp, tmp2, by = c('arm_label','time_label'))
tmp[, p_emp := n/total]
pos <- merge(pos, subset(tmp, select = -c(value, n, total)), by = c('arm_label','time_label','value_label'))

p <- ggplot(data = pos, aes( x = value_label, fill = interaction(arm_label,time_label))) +
  geom_bar(aes( y = p_emp), 
           stat = 'identity', 
           position = position_dodge()) +
  geom_boxplot( aes(ymin = q_lower, lower = iqr_lower, middle = median, upper = iqr_upper, ymax = q_upper), 
                 position = position_dodge(0.9),
                stat = 'identity',
                alpha = 0,
                width = 0.3,
                outliers = FALSE) +
  scale_fill_npg() +
  scale_y_continuous(labels = scales::percent) +
  labs(x = '\nParental mental health\nPHQ4 outcome', y = 'group-level\nposterior probability', fill = '') +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 45,vjust = 1,hjust = 1),
        legend.position = 'bottom'
        )
ggsave(file = file.path(dir.out, paste0(file.prefix,'m0_probs_barplot.png')), 
       plot = p, 
       h = 8, 
       w = 8
       )
```

This is the model fit (in black) relative to the aggregated survey items. Hooray, the model is flexible enough to capture the proportions for each Likert outcome and each group exactly.

```{r ordered_logistic_no_unit_effects_plot, include=TRUE, eval=TRUE, echo=FALSE, tidy=FALSE, fig.align='center', out.width='80%'}
knitr::include_graphics( file.path(dir.out, paste0(file.prefix,'m0_probs_barplot.png')) )
```


```{r ordered_logistic_no_unit_effects_make_plots_2, include=FALSE, eval=TRUE, echo=FALSE, tidy=FALSE, cache=TRUE}
# Frequency plot
tmp <- dcat1[, list(n = length(pid)), by = c('arm_label','time_label','variable','value_label', 'value')]
tmp2 <- tmp[, list(total = sum(n)), by = c('arm_label','time_label','variable')]
tmp <- merge(tmp, tmp2, by = c('arm_label','time_label','variable'))
tmp[, p_emp_by_variable := n/total]


p <- ggplot(tmp, aes(x = value_label)) +
  geom_col(aes(fill = variable, y = p_emp_by_variable), position = position_dodge()) +
  geom_point(data = pos, aes(y = median), pch = 18, size = 2, colour = 'black') +
  geom_linerange(data = pos, aes(ymin = q_lower, ymax = q_upper), colour = 'black') +
  scale_fill_bmj() +
  scale_y_continuous(labels = scales::percent) +
  facet_grid(arm_label ~ time_label) +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 45,vjust = 1,hjust = 1),
        legend.position = 'bottom'
        ) +
  labs(x = '', x = 'Likert scale', y = 'proportion of outcomes', fill = '')
ggsave(file = file.path(dir.out, paste0(file.prefix,'m0_frequencies_v2.png')), 
       plot = p, 
       h = 10, 
       w = 8
       )
```

This is the model fit (in black) relative to each survey item, showing the 95% posterior uncertainty interval in whiskers, the 50% posterior uncertainty interval as a box and the posterior median as a thick black line. There remains unexplained heterogeneity across each 
survey item. We consider this okay because each question elicits the underlying target behaviour in a slightly 
different way.

```{r ordered_logistic_no_unit_effects_plot_2, include=TRUE, eval=TRUE, echo=FALSE, tidy=FALSE, fig.align='center', out.width='80%'}
knitr::include_graphics( file.path(dir.out, paste0(file.prefix,'m0_frequencies_v2.png')) )
```

```{r ordered_logistic_no_unit_effects_polychoric_corrs, include=FALSE, eval=TRUE, echo=FALSE, tidy=FALSE, cache=TRUE}
po <- ordered_categorical_logit_fit_m0$draws(
  variables = c('ypred'),
  inc_warmup = FALSE,
  format = "draws_df"
  )
po <- as.data.table(po)
po <- data.table::melt(po, id.vars = c('.draw','.chain','.iteration'), value.name = 'value')
po[, oid := as.integer(gsub('ypred\\[([0-9]+)\\]','\\1',variable)) ]
set(po, NULL, 'variable', NULL)
tmp <- unique(subset(dcat1, select = c(oid, pid, variable, group )))
po <- merge(po, tmp, by = c('oid'))
tmp <- unique(subset(dcat1, select = c(value, value_label )))
po <- merge(po, tmp, by = c('value'))
po <- data.table::dcast( po, .draw + pid + group ~ variable, value.var = 'value_label')
po <- melt(po, id.vars = c('.draw','pid','group','PHQ4_down'), value.name = 'value_label')
po <- po[, list(k = length(pid)), by = c('.draw', 'group', 'PHQ4_down', 'value_label')]
tmp <- po[, list(n = sum(k)), by = c('.draw', 'group', 'PHQ4_down')]
po <- merge(po, tmp, by = c('.draw', 'group', 'PHQ4_down'))
po[, prob := k/n]
tmp <- CJ(.draw = unique(po$.draw),
          group = unique(po$group), 
          PHQ4_down = unique(po$PHQ4_down), 
          value_label = unique(po$PHQ4_down))
po <- merge(tmp, po, by = c('.draw','group','PHQ4_down','value_label'), all.x = TRUE)
set(po, po[, which(is.na(prob))], 'prob', 0.)
pos <- po[, 
          list( summary_value = quantile(prob, prob = c(0.025, 0.25, 0.5, 0.75, 0.975)),
                summary_name = c('q_lower','iqr_lower','median','iqr_upper','q_upper') 
                ), 
          by = c('group','PHQ4_down','value_label')
          ]
pos <- data.table::dcast(pos, group + PHQ4_down + value_label ~ summary_name, value.var = 'summary_value')
tmp <- unique( subset(dcat1, select = c(group, arm, arm_label, time, time_label)) )
pos <- merge(pos, tmp, by = c('group'))

tmp <- data.table:::dcast( dcat1, time_label + arm_label + pid ~ variable, value.var = 'value_label')
tmp <- melt(tmp, id.vars = c('time_label','arm_label','pid','PHQ4_down'))
tmp <- tmp[, list(k = length(pid)), by = c('time_label','arm_label','PHQ4_down','value')]
tmp2 <- tmp[, list(n = sum(k)), by = c('time_label','arm_label','PHQ4_down')]
tmp <- merge(tmp, tmp2, by = c('time_label','arm_label','PHQ4_down'))
tmp[, p_emp := k/n]
set(tmp, NULL, c('k','n'), NULL)
tmp2 <- CJ(time_label = unique(tmp$time_label), 
           arm_label = unique(tmp$arm_label), 
           PHQ4_down = unique(tmp$PHQ4_down), 
           value = unique(tmp$PHQ4_down))
tmp <- merge(tmp2, tmp, by = c('arm_label','time_label','PHQ4_down','value'), all.x = TRUE)
set(tmp, tmp[, which(is.na(p_emp))], 'p_emp', 0.)
setnames(tmp, 'value','value_label')
pos <- merge(pos, tmp, by = c('arm_label','time_label','PHQ4_down','value_label'), all.y = TRUE)

p <- ggplot(pos, aes( x = PHQ4_down, group = interaction(PHQ4_down, value_label))) +
  geom_col(aes(y = p_emp, fill = value_label), 
           position = position_dodge(0.9)) +
  geom_boxplot( aes(ymin = q_lower, lower = iqr_lower, middle = median, upper = iqr_upper, ymax = q_upper), 
                position = position_dodge(0.9),
                stat = 'identity',
                alpha = 0,
                width = 0.3) +
  scale_fill_bmj() +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 45,vjust = 1,hjust = 1),
        legend.position = 'bottom'
        ) +
  labs(x = 'response to PHQ4_down question', y = 'proportion', fill = 'responses to other PHQ4 questions') +
  facet_grid(arm_label ~ time_label)
ggsave(file = file.path(dir.out, paste0(file.prefix,'m0_polychoric_correlations_across_PHQ_questions_same_arm-time.png')),
       plot = p, 
       h = 8, 
       w = 8
       )
```
Here is a visualisation of the estimated polychoric correlations. We can clearly see that the model is unable to capture 
correlations between item responses. We will revisit this further down below.

```{r ordered_logistic_no_unit_effects_polychoric_corrs_plot, include=TRUE, eval=TRUE, echo=FALSE, tidy=FALSE, fig.align='center', out.width='80%'}
knitr::include_graphics( file.path(dir.out, paste0(file.prefix,'m0_polychoric_correlations_across_PHQ_questions_same_arm-time.png')) )
```


## Models with participant effects

One approach to introduce clustering of responses within the same participant is to suppose that each participant has their own latent underlying factor/trait. We can implement this by introducing random effects around the group-mean latent factor. The updated model, now with participant effects, is as follows: 

\begin{align*}
& \text{Prob}(Y_{ij} = k) = \text{Ordered-Logistic}(k; \eta_{ij}, c_{G(i)}) \\
& \eta_{ij}= \beta_{G(i)} + \gamma_i\\
& \beta \sim \text{S2Z-Normal}(0, 1)\\
& \gamma \sim \text{S2Z-Normal}(0, \sigma_\gamma^2)\\
& \sigma_\gamma \sim \text{Exponential}(2)\\
& c_g = [c_{g1}, c_{g1} + \text{cumsum}(\delta_{g,1:{K-2}})] \\
& c_{g1} \sim \text{Normal}(0 , 3.5^2)\\
& \delta_{g,1:{K-2}} \sim \text{Normal}(0 , 2.5^2)
\end{align*}

To prepare adding participant effects to the ordered categorical model, I investigated the prior densities of the ordered categorial model in detail on various versions of the PHQ data set. It turned out that the prior on the 'cut_point_gaps_groups' had to be specified with a broad variance such as 'normal(0, 2.5)', in order for the baseline model to accurately estimate the empirical frequencies within 50% posterior credible intervals. Once this was established, I changed the priors accordingly in the previous models too.

I also wanted that the model with participant effects closely reproduces the estimates of the baseline model when each participant provides exactly one observation point so that there are no within-participant correlations. I achieved this with a 'beta_unit_sd ~ exponential(2)' prior on the participant random effects model. A standard 'cauchy(0,1)' prior failed in this regard, as it pronounced curvature in the posterior geometry that in turn seems to be associated with numerically unstable evaluations of the log likelihood.

Let me illustrate these findings. First though, here is the Stan code of the ordered categorical model with participant effects. I incorporated an Exponential prior on the participant random effect sd's to help achieve accurate estimates when the model is overly complex/has too many free parameters, but otherwise this follows on straightforwardly from the model without participant effects above.


```{r ordered_logistic_with_unit_effects_endl_otherpriors_stan_code, include=TRUE, eval=TRUE, echo=TRUE, tidy=FALSE, cache=TRUE}
ordered_categorical_logit_txt_m1 <- "
data
{
  int<lower=1> N; // number of observations
  int<lower=3> K; // number of categories
  int<lower=2> P; // number of groups
  int<lower=1> U; // number of units
  array [N] int<lower=1, upper=K> y; // observations
  array [N] int<lower=1,upper=P> group_of_obs;
  array [N] int<lower=1,upper=U> unit_of_obs;
}
transformed data
{
  real s2z_sd_groups;
  real s2z_sd_unit;
  s2z_sd_groups = inv(sqrt(1. - inv(P)));
  s2z_sd_unit = inv(sqrt(1. - inv(U)));
}
parameters
{
  sum_to_zero_vector[P] beta_group;
  real<lower=0> beta_unit_sd;
  sum_to_zero_vector[U] beta_unit;
  
  // the cutpoints in logit space, using the 'ordered' variable type
  real cut_point_1;
  matrix<lower=0>[K - 2,P] cut_point_gaps_groups;
}
transformed parameters
{
  array[P] ordered[K-1] cut_points;
  for (p in 1:P)
  {
    cut_points[p] = 
      append_row(
        rep_vector(cut_point_1, 1), 
        rep_vector(cut_point_1, K-2) + cumulative_sum(cut_point_gaps_groups[,p])
        );
  }
}
model
{
  // likelihood, cannot be vectorized
  for (n in 1:N) {
    y[n] ~ ordered_logistic(
      beta_group[group_of_obs[n]] + 
        beta_unit_sd * beta_unit[unit_of_obs[n]], 
      cut_points[group_of_obs[n]]);
  }
  
  // priors
  beta_group ~ normal(0, s2z_sd_groups); 
  beta_unit ~ normal(0, s2z_sd_unit); 
  beta_unit_sd ~ exponential(2); // a +1 increase corresponds to ~ doubling of prob, 
                                 // although there is an issue in that for small probs, the same increment provides larger multipliers
                                 // gtools::inv.logit(c(-3,-2)) = 0.04742587 0.11920292
                                 // gtools::inv.logit(c(-1,0)) = 0.2689414 0.5000000
  cut_point_1 ~ normal(0, 3.5); // gtools::inv.logit(2*3.5) = 0.999
  to_vector(cut_point_gaps_groups) ~ normal(0, 2.5); // gtools::inv.logit(-3 + 2 = 1) = 0.26, empirically we know the largest next prob is ~ +0.20
}
generated quantities
{
  array [K,N] real<lower=0, upper=1> ordered_prob_by_obs;
  array [N] int<lower=0> ypred;
  array [N] real log_lik;
  vector[U] beta_unit_re;
  
  // now need to calculate ordered_prob for each observation
  {
    real tmp_real;
    for (n in 1:N) 
    {
        tmp_real = beta_group[group_of_obs[n]] + beta_unit_sd * beta_unit[unit_of_obs[n]];
        ordered_prob_by_obs[1,n]       = 1 - inv_logit( tmp_real - cut_points[group_of_obs[n]][1] );
        ordered_prob_by_obs[2:(K-1),n] = to_array_1d( inv_logit( tmp_real - cut_points[group_of_obs[n]][1:(K-2)] ) 
                                                                  - 
                                                                  inv_logit( tmp_real - cut_points[group_of_obs[n]][2:(K-1)] )
                                                                  );
        ordered_prob_by_obs[K,n]       = inv_logit( tmp_real - cut_points[group_of_obs[n]][K-1] );
    }
  }
  
  for( n in 1:N ) {
    ypred[n] = ordered_logistic_rng( 
                beta_group[group_of_obs[n]] + 
                  beta_unit_sd * beta_unit[unit_of_obs[n]], 
                cut_points[group_of_obs[n]]
                ); 
    log_lik[n] =  ordered_logistic_lpmf( y[n] |
                    beta_group[group_of_obs[n]] + 
                      beta_unit_sd * beta_unit[unit_of_obs[n]], 
                    cut_points[group_of_obs[n]]);
  }
  
  beta_unit_re = beta_unit_sd * beta_unit;
}
"

# compile the model
ordered_categorical_logit_m1_filename <- cmdstanr::write_stan_file(
  gsub('\t',' ', ordered_categorical_logit_txt_m1),
  dir = dir.out,
  basename = NULL,
  force_overwrite = FALSE,
  hash_salt = ""
)

# compile Stan model
ordered_categorical_logit_compiled_m1 <- cmdstanr::cmdstan_model(ordered_categorical_logit_m1_filename)
```



### Baseline model without participant effects on endline only data

```{r ordered_logistic_without_unit_effects_endl_otherpriors_stan_code, include=FALSE, eval=TRUE, echo=FALSE, tidy=FALSE, cache=TRUE}
file.prefix <- 'parentalmh_ordered_categorical_endlineonly_priorsv2_'

# this is a vanilla model without unit effects
ordered_categorical_logit_txt_m3 <- "
data
{
  int<lower=1> N; // number of observations
  int<lower=3> K; // number of categories
  int<lower=2> P; // number of groups
  array[N] int<lower=1, upper=K> y; // observations
  array [N] int<lower=1,upper=P> group_of_obs;
}
transformed data
{
    real s2z_sd_groups;
    s2z_sd_groups = inv(sqrt(1. - inv(P)));
}
parameters
{
  sum_to_zero_vector[P] beta_group;
  
  // the cutpoints in logit space, using the 'ordered' variable type
  real cut_point_1;
  matrix<lower=0>[K - 2,P] cut_point_gaps_groups;
}
transformed parameters
{
  array[P] ordered[K-1] cut_points;
  for (p in 1:P)
  {
    cut_points[p] = 
      append_row(
        rep_vector(cut_point_1, 1), 
        rep_vector(cut_point_1, K-2) + cumulative_sum(cut_point_gaps_groups[,p])
        );
  }
}
model
{
  // likelihood, cannot be vectorized
  for (n in 1:N) {
    y[n] ~ ordered_logistic(
      beta_group[group_of_obs[n]], 
      cut_points[group_of_obs[n]]);
  }
  
  // priors
  beta_group ~ normal(0, s2z_sd_groups); 
  cut_point_1 ~ normal(0, 3.5); // gtools::inv.logit(2*3.5) = 0.9991
  to_vector(cut_point_gaps_groups) ~ normal(0, 2.5); // gtools::inv.logit(2*2) = 0.98
}
generated quantities
{
  matrix [K,P] ordered_prob;
  array [N] int<lower=0> ypred;
  array [N] real log_lik;
  
  for (p in 1:P) {
    ordered_prob[1,p]       = 1 - inv_logit( beta_group[p] - cut_points[p][1] );
    ordered_prob[2:(K-1),p] = inv_logit( beta_group[p] - cut_points[p][1:(K-2)] ) 
                              - 
                              inv_logit( beta_group[p] - cut_points[p][2:(K-1)] );
    ordered_prob[K,p]       = inv_logit( beta_group[p] - cut_points[p][K-1] );
  }
    
  for( n in 1:N ) {
    ypred[n] = ordered_logistic_rng( 
                beta_group[group_of_obs[n]], 
                cut_points[group_of_obs[n]]
                ); 
    log_lik[n] =  ordered_logistic_lpmf( y[n] |
                    beta_group[group_of_obs[n]], 
                    cut_points[group_of_obs[n]]);
  }
}
"

# compile the model
ordered_categorical_logit_filename_m3 <- cmdstanr::write_stan_file(
  gsub('\t',' ', ordered_categorical_logit_txt_m3),
  dir = dir.out,
  basename = NULL,
  force_overwrite = FALSE,
  hash_salt = ""
)

# compile Stan model
ordered_categorical_logit_compiled_m3 <- cmdstanr::cmdstan_model(ordered_categorical_logit_filename_m3, force_recompile = TRUE)
```

Here I engineer the situation that we have one observation per participant, using the endline data:

```{r ordered_logistic_without_unit_effects_endl_otherpriors_select_data, include=TRUE, eval=TRUE, echo=TRUE, message=FALSE, tidy=FALSE, cache=TRUE}
file.prefix <- 'parentalmh_ordered_categorical_endlineonly_priorsv2_'

# reduce data to one obs per participant
dcat1e <- subset(dcat1, time_label == 'Endline')
dcat1e[, group := group - 2L]
tmp <- dcat1e[, list(value = quantile(value, p = 0.5, type = 1)), by = c('arm','pid')]
dcat1e <- merge(dcat1e, tmp, by = c('arm','pid','value'))
dcat1e <- unique(dcat1e, by = c('arm','pid','value'))
dcat1e[, oid := 1:nrow(dcat1e)]

dcat1e[, table(pid)]

dcat1e[, table(arm)]
```

```{r ordered_logistic_without_unit_effects_endl_otherpriors_run_model, include=FALSE, eval=TRUE, echo=FALSE, message=FALSE, tidy=FALSE, cache=TRUE}
# define data in format needed for model specification
stan_data <- list()
stan_data$N <- nrow(dcat1e)
stan_data$P <- max(dcat1e$group)
stan_data$U <- max(dcat1e$pid)
stan_data$K <- length(unique(dcat1e$value))
stan_data$y <- dcat1e$value
stan_data$group_of_obs <- dcat1e$group
stan_data$unit_of_obs <- dcat1e$pid

# sample
ordered_categorical_logit_endl_fit_m3 <- ordered_categorical_logit_compiled_m3$sample(
  data = stan_data,
  seed = 123,
  chains = 2,
  parallel_chains = 2,
  iter_warmup = 5e2,
  iter_sampling = 15e2,
  refresh = 500,
  save_warmup = TRUE
)

# save output to RDS
ordered_categorical_logit_endl_fit_m3$save_object(file = file.path(dir.out, paste0(file.prefix,"m3_stan.rds")))
```

```{r ordered_logistic_without_unit_effects_endl_otherpriors_load_model, include=FALSE, eval=TRUE, echo=FALSE, tidy=FALSE, cache=TRUE}
ordered_categorical_logit_endl_fit_m3 <- readRDS(file = file.path(dir.out, paste0(file.prefix,'m3_stan.rds')))
```

```{r ordered_logistic_without_unit_effects_endl_otherpriors_check mixing convergence, include=FALSE, eval=TRUE, echo=FALSE, tidy=FALSE, cache=TRUE}
#	check convergence and check mixing
#	get 95% credible intervals
tmp <- ordered_categorical_logit_endl_fit_m3$summary(
  variables = c('beta_group','cut_point_1','cut_point_gaps_groups'),
  posterior::default_summary_measures(),
  posterior::default_convergence_measures(),
  extra_quantiles = ~posterior::quantile2(., probs = c(.0275, .975))
)
tmp <- as.data.table(tmp)
tmp
  
# parameter with lowest ess_bulk
worst_var <- tmp$variable[ which.min(tmp$ess_bulk) ]

# extract samples
po <- ordered_categorical_logit_endl_fit_m3$draws(
  variables = c("lp__", worst_var),
  inc_warmup = TRUE,
  format = "draws_array"
  )

# make worst trace plot
p <- bayesplot:::mcmc_trace(po,  
                            pars = c("lp__",worst_var), 
                            n_warmup = 500,
                            facet_args = list(nrow = 2)
                            )
p <- p + theme_bw()
ggsave(file = file.path(dir.out, paste0(file.prefix,'m3_worsttrace.png')), 
       plot = p, 
       h = 10, 
       w = 8
       )

# make pairs plot
po <- ordered_categorical_logit_endl_fit_m3$draws(
  variables = c('beta_group','cut_point_1','cut_point_gaps_groups'),
  inc_warmup = FALSE,
  format = "draws_array"
  )

color_scheme_set("teal")
p <- bayesplot::mcmc_intervals(po, prob = 0.5, prob_outer = 0.95, outer_size = 1, point_size = 2) +
  theme_bw()
ggsave(file = file.path(dir.out, paste0(file.prefix,'m3_intervals.png')), 
       plot = p, 
       h = 7, 
       w = 8, 
       limitsize = FALSE
       )

po <- ordered_categorical_logit_endl_fit_m3$draws(
  variables = c('lp__','ordered_prob[1,1]','ordered_prob[2,1]','ordered_prob[3,1]','ordered_prob[4,1]','beta_group[1]','cut_point_1','cut_point_gaps_groups'),
  inc_warmup = FALSE,
  format = "draws_array"
  )

bayesplot::color_scheme_set('viridisC')
p <- bayesplot::mcmc_pairs(po, 
                           diag_fun = "dens", 
                           off_diag_fun = "hex"
                           ) 
ggsave(file = file.path(dir.out, paste0(file.prefix,'m3_pairsplot.png')), 
       plot = p, 
       h = 50, 
       w = 50, 
       limitsize = FALSE
       )
bayesplot::color_scheme_set('brewer-RdYlBu')

# make posterior predictive check

# create median and 95\% credible intervals
po <- ordered_categorical_logit_endl_fit_m3$draws(
  variables = c('ypred'),
  inc_warmup = FALSE,
  format = "draws_df"
  )
po <- as.data.table(po)
po <- data.table::melt(po, id.vars = c('.draw','.chain','.iteration'), value.name = 'ypred')
po[, oid := as.integer(gsub('ypred\\[([0-9]+)\\]','\\1',variable)) ]
pos <- 
  po[,
     list( summary_value = quantile(ypred, prob = c(0.025, 0.25, 0.5, 0.75, 0.975)),
           summary_name = c('q_lower','iqr_lower','median','iqr_upper','q_upper') 
           ),
     by = 'oid'
     ]
pos <- 
  data.table::dcast(pos,
                    oid ~ summary_name, 
                    value.var = 'summary_value'
                    )
pos <- merge(pos, dcat1e, by = 'oid')
pos[, IN_PPI := value >= q_lower & value <= q_upper]
pos[, mean(IN_PPI)]

# plot posterior predictive check
p <- ggplot(pos, aes(x = oid, group = oid)) + 
  geom_boxplot( aes( ymin = q_lower,
                     lower = iqr_lower,
                     middle = median,
                     upper = iqr_upper,
                     ymax = q_upper),
                stat = 'identity') +
  geom_point( aes(y = value, colour = IN_PPI ) ) +
  facet_grid(variable ~ arm_label+time_label, scales = 'free') +
  scale_x_discrete() +
  scale_y_continuous() +
  ggsci::scale_color_npg() +
  labs(x = '', 
       y = 'outcome', 
       colour = 'within\n95% posterior\nprediction\ninterval') +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 45,vjust = 1,hjust = 1))
ggsave(file = file.path(dir.out, paste0(file.prefix,'m3_ppcheck.png')), 
       p, 
       w = 12, 
       h = 12)
```

```{r ordered_logistic_without_unit_effects_endl_otherpriors_class_probs, include=FALSE, eval=TRUE, echo=FALSE, tidy=FALSE, cache=TRUE}
# extract model probabilities
po <- ordered_categorical_logit_endl_fit_m3$draws(
  variables = c('ordered_prob'),
  inc_warmup = FALSE,
  format = "draws_df"
  )
po <- as.data.table(po)
setnames(po, colnames(po), gsub('\\]','',gsub('\\[','_',colnames(po))))
po <- data.table::melt(po, id.vars = c('.draw','.chain','.iteration'), value.name = 'prob')
po[, value := as.integer(gsub('ordered_prob_([0-9]),([0-9])','\\1',variable)) ]
po[, group := as.integer(gsub('ordered_prob_([0-9]),([0-9])','\\2',variable)) ]

tmp <- unique( subset(dcat1e, select = c(group, arm, arm_label, time, time_label, value, value_label)) )
po <- merge(po, tmp, by = c('group','value'))
```

```{r ordered_logistic_without_unit_effects_endl_otherpriors_make_plots, include=FALSE, eval=TRUE, echo=FALSE, tidy=FALSE, cache=TRUE}
pos <- po[, 
          list( summary_value = quantile(prob, prob = c(0.025, 0.25, 0.5, 0.75, 0.975)),
                summary_name = c('q_lower','iqr_lower','median','iqr_upper','q_upper') 
                ), 
          by = c('group','arm','arm_label','time','time_label','value','value_label')
          ]
pos <- data.table::dcast(pos, group + arm + arm_label + time + time_label + value + value_label ~ summary_name, value.var = 'summary_value')

tmp <- dcat1e[, list(n = length(pid)), by = c('arm_label','time_label','value_label','value')]
tmp2 <- tmp[, list(total = sum(n)), by = c('arm_label','time_label')]
tmp <- merge(tmp, tmp2, by = c('arm_label','time_label'))
tmp[, p_emp := n/total]
pos <- merge(pos, subset(tmp, select = -c(value, n, total)), by = c('arm_label','time_label','value_label'))

p <- ggplot(data = pos, aes( x = value_label, fill = interaction(arm_label,time_label))) +
  geom_bar(aes( y = p_emp), 
           stat = 'identity', 
           position = position_dodge()) +
  geom_boxplot( aes(ymin = q_lower, lower = iqr_lower, middle = median, upper = iqr_upper, ymax = q_upper), 
                 position = position_dodge(0.9),
                stat = 'identity',
                alpha = 0,
                width = 0.3,
                outliers = FALSE) +
  scale_fill_npg() +
  scale_y_continuous(labels = scales::percent) +
  labs(x = '\nParental mental health\nPHQ4 outcome', y = 'group-level\nposterior probability', fill = '') +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 45,vjust = 1,hjust = 1),
        legend.position = 'bottom'
        )
ggsave(file = file.path(dir.out, paste0(file.prefix,'m3_probs_barplot.png')), 
       plot = p, 
       h = 8, 
       w = 8
       )
```

I first fitted a model without participant effects on the endline only data as a benchmark:

```{r ordered_logistic_without_unit_effects_endl_otherpriors_plot, include=TRUE, eval=TRUE, echo=FALSE, tidy=FALSE, fig.align='center', out.width='80%'}
knitr::include_graphics( file.path(dir.out, paste0(file.prefix,'m3_probs_barplot.png')) )
```


### Model with participant effects on endline only data

I next fitted the ordered categorical model with participants effects to the endline only data.


```{r ordered_logistic_with_unit_effects_endl_run_model, include=TRUE, eval=TRUE, echo=TRUE, message=FALSE, tidy=FALSE, cache=TRUE}
file.prefix <- 'parentalmh_ordered_categorical_endlineonly_priorsv2_'

# define data in format needed for model specification
stan_data <- list()
stan_data$N <- nrow(dcat1e)
stan_data$P <- max(dcat1e$group)
stan_data$U <- max(dcat1e$pid)
stan_data$K <- length(unique(dcat1e$value))
stan_data$y <- dcat1e$value
stan_data$group_of_obs <- dcat1e$group
stan_data$unit_of_obs <- dcat1e$pid

# sample
ordered_categorical_logit_endl_fit_m1 <- ordered_categorical_logit_compiled_m1$sample(
  data = stan_data,
  seed = 123,
  chains = 2,
  parallel_chains = 2,
  iter_warmup = 5e2,
  iter_sampling = 15e2,
  refresh = 500,
  save_warmup = TRUE
)

# save output to RDS
ordered_categorical_logit_endl_fit_m1$save_object(file = file.path(dir.out, paste0(file.prefix,"m1_stan.rds")))
```

```{r ordered_logistic_with_unit_effects_endl_otherpriors_load_model, include=FALSE, eval=TRUE, echo=FALSE, tidy=FALSE, cache=TRUE}
ordered_categorical_logit_endl_fit_m1 <- readRDS(file = file.path(dir.out, paste0(file.prefix,'m1_stan.rds')))
```

```{r ordered_logistic_with_unit_effects_endl_otherpriors_check_mixing_convergence, include=FALSE, eval=TRUE, echo=FALSE, tidy=FALSE, cache=TRUE}
#	check convergence and check mixing
#	get 95% credible intervals
tmp <- ordered_categorical_logit_endl_fit_m1$summary(
  variables = c('beta_group','beta_unit','beta_unit_sd','cut_point_1','cut_point_gaps_groups'),
  posterior::default_summary_measures(),
  posterior::default_convergence_measures(),
  extra_quantiles = ~posterior::quantile2(., probs = c(.0275, .975))
)
tmp <- as.data.table(tmp)
tmp
  
# parameter with lowest ess_bulk
worst_var <- tmp$variable[ which.min(tmp$ess_bulk) ]

# extract samples
po <- ordered_categorical_logit_endl_fit_m1$draws(
  variables = c("lp__", worst_var),
  inc_warmup = TRUE,
  format = "draws_array"
  )

# make worst trace plot
p <- bayesplot:::mcmc_trace(po,  
                            pars = c("lp__",worst_var), 
                            n_warmup = 500,
                            facet_args = list(nrow = 2)
                            )
p <- p + theme_bw()
ggsave(file = file.path(dir.out, paste0(file.prefix,'m1_worsttrace.png')), 
       plot = p, 
       h = 10, 
       w = 8
       )

# make intervals plot
po <- ordered_categorical_logit_endl_fit_m1$draws(
  variables = c('beta_group','cut_point_1','cut_point_gaps_groups','beta_unit_sd','beta_unit_re'),
  inc_warmup = FALSE,
  format = "draws_array"
  )

color_scheme_set("teal")
p <- bayesplot::mcmc_intervals(po, prob = 0.5, prob_outer = 0.95, outer_size = 1, point_size = 2) +
  theme_bw()
ggsave(file = file.path(dir.out, paste0(file.prefix,'m1_intervals.png')), 
       plot = p, 
       h = 35, 
       w = 8, 
       limitsize = FALSE
       )

# make pairs plot
po <- ordered_categorical_logit_endl_fit_m1$draws(
  variables = c('lp__','beta_group[1]','cut_point_1','cut_point_gaps_groups','beta_unit[1]','beta_unit_re[1]','beta_unit[3]','beta_unit_re[3]'),
  inc_warmup = FALSE,
  format = "draws_array"
  )

bayesplot::color_scheme_set('viridisC')
p <- bayesplot::mcmc_pairs(po, 
                           diag_fun = "dens", 
                           off_diag_fun = "hex"
                           ) 
ggsave(file = file.path(dir.out, paste0(file.prefix,'m1_pairsplot.png')), 
       plot = p, 
       h = 50, 
       w = 50, 
       limitsize = FALSE
       )
bayesplot::color_scheme_set('brewer-RdYlBu')

# make posterior predictive check

# create median and 95\% credible intervals
po <- ordered_categorical_logit_endl_fit_m1$draws(
  variables = c('ypred'),
  inc_warmup = FALSE,
  format = "draws_df"
  )
po <- as.data.table(po)
po <- data.table::melt(po, id.vars = c('.draw','.chain','.iteration'), value.name = 'ypred')
po[, oid := as.integer(gsub('ypred\\[([0-9]+)\\]','\\1',variable)) ]
pos <- 
  po[,
     list( summary_value = quantile(ypred, prob = c(0.025, 0.25, 0.5, 0.75, 0.975)),
           summary_name = c('q_lower','iqr_lower','median','iqr_upper','q_upper') 
           ),
     by = 'oid'
     ]
pos <- 
  data.table::dcast(pos,
                    oid ~ summary_name, 
                    value.var = 'summary_value'
                    )
pos <- merge(pos, dcat1e, by = 'oid')
pos[, IN_PPI := value >= q_lower & value <= q_upper]

# plot posterior predictive check
p <- ggplot(pos, aes(x = oid, group = oid)) + 
  geom_boxplot( aes( ymin = q_lower,
                     lower = iqr_lower,
                     middle = median,
                     upper = iqr_upper,
                     ymax = q_upper),
                stat = 'identity') +
  geom_point( aes(y = value, colour = IN_PPI ) ) +
  facet_grid(variable ~ arm_label+time_label, scales = 'free') +
  scale_x_discrete() +
  scale_y_continuous() +
  ggsci::scale_color_npg() +
  labs(x = '', 
       y = 'outcome', 
       colour = 'within\n95% posterior\nprediction\ninterval') +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 45,vjust = 1,hjust = 1))
ggsave(file = file.path(dir.out, paste0(file.prefix,'m1_ppcheck.png')), 
       p, 
       w = 12, 
       h = 12)
```

```{r ordered_logistic_with_unit_effects_endl_otherpriors_extract_participant_effects, include=FALSE, eval=TRUE, echo=FALSE, tidy=FALSE, cache=TRUE}
# extract participant effects
po <- ordered_categorical_logit_endl_fit_m1$draws(
  variables = c('beta_unit_sd','beta_unit'),
  inc_warmup = FALSE,
  format = "draws_df"
  )
po <- as.data.table(po)
po <- data.table::melt(po, id.vars = c('.draw','.chain','.iteration'))
tmp <- subset(po, variable == 'beta_unit_sd')
tmp <- data.table::dcast(tmp, .draw ~ variable, value.var = 'value')
po <- subset(po, variable != 'beta_unit_sd')
po <- merge(po, tmp, by = '.draw')
po[, pid := as.integer(gsub('beta_unit\\[([0-9]+)\\]','\\1',variable)) ]
po[, value := value * beta_unit_sd]

pos <- po[, 
          list( summary_value = quantile(value, prob = c(0.025, 0.25, 0.5, 0.75, 0.975)),
                summary_name = c('q_lower','iqr_lower','median','iqr_upper','q_upper') 
                ), 
          by = c('pid')
          ]
pos <- data.table::dcast(pos, pid ~ summary_name, value.var = 'summary_value')
tmp <- unique(subset(dcat1e, select = c(pid, arm_label, time_label)))
pos <- merge(pos, tmp, by = c('pid'))
tmp <- dcat1e[, list(avg_value = mean(value)), by = 'pid']
tmp[, avg_value_jitter := avg_value + runif(nrow(tmp), min = -1/8, max = 1/8)]
pos <- merge(pos, tmp, by = 'pid')

tmp <- pos[, list(avg_effect = mean(median)), by = c('arm_label','time_label')]

p <- ggplot(data = pos, aes( x = avg_value_jitter)) +
  geom_hline(data = tmp, aes( yintercept = avg_effect), colour = 'grey50') +
  geom_linerange(aes( ymin = q_lower, ymax = q_upper), 
           stat = 'identity') +
  geom_point(aes( y = median)) +
  labs(x = '\nresponse of participant', y = 'participant effect') +
  theme_bw() +
  facet_grid(arm_label ~ time_label, scales = 'free') +
  theme(axis.text.x = element_text(angle = 45,vjust = 1,hjust = 1),
        legend.position = 'bottom'
        )
ggsave(file = file.path(dir.out, paste0(file.prefix,'m1_participant_effects.png')), 
       plot = p, 
       h = 8, 
       w = 12
       )
```

Here are the marginal posterior participant effects, shown by the response of the participant. We see the credible intervals are not symmetric around zero, reflecting curvature in the geometry of the joint posterior distribution. It is because of this curvature that an Exponential prior on the random effect variation improves accuracy.

```{r ordered_logistic_with_unit_effects_endl_otherpriors_plot_participant_effects, include=TRUE, eval=TRUE, echo=FALSE, tidy=FALSE, fig.align='center', out.width='80%'}
knitr::include_graphics( file.path(dir.out, paste0(file.prefix,'m1_participant_effects.png')) )
```


```{r ordered_logistic_with_unit_effects_endl_otherpriors_extract_class_probs, include=FALSE, eval=TRUE, echo=FALSE, tidy=FALSE, cache=TRUE}
# extract model parameters pairs plot
po <- ordered_categorical_logit_endl_fit_m1$draws(
  variables = c('ordered_prob_by_obs'),
  inc_warmup = FALSE,
  format = "draws_df"
  )
po <- as.data.table(po)
po <- data.table::melt(po, id.vars = c('.draw','.chain','.iteration'), value.name = 'prob')
po[, value := as.integer(gsub('ordered_prob_by_obs\\[([0-9]),([0-9]+)\\]','\\1',variable)) ]
po[, oid := as.integer(gsub('ordered_prob_by_obs\\[([0-9]),([0-9]+)\\]','\\2',variable)) ]
set(po, NULL, 'variable', NULL)
tmp <- unique(subset(dcat1e, select = c(oid, pid, vid, group )))
po <- merge(po, tmp, by = c('oid'))
po <- po[, list(prob = mean(prob)), by = c('.draw','group','value')]

pos <- po[, 
          list( summary_value = quantile(prob, prob = c(0.025, 0.25, 0.5, 0.75, 0.975)),
                summary_name = c('q_lower','iqr_lower','median','iqr_upper','q_upper') 
                ), 
          by = c('group','value')
          ]
pos <- data.table::dcast(pos, group + value ~ summary_name, value.var = 'summary_value')

tmp <- unique( subset(dcat1e, select = c(group, arm, arm_label, time, time_label, value, value_label)) )
pos <- merge(pos, tmp, by = c('group','value'))

tmp <- dcat1e[, list(n = length(pid)), by = c('arm_label','time_label','value_label','value')]
tmp2 <- tmp[, list(total = sum(n)), by = c('arm_label','time_label')]
tmp <- merge(tmp, tmp2, by = c('arm_label','time_label'))
tmp[, p_emp := n/total]
pos <- merge(pos, subset(tmp, select = -c(value, n, total)), by = c('arm_label','time_label','value_label'))

p <- ggplot(data = pos, aes( x = value_label, fill = interaction(arm_label,time_label))) +
  geom_bar(aes( y = p_emp), 
           stat = 'identity', 
           position = position_dodge()) +
  geom_point(aes( y = median),
             position = position_dodge(0.9), colour = 'black') +
  geom_boxplot( aes(ymin = q_lower, lower = iqr_lower, middle = median, upper = iqr_upper, ymax = q_upper), 
                position = position_dodge(0.9),
                stat = 'identity',
                alpha = 0,
                width = 0.3) +
  scale_fill_npg() +
  scale_y_continuous(labels = scales::percent) +
  labs(x = '\nParental mental health\nPHQ4 outcome', y = 'group-level\nposterior probability', fill = '') +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 45,vjust = 1,hjust = 1),
        legend.position = 'bottom'
        )
ggsave(file = file.path(dir.out, paste0(file.prefix,'m1_probs_barplot.png')), 
       plot = p, 
       h = 8, 
       w = 8
       )
```

Fitting the model produces the following fits. We obtain estimates that are essentially as good as those of the baseline model from the previous section, even though the model is overly flexible/has too many free parameters:

```{r ordered_logistic_with_unit_effects_endl_otherpriors_endl_plot, include=TRUE, eval=TRUE, echo=FALSE, tidy=FALSE, fig.align='center', out.width='80%'}
knitr::include_graphics( file.path(dir.out, paste0(file.prefix,'m1_probs_barplot.png')) )
```


### Model with participant effects on full data

After all these checks, let me now move on to apply the ordered categorical model with participant effects to the entire PHQ4 data.


```{r ordered_logistic_with_unit_effects_run_model, include=TRUE, eval=TRUE, echo=TRUE, message=FALSE, tidy=FALSE, cache=TRUE}
file.prefix <- 'parentalmh_ordered_categorical_'

# define data in format needed for model specification
stan_data <- list()
stan_data$N <- nrow(dcat1)
stan_data$P <- max(dcat1$group)
stan_data$U <- max(dcat1$pid)
stan_data$K <- length(unique(dcat1$value))
stan_data$y <- dcat1$value
stan_data$group_of_obs <- dcat1$group
stan_data$unit_of_obs <- dcat1$pid

# sample
ordered_categorical_logit_fit <- ordered_categorical_logit_compiled_m1$sample(
  data = stan_data,
  seed = 123,
  chains = 2,
  parallel_chains = 2,
  iter_warmup = 5e2,
  iter_sampling = 15e2,
  refresh = 500,
  save_warmup = TRUE
)

# save output to RDS
ordered_categorical_logit_fit$save_object(file = file.path(dir.out, paste0(file.prefix,"m1_stan.rds")))
```

```{r ordered_logistic_with_unit_effects_load_model, include=FALSE, eval=TRUE, echo=FALSE, tidy=FALSE, cache=TRUE}
ordered_categorical_logit_fit <- readRDS(file = file.path(dir.out, paste0(file.prefix,'m1_stan.rds')))
```

```{r ordered_logistic_with_unit_effects_check_mixing_convergence, include=FALSE, eval=TRUE, echo=FALSE, tidy=FALSE, cache=TRUE}
#	check convergence and check mixing
#	get 95% credible intervals
tmp <- ordered_categorical_logit_fit$summary(
  variables = c('beta_group','beta_unit','beta_unit_sd','cut_point_1','cut_point_gaps_groups'),
  posterior::default_summary_measures(),
  posterior::default_convergence_measures(),
  extra_quantiles = ~posterior::quantile2(., probs = c(.0275, .975))
)
tmp <- as.data.table(tmp)
tmp
  
# parameter with lowest ess_bulk
worst_var <- tmp$variable[ which.min(tmp$ess_bulk) ]

# extract samples
po <- ordered_categorical_logit_fit$draws(
  variables = c("lp__", worst_var),
  inc_warmup = TRUE,
  format = "draws_array"
  )

# make worst trace plot
p <- bayesplot:::mcmc_trace(po,  
                            pars = c("lp__",worst_var), 
                            n_warmup = 500,
                            facet_args = list(nrow = 2)
                            )
p <- p + theme_bw()
ggsave(file = file.path(dir.out, paste0(file.prefix,'m1_worsttrace.png')), 
       plot = p, 
       h = 10, 
       w = 8
       )

# make intervals plot
po <- ordered_categorical_logit_fit$draws(
  variables = c('beta_group','cut_point_1','cut_point_gaps_groups','beta_unit_sd','beta_unit'),
  inc_warmup = FALSE,
  format = "draws_array"
  )

color_scheme_set("teal")
p <- bayesplot::mcmc_intervals(po, prob = 0.5, prob_outer = 0.95, outer_size = 1, point_size = 2) +
  theme_bw()
ggsave(file = file.path(dir.out, paste0(file.prefix,'m1_intervals.png')), 
       plot = p, 
       h = 35, 
       w = 8, 
       limitsize = FALSE
       )

# make pairs plot
po <- ordered_categorical_logit_fit$draws(
  variables = c('beta_group','cut_point_1','cut_point_gaps_groups'),
  inc_warmup = FALSE,
  format = "draws_array"
  )

bayesplot::color_scheme_set('viridisC')
p <- bayesplot::mcmc_pairs(po, 
                           diag_fun = "dens", 
                           off_diag_fun = "hex"
                           ) 
ggsave(file = file.path(dir.out, paste0(file.prefix,'m1_pairsplot.png')), 
       plot = p, 
       h = 30, 
       w = 30, 
       limitsize = FALSE
       )
bayesplot::color_scheme_set('brewer-RdYlBu')

# make posterior predictive check

# create median and 95\% credible intervals
po <- ordered_categorical_logit_fit$draws(
  variables = c('ypred'),
  inc_warmup = FALSE,
  format = "draws_df"
  )
po <- as.data.table(po)
po <- data.table::melt(po, id.vars = c('.draw','.chain','.iteration'), value.name = 'ypred')
po[, oid := as.integer(gsub('ypred\\[([0-9]+)\\]','\\1',variable)) ]
pos <- 
  po[,
     list( summary_value = quantile(ypred, prob = c(0.025, 0.25, 0.5, 0.75, 0.975)),
           summary_name = c('q_lower','iqr_lower','median','iqr_upper','q_upper') 
           ),
     by = 'oid'
     ]
pos <- 
  data.table::dcast(pos,
                    oid ~ summary_name, 
                    value.var = 'summary_value'
                    )
pos <- merge(pos, dcat1, by = 'oid')
pos[, IN_PPI := value >= q_lower & value <= q_upper]

# plot posterior predictive check
p <- ggplot(pos, aes(x = oid, group = oid)) + 
  geom_boxplot( aes( ymin = q_lower,
                     lower = iqr_lower,
                     middle = median,
                     upper = iqr_upper,
                     ymax = q_upper),
                stat = 'identity') +
  geom_point( aes(y = value, colour = IN_PPI ) ) +
  facet_grid(variable ~ arm_label+time_label, scales = 'free') +
  scale_x_discrete() +
  scale_y_continuous() +
  ggsci::scale_color_npg() +
  labs(x = '', 
       y = 'outcome', 
       colour = 'within\n95% posterior\nprediction\ninterval') +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 45,vjust = 1,hjust = 1))
ggsave(file = file.path(dir.out, paste0(file.prefix,'m1_ppcheck.png')), 
       p, 
       w = 12, 
       h = 12)
```

```{r ordered_logistic_with_unit_effects_extract_participant_effects, include=FALSE, eval=TRUE, echo=FALSE, tidy=FALSE, cache=TRUE}
# extract participant effects
po <- ordered_categorical_logit_fit$draws(
  variables = c('beta_unit_sd','beta_unit'),
  inc_warmup = FALSE,
  format = "draws_df"
  )
po <- as.data.table(po)
po <- data.table::melt(po, id.vars = c('.draw','.chain','.iteration'))
tmp <- subset(po, variable == 'beta_unit_sd')
tmp <- data.table::dcast(tmp, .draw ~ variable, value.var = 'value')
po <- subset(po, variable != 'beta_unit_sd')
po <- merge(po, tmp, by = '.draw')
po[, pid := as.integer(gsub('beta_unit\\[([0-9]+)\\]','\\1',variable)) ]
po[, value := value * beta_unit_sd]

pos <- po[, 
          list( summary_value = quantile(value, prob = c(0.025, 0.25, 0.5, 0.75, 0.975)),
                summary_name = c('q_lower','iqr_lower','median','iqr_upper','q_upper') 
                ), 
          by = c('pid')
          ]
pos <- data.table::dcast(pos, pid ~ summary_name, value.var = 'summary_value')
tmp <- unique(subset(dcat1, select = c(pid, arm_label, time_label)))
pos <- merge(pos, tmp, by = c('pid'))
tmp <- dcat1[, list(avg_value = mean(value)), by = 'pid']
tmp[, avg_value_jitter := avg_value + runif(nrow(tmp), min = -1/8, max = 1/8)]
pos <- merge(pos, tmp, by = 'pid')

tmp <- pos[, list(avg_effect = mean(median)), by = c('arm_label','time_label')]

p <- ggplot(data = pos, aes( x = avg_value_jitter)) +
  geom_hline(data = tmp, aes( yintercept = avg_effect), colour = 'grey50') +
  geom_linerange(aes( ymin = q_lower, ymax = q_upper), 
           stat = 'identity') +
  geom_point(aes( y = median)) +
  labs(x = '\naverage response of participant', y = 'participant effect') +
  theme_bw() +
  facet_grid(arm_label ~ time_label, scales = 'free') +
  theme(axis.text.x = element_text(angle = 45,vjust = 1,hjust = 1),
        legend.position = 'bottom'
        )
ggsave(file = file.path(dir.out, paste0(file.prefix,'m1_participant_effects.png')), 
       plot = p, 
       h = 8, 
       w = 12
       )
```

I illustrate the participant effects first. We observe that the model accounts for the observed clustering of responses among participants and across time by placing negative effects to individual-level responses when the typical response is response is 'Not at all' or 'Several days', and positive effects when the typical response is 'More than half the days' or 'Nearly every day'.

```{r ordered_logistic_with_unit_effects_extract_participant_effects_plot, include=TRUE, eval=TRUE, echo=FALSE, tidy=FALSE, fig.align='center', out.width='80%'}
knitr::include_graphics( file.path(dir.out, paste0(file.prefix,'m1_participant_effects.png')) )
```

```{r ordered_logistic_with_unit_effects_make_class_probs, include=FALSE, eval=TRUE, echo=FALSE, tidy=FALSE, cache=TRUE}
# extract model parameters pairs plot
po <- ordered_categorical_logit_fit$draws(
  variables = c('ordered_prob_by_obs'),
  inc_warmup = FALSE,
  format = "draws_df"
  )
po <- as.data.table(po)
po <- data.table::melt(po, id.vars = c('.draw','.chain','.iteration'), value.name = 'prob')
po[, value := as.integer(gsub('ordered_prob_by_obs\\[([0-9]),([0-9]+)\\]','\\1',variable)) ]
po[, oid := as.integer(gsub('ordered_prob_by_obs\\[([0-9]),([0-9]+)\\]','\\2',variable)) ]
set(po, NULL, 'variable', NULL)
tmp <- unique(subset(dcat1, select = c(oid, pid, vid, group )))
po <- merge(po, tmp, by = c('oid'))
po <- po[, list(prob = mean(prob)), by = c('.draw','group','value')]

pos <- po[, 
          list( summary_value = quantile(prob, prob = c(0.025, 0.25, 0.5, 0.75, 0.975)),
                summary_name = c('q_lower','iqr_lower','median','iqr_upper','q_upper') 
                ), 
          by = c('group','value')
          ]
pos <- data.table::dcast(pos, group + value ~ summary_name, value.var = 'summary_value')

tmp <- unique( subset(dcat1, select = c(group, arm, arm_label, time, time_label, value, value_label)) )
pos <- merge(pos, tmp, by = c('group','value'))

tmp <- dcat1[, list(n = length(pid)), by = c('arm_label','time_label','value_label','value')]
tmp2 <- tmp[, list(total = sum(n)), by = c('arm_label','time_label')]
tmp <- merge(tmp, tmp2, by = c('arm_label','time_label'))
tmp[, p_emp := n/total]
pos <- merge(pos, subset(tmp, select = -c(value, n, total)), by = c('arm_label','time_label','value_label'))

p <- ggplot(data = pos, aes( x = value_label, fill = interaction(arm_label,time_label))) +
  geom_bar(aes( y = p_emp), 
           stat = 'identity', 
           position = position_dodge()) +
  geom_point(aes( y = median),
             position = position_dodge(0.9), colour = 'black') +
  geom_boxplot( aes(ymin = q_lower, lower = iqr_lower, middle = median, upper = iqr_upper, ymax = q_upper), 
                position = position_dodge(0.9),
                stat = 'identity',
                alpha = 0,
                width = 0.3) +
  scale_fill_npg() +
  scale_y_continuous(labels = scales::percent) +
  labs(x = '\nParental mental health\nPHQ4 outcome', y = 'group-level\nposterior probability', fill = '') +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 45,vjust = 1,hjust = 1),
        legend.position = 'bottom'
        )
ggsave(file = file.path(dir.out, paste0(file.prefix,'m1_probs_barplot.png')), 
       plot = p, 
       h = 8, 
       w = 8
       )
```

This is the model fit (in black) relative to the aggregated survey items. This looks pretty good with regards to our target statistics, and captures some of the clustering of responses among the same participant, especially over time. However note that the $\eta_{ij}$ term in the model only depends on $i$, and so there the model cannot account for clustering among the responses for each participant. I will return to this in a bit.

```{r ordered_logistic_with_unit_effects_plot, include=TRUE, eval=TRUE, echo=FALSE, tidy=FALSE, fig.align='center', out.width='80%'}
knitr::include_graphics( file.path(dir.out, paste0(file.prefix,'m1_probs_barplot.png')) )
```

# PHQ4 - Poisson model

More pressingly, I will next consider the Likert outcomes as if they were counts and aggregate these across all itemized questions that target the same behaviour. The idea is that we might be able to model these "count data" with a simpler Poisson model.

```{r Subset data for PHQ4 poisson models, include=FALSE, eval=TRUE, message=FALSE, echo=FALSE, warning=FALSE, tidy=TRUE}
# Select ordered categorical outcome
dcat2 <- subset(dp, vid < 6 & variable_type == 'agg_likert')
file.prefix <- 'parentalmh_poisson_aggregated_'

# Preprocessing
dcat2 <- dcat2[order(pid, arm, time, variable)]
dcat2[, oid := seq_len(nrow(dcat2))]
set(dcat2, NULL, 'arm', dcat2[, arm + 1L])
set(dcat2, NULL, 'time', dcat2[, time + 1L])
set(dcat2, NULL, 'group', dcat2[, max(arm) * (time - 1L) + arm])

# Frequency plot
tmp <- dcat2[, list(n = length(pid)), by = c('arm_label','time_label','value')]

p <- ggplot(tmp, aes(x = value, y = n)) +
  geom_col(aes(fill = interaction(arm_label,time_label)), position = position_dodge()) +
  scale_fill_npg() +
  facet_grid(arm_label ~ time_label) +
  theme_bw() +
  theme(legend.position = 'bottom') +
  labs(x = '', fill = 'counts of Likert outcomes')
ggsave(file = file.path(dir.out, paste0(file.prefix,'frequencies.png')), 
       plot = p, 
       h = 10, 
       w = 8
       )
```

```{r Subset data for PHQ4 poisson models_plot, include=TRUE, eval=TRUE, echo=FALSE, tidy=FALSE, fig.align='center', out.width='80%'}
knitr::include_graphics( file.path(dir.out, paste0(file.prefix,'frequencies.png')) )
```



## Poisson model without participant effects

Here is the Poisson model without participant effects,

\begin{align*}
& \text{Prob}(Y_{ij} = k) = \text{Poisson}(k; \lambda_{ij}) \\
& \log\lambda_{ij}= \beta_0 + \beta_{G(i)}\\
& \beta_0 \sim \text{S2Z-Normal}(0, 5^2)\\
& \beta \sim \text{S2Z-Normal}(0, 1)
\end{align*}

and here is the corresponding Stan code: 

```{r agg_poisson_without_unit_effects_stan_code, include=TRUE, eval=TRUE, echo=TRUE, tidy=TRUE, cache=TRUE}
# this is a vanilla model without unit effects
poisson_log_txt_m0 <- "
functions
{
  matrix sum2zero_generating_matrix(int K) {
    matrix[K, K] A = diag_matrix(rep_vector(1, K));
    for (i in 1:K - 1) A[K, i] = -1;
    A[K, K] = 0;
    return qr_Q(A)[ , 1:(K - 1)];
  }
}
data
{
  int<lower=1> N; // number of observations
  int<lower=2> P; // number of groups
  int<lower=1> K; // max number of counts
  array [N] int<lower=0, upper=K> y; // observations
  array [N] int<lower=1,upper=P> group_of_obs;
}
transformed data
{
    real s2z_sd_groups;
    matrix[P,P-1] s2z_Q_groups;
    
    s2z_sd_groups = inv(sqrt(1. - inv(P)));
    s2z_Q_groups = sum2zero_generating_matrix(P);
}
parameters
{
  real beta_0;
  vector[P-1] beta_groups_m1;
}
transformed parameters
{
  vector[P] beta_group;
  vector[N] log_lambda;
  beta_group =  s2z_Q_groups * beta_groups_m1;
  log_lambda = beta_0 + beta_group[group_of_obs];
}
model
{
  // likelihood
  target += poisson_log_lpmf(y | log_lambda);
  
  // priors
  target += normal_lupdf(beta_groups_m1 | 0, s2z_sd_groups); 
  target += normal_lupdf(beta_0 | 0, 5); 
}
generated quantities
{
  array [N] real log_lik;
  array [P] real log_lambda_group;
  array [N] int<lower=0> ypred;

  for( n in 1:N ) 
  {
    ypred[n] = poisson_log_rng( log_lambda[n] );
    log_lik[n] =  poisson_log_lpmf( y[n] | log_lambda[n] );
  }
  for( p in 1:P )
  {
    log_lambda_group[p] = beta_0 + beta_group[p];  
  }
}
"

# compile the model
poisson_log_filename_m0 <- cmdstanr::write_stan_file(
  gsub('\t',' ', poisson_log_txt_m0),
  dir = dir.out,
  basename = NULL,
  force_overwrite = FALSE,
  hash_salt = ""
)

# compile Stan model
poisson_log_compiled_m0 <- cmdstanr::cmdstan_model(poisson_log_filename_m0, force_recompile = TRUE)
```

The Poisson model has fewer free parameters and is much easier and faster to fit (compare the runtimes):

```{r agg_poisson_without_unit_effects_run_model, include=TRUE, eval=TRUE, echo=TRUE, message=FALSE, tidy=FALSE, cache=TRUE}
# define data in format needed for model specification
stan_data <- list()
stan_data$N <- nrow(dcat2)
stan_data$K <- max(dcat2$value)
stan_data$P <- max(dcat2$group)
stan_data$y <- dcat2$value
stan_data$group_of_obs <- dcat2$group

# sample
poisson_log_fit_m0 <- poisson_log_compiled_m0$sample(
  data = stan_data,
  seed = 123,
  chains = 2,
  parallel_chains = 2,
  iter_warmup = 5e2,
  iter_sampling = 15e2,
  refresh = 500,
  init = list(list(beta_0 = 6, beta_groups_m1 = rep(0,3)), list(beta_0 = 6, beta_groups_m1 = rep(0,3))),
  save_warmup = TRUE
)

# save output to RDS
poisson_log_fit_m0$save_object(file = file.path(dir.out, paste0(file.prefix,"m0_stan.rds")))
```

```{r agg_poisson_without_unit_effects_check mixing convergence, include=FALSE, eval=TRUE, echo=FALSE, tidy=FALSE, cache=TRUE}
#	check convergence and check mixing
#	get 95% credible intervals
tmp <- poisson_log_fit_m0$summary(
  variables = c('beta_groups_m1','beta_0'),
  posterior::default_summary_measures(),
  posterior::default_convergence_measures(),
  extra_quantiles = ~posterior::quantile2(., probs = c(.0275, .975))
)
tmp <- as.data.table(tmp)
tmp
  
# parameter with lowest ess_bulk
worst_var <- tmp$variable[ which.min(tmp$ess_bulk) ]

# extract samples
po <- poisson_log_fit_m0$draws(
  variables = c("lp__", worst_var),
  inc_warmup = TRUE,
  format = "draws_array"
  )

# make worst trace plot
p <- bayesplot:::mcmc_trace(po,  
                            pars = c("lp__",worst_var), 
                            n_warmup = 500,
                            facet_args = list(nrow = 2)
                            )
p <- p + theme_bw()
ggsave(file = file.path(dir.out, paste0(file.prefix,'m0_worsttrace.png')), 
       plot = p, 
       h = 10, 
       w = 8
       )

# make intervals plot
po <- poisson_log_fit_m0$draws(
  variables = c('beta_groups_m1','beta_group','beta_0'),
  inc_warmup = FALSE,
  format = "draws_array"
  )

color_scheme_set("teal")
p <- bayesplot::mcmc_intervals(po, prob = 0.5, prob_outer = 0.95, outer_size = 1, point_size = 2) +
  theme_bw()
ggsave(file = file.path(dir.out,paste0(file.prefix,'m0_intervals.png')), 
       plot = p, 
       h = 7, 
       w = 8, 
       limitsize = FALSE
       )

po <- poisson_log_fit_m0$draws(
  variables = c('beta_groups_m1','beta_0'),
  inc_warmup = FALSE,
  format = "draws_array"
  )

bayesplot::color_scheme_set('viridisC')
p <- bayesplot::mcmc_pairs(po, 
                           diag_fun = "dens", 
                           off_diag_fun = "hex"
                           ) 
ggsave(file = file.path(dir.out,paste0(file.prefix,'m0_pairsplot.png')), 
       plot = p, 
       h = 30, 
       w = 30, 
       limitsize = FALSE
       )
bayesplot::color_scheme_set('brewer-RdYlBu')

# make posterior predictive check

# create median and 95\% credible intervals
po <- poisson_log_fit_m0$draws(
  variables = c('ypred'),
  inc_warmup = FALSE,
  format = "draws_df"
  )
po <- as.data.table(po)
po <- data.table::melt(po, id.vars = c('.draw','.chain','.iteration'), value.name = 'ypred')
po[, oid := as.integer(gsub('ypred\\[([0-9]+)\\]','\\1',variable)) ]
pos <- 
  po[,
     list( summary_value = quantile(ypred, prob = c(0.025, 0.25, 0.5, 0.75, 0.975)),
           summary_name = c('q_lower','iqr_lower','median','iqr_upper','q_upper') 
           ),
     by = 'oid'
     ]
pos <- 
  data.table::dcast(pos,
                    oid ~ summary_name, 
                    value.var = 'summary_value'
                    )
pos <- merge(pos, dcat2, by = 'oid')
pos[, IN_PPI := value >= q_lower & value <= q_upper]
pos[, mean(IN_PPI)]

# plot posterior predictive check
p <- ggplot(pos, aes(x = oid, group = oid)) + 
  geom_boxplot( aes( ymin = q_lower,
                     lower = iqr_lower,
                     middle = median,
                     upper = iqr_upper,
                     ymax = q_upper),
                stat = 'identity') +
  geom_point( aes(y = value, colour = IN_PPI ) ) +
  facet_grid(variable ~ arm_label+time_label, scales = 'free') +
  scale_x_discrete() +
  scale_y_continuous() +
  ggsci::scale_color_npg() +
  labs(x = '', 
       y = 'outcome', 
       colour = 'within\n95% posterior\nprediction\ninterval') +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 45,vjust = 1,hjust = 1))
ggsave(file = file.path(dir.out, paste0(file.prefix,'m0_ppcheck.png')), 
       p, 
       w = 12, 
       h = 12)
```

```{r agg_poisson_without_unit_effects_extract_class_probs, include=FALSE, eval=TRUE, echo=FALSE, tidy=FALSE, cache=TRUE}
# extract model probabilities
po <- poisson_log_fit_m0$draws(
  variables = c('beta_0','beta_group'),
  inc_warmup = FALSE,
  format = "draws_df"
  )
po <- as.data.table(po)
po <- data.table::melt(po, id.vars = c('.draw','.chain','.iteration','beta_0'))
po[, group := as.integer(gsub('beta_group\\[([0-9])\\]','\\1',variable)) ]
po[, value := exp(beta_0 + value)]
po <- po[, list(value = 0:12, pdf = dpois(0:12, value)), by = c('.draw','group')]
tmp <- po[, list(total = sum(pdf)), by = c('.draw','group')]
po <- merge(po, tmp, by = c('.draw','group'))
po[, pdf := pdf/total]

pos <- 
  po[,
     list( summary_value = quantile(pdf, prob = c(0.025, 0.25, 0.5, 0.75, 0.975)),
           summary_name = c('q_lower','iqr_lower','median','iqr_upper','q_upper') 
           ),
     by = c('group','value')
     ]
pos <- 
  data.table::dcast(pos,
                    group + value ~ summary_name, 
                    value.var = 'summary_value'
                    )

tmp <- unique( subset(dcat2, select = c(group, arm, arm_label, time, time_label)) )
pos <- merge(pos, tmp, by = c('group'))

# Add empirical frequencies
tmp <- dcat2[, list(n = length(pid)), by = c('arm_label','time_label','value')]
tmp2 <- tmp[, list(total = sum(n)), by = c('arm_label','time_label')]
tmp <- merge(tmp, tmp2, by = c('arm_label','time_label'))
tmp[, p_emp := n/total]
pos <- merge(pos, subset(tmp, select = c(arm_label, time_label, value, p_emp)), by = c('arm_label','time_label','value'), all.x = TRUE)
set(pos, pos[, which(is.na(p_emp))], 'p_emp', 0.)

p <- ggplot(pos, aes(x = value)) +
  geom_col(aes(fill = interaction(arm_label,time_label), y = p_emp), alpha = 1) +
  geom_linerange(aes(ymin = q_lower, ymax = q_upper), colour = 'black') +
  geom_point(aes(y = median), colour = 'black', pch = 18, size = 2) +
  scale_fill_npg() +
  facet_grid(arm_label ~ time_label) +
  theme_bw() +
  theme(legend.position = 'bottom') +
  labs(x = 'aggregated Likert outcomes', fill = '', colour = '', y = 'prob')
ggsave(file = file.path(dir.out, paste0(file.prefix,'m0_frequencies.png')), 
       plot = p, 
       h = 10, 
       w = 8
       )
```

This is the model fit to the aggregated Likert outcomes when considered as counts. We observe in this particular case here, the Poisson model does not capture the structure in the Likert outcome data at all, reflecting that it has much fewer parameters than the Ordered Categorical model.

```{r agg_poisson_without_unit_effects_extract_class_probs_plot, include=TRUE, eval=TRUE, echo=FALSE, tidy=FALSE, fig.align='center', out.width='80%'}
knitr::include_graphics( file.path(dir.out, paste0(file.prefix,'m0_frequencies.png')) )
```


## Model comparison on non-aggregated data

We cannot directly compare the Poisson model above to the Ordered Categorical models of the previous section because they are fitted on different data sets. For this reason, I will now consider the non-aggregated responses for each survey question, but instead of interpreting these data as outcomes on a Likert scale, I will interpret these data as counts, and model these with a Poisson distribution. I will start with the model without participant effects.

```{r likert_poisson_without_unit_effects_run_model, include=TRUE, eval=TRUE, message=FALSE, echo=TRUE, tidy=FALSE, cache=TRUE}
file.prefix <- 'parentalmh_poisson_likert_'

# define data in format needed for model specification
stan_data <- list()
stan_data$N <- nrow(dcat1)
stan_data$K <- max(dcat1$value - 1L)
stan_data$P <- max(dcat1$group)
stan_data$y <- dcat1$value - 1L
stan_data$group_of_obs <- dcat1$group

# sample
poisson_log_likert_fit_m0 <- poisson_log_compiled_m0$sample(
  data = stan_data,
  seed = 123,
  chains = 2,
  parallel_chains = 2,
  iter_warmup = 5e2,
  iter_sampling = 15e2,
  refresh = 500,
  init = list(list(beta_0 = 6, beta_groups_m1 = rep(0,3)), list(beta_0 = 6, beta_groups_m1 = rep(0,3))),
  save_warmup = TRUE
)

# save output to RDS
poisson_log_likert_fit_m0$save_object(file = file.path(dir.out, paste0(file.prefix,"m0_stan.rds")))
```

```{r likert_poisson_without_unit_effects_check mixing convergence, include=FALSE, eval=TRUE, echo=FALSE, tidy=FALSE, cache=TRUE}
#	check convergence and check mixing
#	get 95% credible intervals
tmp <- poisson_log_likert_fit_m0$summary(
  variables = c('beta_groups_m1','beta_0'),
  posterior::default_summary_measures(),
  posterior::default_convergence_measures(),
  extra_quantiles = ~posterior::quantile2(., probs = c(.0275, .975))
)
tmp <- as.data.table(tmp)
tmp
  
# parameter with lowest ess_bulk
worst_var <- tmp$variable[ which.min(tmp$ess_bulk) ]

# extract samples
po <- poisson_log_likert_fit_m0$draws(
  variables = c("lp__", worst_var),
  inc_warmup = TRUE,
  format = "draws_array"
  )

# make worst trace plot
p <- bayesplot:::mcmc_trace(po,  
                            pars = c("lp__",worst_var), 
                            n_warmup = 500,
                            facet_args = list(nrow = 2)
                            )
p <- p + theme_bw()
ggsave(file = file.path(dir.out, paste0(file.prefix,'m0_worsttrace.png')), 
       plot = p, 
       h = 10, 
       w = 8
       )

# make intervals plot
po <- poisson_log_likert_fit_m0$draws(
  variables = c('beta_groups_m1','beta_group','beta_0'),
  inc_warmup = FALSE,
  format = "draws_array"
  )

color_scheme_set("teal")
p <- bayesplot::mcmc_intervals(po, prob = 0.5, prob_outer = 0.95, outer_size = 1, point_size = 2) +
  theme_bw()
ggsave(file = file.path(dir.out, paste0(file.prefix,'m0_intervals.png')), 
       plot = p, 
       h = 7, 
       w = 8, 
       limitsize = FALSE
       )

po <- poisson_log_likert_fit_m0$draws(
  variables = c('beta_groups_m1','beta_0'),
  inc_warmup = FALSE,
  format = "draws_array"
  )

bayesplot::color_scheme_set('viridisC')
p <- bayesplot::mcmc_pairs(po, 
                           diag_fun = "dens", 
                           off_diag_fun = "hex"
                           ) 
ggsave(file = file.path(dir.out, paste0(file.prefix,'m0_pairsplot.png')), 
       plot = p, 
       h = 30, 
       w = 30, 
       limitsize = FALSE
       )
bayesplot::color_scheme_set('brewer-RdYlBu')

# make posterior predictive check

# create median and 95\% credible intervals
po <- poisson_log_likert_fit_m0$draws(
  variables = c('ypred'),
  inc_warmup = FALSE,
  format = "draws_df"
  )
po <- as.data.table(po)
po <- data.table::melt(po, id.vars = c('.draw','.chain','.iteration'), value.name = 'ypred')
po[, oid := as.integer(gsub('ypred\\[([0-9]+)\\]','\\1',variable)) ]
pos <- 
  po[,
     list( summary_value = quantile(ypred, prob = c(0.025, 0.25, 0.5, 0.75, 0.975)),
           summary_name = c('q_lower','iqr_lower','median','iqr_upper','q_upper') 
           ),
     by = 'oid'
     ]
pos <- 
  data.table::dcast(pos,
                    oid ~ summary_name, 
                    value.var = 'summary_value'
                    )
pos <- merge(pos, dcat1, by = 'oid')
pos[, IN_PPI := value >= q_lower & value <= q_upper]
pos[, mean(IN_PPI)]

# plot posterior predictive check
p <- ggplot(pos, aes(x = oid, group = oid)) + 
  geom_boxplot( aes( ymin = q_lower,
                     lower = iqr_lower,
                     middle = median,
                     upper = iqr_upper,
                     ymax = q_upper),
                stat = 'identity') +
  geom_point( aes(y = value, colour = IN_PPI ) ) +
  facet_grid(variable ~ arm_label+time_label, scales = 'free') +
  scale_x_discrete() +
  scale_y_continuous() +
  ggsci::scale_color_npg() +
  labs(x = '', 
       y = 'outcome', 
       colour = 'within\n95% posterior\nprediction\ninterval') +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 45,vjust = 1,hjust = 1))
ggsave(file = file.path(dir.out, paste0(file.prefix,'m0_ppcheck.png')), 
       p, 
       w = 12, 
       h = 12)
```

```{r likert_poisson_without_unit_effects_extract class probs, include=FALSE, eval=TRUE, echo=FALSE, tidy=FALSE, cache=TRUE}
# extract model probabilities
po <- poisson_log_likert_fit_m0$draws(
  variables = c('beta_0','beta_group'),
  inc_warmup = FALSE,
  format = "draws_df"
  )
po <- as.data.table(po)
po <- data.table::melt(po, id.vars = c('.draw','.chain','.iteration','beta_0'))
po[, group := as.integer(gsub('beta_group\\[([0-9])\\]','\\1',variable)) ]
po[, value := exp(beta_0 + value)]
po <- po[, list(value = 0:3, pdf = dpois(0:3, value)), by = c('.draw','group')]
tmp <- po[, list(total = sum(pdf)), by = c('.draw','group')]
po <- merge(po, tmp, by = c('.draw','group'))
po[, pdf := pdf/total]

pos <- 
  po[,
     list( summary_value = quantile(pdf, prob = c(0.025, 0.25, 0.5, 0.75, 0.975)),
           summary_name = c('q_lower','iqr_lower','median','iqr_upper','q_upper') 
           ),
     by = c('group','value')
     ]
pos <- 
  data.table::dcast(pos,
                    group + value ~ summary_name, 
                    value.var = 'summary_value'
                    )

tmp <- unique( subset(dcat1, select = c(group, arm, arm_label, time, time_label, value, value_label)) )
tmp[, value := value - 1L]
pos <- merge(pos, tmp, by = c('group','value'))

# Add empirical frequencies
tmp <- dcat1[, list(n = length(pid)), by = c('arm_label','time_label','variable','value','value_label')]
tmp2 <- tmp[, list(total = sum(n)), by = c('arm_label','time_label','variable')]
tmp <- merge(tmp, tmp2, by = c('arm_label','time_label','variable'))
tmp[, p_emp := n/total]
tmp[, value := value - 1L]
pos <- merge(pos, subset(tmp, select = -c(n, total)), by = c('arm_label','time_label','value','value_label'), all.x = TRUE)
set(pos, pos[, which(is.na(p_emp))], 'p_emp', 0.)

p <- ggplot(pos, aes(x = value)) +
  geom_col(aes(fill = variable, y = p_emp), alpha = 1, position = position_dodge(0.9)) +
  geom_linerange(aes(ymin = q_lower, ymax = q_upper), colour = 'black') +
  geom_point(aes(y = median), colour = 'black', pch = 18, size = 2) +
  scale_fill_bmj() +
  scale_y_continuous(labels = scales::percent) +
  facet_grid(arm_label ~ time_label) +
  theme_bw() +
  theme(legend.position = 'bottom') +
  labs(x = 'Likert outcomes as counts', fill = '', colour = '', y = 'proportion of coutcomes')
ggsave(file = file.path(dir.out, paste0(file.prefix,'m0_frequencies.png')), 
       plot = p, 
       h = 10, 
       w = 8
       )
```

This is the fit of the Poisson model. Note that the Likert outcomes are interpreted as counts 0,1,2,3 and described with a Poisson model, rather than distinct ordered scales. We can see that the Poisson fit always follows a unimodal shape that does not capture the more irregular/higher frequencies of the 2 and 3 category.

```{r likert_poisson_without_unit_effects_extract_probs_plot, include=TRUE, eval=TRUE, echo=FALSE, tidy=FALSE, fig.align='center', out.width='80%'}
knitr::include_graphics( file.path(dir.out, paste0(file.prefix,'m0_frequencies.png')) )
```


## Poisson model with participant effects

The Poisson model can be easily extended to include participant effects,

\begin{align*}
& \text{Prob}(Y_{ij} = k) = \text{Poisson}(k; \lambda_{ij}) \\
& \log\lambda_{ij}= \beta_0 + \beta_{G(i)} + \gamma_i\\
& \beta_0 \sim \text{S2Z-Normal}(0, 5^2)\\
& \beta \sim \text{S2Z-Normal}(0, 1)\\
& \gamma \sim \text{S2Z-Normal}(0, \sigma^2_\gamma)\\
& \sigma_\gamma \sim \text{Cauchy}(0,1)
\end{align*}

Here is the corresponding Stan code:

```{r likert_poisson_with_unit_effects_stan_code, include=TRUE, eval=TRUE, echo=TRUE, tidy=TRUE, cache=TRUE}
# this is a vanilla model without unit effects
poisson_log_txt_m1 <- "
functions
{
  matrix sum2zero_generating_matrix(int K) {
    matrix[K, K] A = diag_matrix(rep_vector(1, K));
    for (i in 1:K - 1) A[K, i] = -1;
    A[K, K] = 0;
    return qr_Q(A)[ , 1:(K - 1)];
  }
}
data
{
  int<lower=1> N; // number of observations
  int<lower=2> P; // number of groups
  int<lower=1> K; // max number of counts
  array [N] int<lower=0, upper=K> y; // observations
  array [N] int<lower=1,upper=P> group_of_obs;
  int<lower=1> U; // number of units
  array [N] int<lower=1,upper=U> unit_of_obs;
}
transformed data
{
    real s2z_sd_groups;
    matrix[P,P-1] s2z_Q_groups;
    real s2z_sd_unit;
    matrix[U,U-1] s2z_Q_unit;
    array [N] int<lower=1,upper=U> unit_of_group;
    array [P] int<lower=1,upper=N> unit_of_group_n;
    array [P] int<lower=1,upper=N> unit_of_group_n_cumulated;
    int tmp;
  
    s2z_sd_groups = inv(sqrt(1. - inv(P)));
    s2z_Q_groups = sum2zero_generating_matrix(P);
    
    s2z_sd_unit = inv(sqrt(1. - inv(U)));
    s2z_Q_unit = sum2zero_generating_matrix(U);
    
    tmp = 0;
    for( p in 1:P){
      unit_of_group_n[p] = 0;
      for( n in 1:N){
        if( group_of_obs[n] == p)
        {
          unit_of_group_n[p] = unit_of_group_n[p] + 1;
          tmp = tmp + 1;
          unit_of_group[tmp] = unit_of_obs[n];
        }
      }
      unit_of_group_n_cumulated[p] = sum(unit_of_group_n[1:p]);
    }
}
parameters
{
  real beta_0;
  vector[P-1] beta_groups_m1;
  real<lower=0> beta_unit_sd;
  vector[U-1] beta_unit_m1;
}
transformed parameters
{
  vector[P] beta_group;
  vector[N] log_lambda;
  vector[U] beta_unit;
  
  beta_group =  s2z_Q_groups * beta_groups_m1;
  beta_unit =  s2z_Q_unit * beta_unit_m1;
  log_lambda = beta_0 + beta_group[group_of_obs] + beta_unit_sd * beta_unit[unit_of_obs];
}
model
{
  // likelihood
  target += poisson_log_lpmf(y | log_lambda);
  
  // priors
  target += normal_lupdf(beta_groups_m1 | 0, s2z_sd_groups); 
  target += normal_lupdf(beta_0 | 0, 5); 
  target += normal_lupdf(beta_unit_m1 | 0, s2z_sd_unit); 
  target += cauchy_lupdf(beta_unit_sd | 0, 1);
}
generated quantities
{
  array [N] real log_lik;
  array [P] real log_lambda_group;
  array [N] int<lower=0> ypred;
  real tmp_real;
  
  for( n in 1:N ) 
  {
    ypred[n] = poisson_log_rng( log_lambda[n] );
    log_lik[n] =  poisson_log_lpmf( y[n] | log_lambda[n] );
  }
  for( p in 1:P )
  {
    tmp_real = 0;
    if( unit_of_group_n[p] > 0 )
    {
      tmp_real = mean( beta_unit_sd * beta_unit[ segment(unit_of_group, p==1 ? 1 : (unit_of_group_n_cumulated[p-1] + 1), unit_of_group_n[p]) ] );
    }
    log_lambda_group[p] = beta_0 + beta_group[p] + tmp_real;  
  }
}
"

# compile the model
poisson_log_filename_m1 <- cmdstanr::write_stan_file(
  gsub('\t',' ', poisson_log_txt_m1),
  dir = dir.out,
  basename = NULL,
  force_overwrite = FALSE,
  hash_salt = ""
)

# compile Stan model
poisson_log_compiled_m1 <- cmdstanr::cmdstan_model(poisson_log_filename_m1, force_recompile = TRUE)
```

```{r likert_poisson_with_unit_effects_run_model, include=FALSE, eval=TRUE, echo=FALSE, message=FALSE, tidy=FALSE, cache=TRUE}
# define data in format needed for model specification
stan_data <- list()
stan_data$N <- nrow(dcat1)
stan_data$K <- max(dcat1$value - 1L)
stan_data$P <- max(dcat1$group)
stan_data$y <- dcat1$value - 1L
stan_data$group_of_obs <- dcat1$group
stan_data$U <- max(dcat1$pid)
stan_data$unit_of_obs <- dcat1$pid

# sample
poisson_log_likert_fit_m1 <- poisson_log_compiled_m1$sample(
  data = stan_data,
  seed = 123,
  chains = 2,
  parallel_chains = 2,
  iter_warmup = 5e2,
  iter_sampling = 15e2,
  refresh = 500,
  init = list(list(beta_0 = 6, beta_groups_m1 = rep(0,3)), list(beta_0 = 6, beta_groups_m1 = rep(0,3))),
  save_warmup = TRUE
)

# save output to RDS
poisson_log_likert_fit_m1$save_object(file = file.path(dir.out, paste0(file.prefix,"m1_stan.rds")))
```

```{r likert_poisson_with_unit_effects_check mixing convergence, include=FALSE, eval=TRUE, echo=FALSE, tidy=FALSE, cache=TRUE}
#	check convergence and check mixing
#	get 95% credible intervals
tmp <- poisson_log_likert_fit_m1$summary(
  variables = c('beta_groups_m1','beta_0','beta_unit_m1','beta_unit_sd'),
  posterior::default_summary_measures(),
  posterior::default_convergence_measures(),
  extra_quantiles = ~posterior::quantile2(., probs = c(.0275, .975))
)
tmp <- as.data.table(tmp)
tmp
  
# parameter with lowest ess_bulk
worst_var <- tmp$variable[ which.min(tmp$ess_bulk) ]

# extract samples
po <- poisson_log_likert_fit_m1$draws(
  variables = c("lp__", worst_var),
  inc_warmup = TRUE,
  format = "draws_array"
  )

# make worst trace plot
p <- bayesplot:::mcmc_trace(po,  
                            pars = c("lp__",worst_var), 
                            n_warmup = 500,
                            facet_args = list(nrow = 2)
                            )
p <- p + theme_bw()
ggsave(file = file.path(dir.out, paste0(file.prefix,'m1_worsttrace.png')), 
       plot = p, 
       h = 10, 
       w = 8
       )

# make intervals plot
po <- poisson_log_likert_fit_m1$draws(
  variables = c('beta_groups_m1','beta_group','beta_0','beta_unit_m1','beta_unit_sd'),
  inc_warmup = FALSE,
  format = "draws_array"
  )

color_scheme_set("teal")
p <- bayesplot::mcmc_intervals(po, prob = 0.5, prob_outer = 0.95, outer_size = 1, point_size = 2) +
  theme_bw()
ggsave(file = file.path(dir.out, paste0(file.prefix,'m1_intervals.png')), 
       plot = p, 
       h = 25, 
       w = 8, 
       limitsize = FALSE
       )

po <- poisson_log_likert_fit_m1$draws(
  variables = c('beta_groups_m1','beta_0'),
  inc_warmup = FALSE,
  format = "draws_array"
  )

bayesplot::color_scheme_set('viridisC')
p <- bayesplot::mcmc_pairs(po, 
                           diag_fun = "dens", 
                           off_diag_fun = "hex"
                           ) 
ggsave(file = file.path(dir.out, paste0(file.prefix,'m1_pairsplot.png')), 
       plot = p, 
       h = 30, 
       w = 30, 
       limitsize = FALSE
       )
bayesplot::color_scheme_set('brewer-RdYlBu')

# make posterior predictive check

# create median and 95\% credible intervals
po <- poisson_log_likert_fit_m1$draws(
  variables = c('ypred'),
  inc_warmup = FALSE,
  format = "draws_df"
  )
po <- as.data.table(po)
po <- data.table::melt(po, id.vars = c('.draw','.chain','.iteration'), value.name = 'ypred')
po[, oid := as.integer(gsub('ypred\\[([0-9]+)\\]','\\1',variable)) ]
pos <- 
  po[,
     list( summary_value = quantile(ypred, prob = c(0.025, 0.25, 0.5, 0.75, 0.975)),
           summary_name = c('q_lower','iqr_lower','median','iqr_upper','q_upper') 
           ),
     by = 'oid'
     ]
pos <- 
  data.table::dcast(pos,
                    oid ~ summary_name, 
                    value.var = 'summary_value'
                    )
pos <- merge(pos, dcat1, by = 'oid')
pos[, IN_PPI := value >= q_lower & value <= q_upper]
pos[, mean(IN_PPI)]

# plot posterior predictive check
p <- ggplot(pos, aes(x = oid, group = oid)) + 
  geom_boxplot( aes( ymin = q_lower,
                     lower = iqr_lower,
                     middle = median,
                     upper = iqr_upper,
                     ymax = q_upper),
                stat = 'identity') +
  geom_point( aes(y = value, colour = IN_PPI ) ) +
  facet_grid(variable ~ arm_label+time_label, scales = 'free') +
  scale_x_discrete() +
  scale_y_continuous() +
  ggsci::scale_color_npg() +
  labs(x = '', 
       y = 'outcome', 
       colour = 'within\n95% posterior\nprediction\ninterval') +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 45,vjust = 1,hjust = 1))
ggsave(file = file.path(dir.out, paste0(file.prefix,'m1_ppcheck.png')), 
       p, 
       w = 12, 
       h = 12)
```

```{r likert_poisson_with_unit_effects_extract class probs, include=FALSE, eval=TRUE, echo=FALSE, tidy=FALSE, cache=TRUE}
# extract model probabilities
po <- poisson_log_likert_fit_m1$draws(
  variables = c('log_lambda_group'),
  inc_warmup = FALSE,
  format = "draws_df"
  )
po <- as.data.table(po)
po <- data.table::melt(po, id.vars = c('.draw','.chain','.iteration'))
po[, group := as.integer(gsub('log_lambda_group\\[([0-9])\\]','\\1',variable)) ]
po[, value := exp(value)]
po <- po[, list(value = 0:3, pdf = dpois(0:3, value)), by = c('.draw','group')]
tmp <- po[, list(total = sum(pdf)), by = c('.draw','group')]
po <- merge(po, tmp, by = c('.draw','group'))
po[, pdf := pdf/total]

pos <- 
  po[,
     list( summary_value = quantile(pdf, prob = c(0.025, 0.25, 0.5, 0.75, 0.975)),
           summary_name = c('q_lower','iqr_lower','median','iqr_upper','q_upper') 
           ),
     by = c('group','value')
     ]
pos <- 
  data.table::dcast(pos,
                    group + value ~ summary_name, 
                    value.var = 'summary_value'
                    )

tmp <- unique( subset(dcat1, select = c(group, arm, arm_label, time, time_label, value, value_label)) )
tmp[, value := value - 1L]
pos <- merge(pos, tmp, by = c('group','value'))

# Add empirical frequencies
tmp <- dcat1[, list(n = length(pid)), by = c('arm_label','time_label','variable','value','value_label')]
tmp2 <- tmp[, list(total = sum(n)), by = c('arm_label','time_label','variable')]
tmp <- merge(tmp, tmp2, by = c('arm_label','time_label','variable'))
tmp[, p_emp := n/total]
tmp[, value := value - 1L]
pos <- merge(pos, subset(tmp, select = -c(n, total)), by = c('arm_label','time_label','value','value_label'), all.x = TRUE)
set(pos, pos[, which(is.na(p_emp))], 'p_emp', 0.)

p <- ggplot(pos, aes(x = value)) +
  geom_col(aes(fill = variable, y = p_emp), alpha = 1, position = position_dodge(0.9)) +
  geom_linerange(aes(ymin = q_lower, ymax = q_upper), colour = 'black') +
  geom_point(aes(y = median), colour = 'black', pch = 18, size = 2) +
  scale_fill_bmj() +
  scale_y_continuous(labels = scales::percent) +
  facet_grid(arm_label ~ time_label) +
  theme_bw() +
  theme(legend.position = 'bottom') +
  labs(x = 'Likert outcomes as counts', fill = '', colour = '', y = 'proportion of coutcomes')
ggsave(file = file.path(dir.out, paste0(file.prefix,'m1_frequencies.png')), 
       plot = p, 
       h = 10, 
       w = 8
       )
```

This is the fit of the Poisson model with participant effects. We see that adding participant effects does not improve model performance.


```{r likert_poisson_with_unit_effects_extract_class_probs_plot, include=TRUE, eval=TRUE, echo=FALSE, tidy=FALSE, fig.align='center', out.width='80%'}
knitr::include_graphics( file.path(dir.out, paste0(file.prefix,'m1_frequencies.png')) )
```

## Compare Poisson to Ordered Categorical models

To quantify the differences in model fit not only by eye but also quantitatively, we consider as accuracy measure Bayesian leave-one-out expected log posterior density (Bayes-LOO-ELPD):

```{r PHQ4_Compare Bayesian LOO approximations, include=FALSE, eval=TRUE, echo=FALSE, message=FALSE, tidy=FALSE, cache=TRUE}
poisson_log_likert_fit_m0 <- readRDS(file = file.path(dir.out, 'parentalmh_poisson_likert_m0_stan.rds'))
poisson_log_likert_fit_m1 <- readRDS(file = file.path(dir.out, 'parentalmh_poisson_likert_m1_stan.rds'))
ordered_categorical_logit_fit_m0a <- readRDS(file = file.path(dir.out, 'parentalmh_ordered_categorical_m0a_stan.rds'))
ordered_categorical_logit_fit_m0 <- readRDS(file = file.path(dir.out, 'parentalmh_ordered_categorical_m0_stan.rds'))
ordered_categorical_logit_fit <- readRDS(file = file.path(dir.out, 'parentalmh_ordered_categorical_m1_stan.rds'))

poisson_log_likert_fit_m0_loo <- poisson_log_likert_fit_m0$loo()
poisson_log_likert_fit_m1_loo <- poisson_log_likert_fit_m1$loo()
ordered_categorical_logit_fit_m0a_loo <- ordered_categorical_logit_fit_m0a$loo()
ordered_categorical_logit_fit_m0_loo <- ordered_categorical_logit_fit_m0$loo()
ordered_categorical_logit_fit_loo <- ordered_categorical_logit_fit$loo()
```

```{r PHQ4_Compare Bayesian LOO approximations table, include=TRUE, eval=TRUE, echo=TRUE, tidy=FALSE, cache=TRUE}
# Model 1 - Poisson no participant effects
# Model 2 - Poisson with participant random effects
# Model 3 - Textbook Ordered Cat, no participant effects
# Model 4 - Multi-group Ordered Cat, no participant effects
# Model 5 - Multi-group Ordered Cat, with participant random effects
comp <- loo::loo_compare(poisson_log_likert_fit_m0_loo,
                         poisson_log_likert_fit_m1_loo,
                         ordered_categorical_logit_fit_m0a_loo,
                         ordered_categorical_logit_fit_m0_loo, 
                         ordered_categorical_logit_fit_loo
                         )
print(comp, simplify = FALSE)
```

Woah, but also as expected from all the plots above. We see that the multi-group ordered categorical with participant effects is the top performing model in terms of Bayes-LOO-ELPD. All other models have estimated ELPD's such that ELPD + 2 x standard errors remains below the top performing model. The difference in ELPD between the Ordered Categorical and Poisson model is very large. With this we conclude that the differences in model performances are significant. This matches the visual behavior of the model fits shown in the plots above. The Poisson models thereofore provide a worse fit to the data than the Ordered Categorical models.


# PHQ4 - Ordered Categorical Models with response correlations

We still cannot capture any of the polychoric correlations between responses at the same time point that we observed in the data. 

## Model with latent MVN correlations

To fix this, we could adopt the classical model used to quantify polychoric correlations. 
That is, we add multivariate normal random effects for each item response, and such that the correlations are explicitly tracked in the MVN-correlation matrix. The corresponding model is

\begin{align*}
& \text{Prob}(Y_{ij} = k) = \text{Ordered-Logistic}(k; \eta_{ij}, c_{G(i)}) \\
& \eta_{ij}= \beta_{G(i)} + \gamma_j\\
& c_g = [c_{g1}, c_{g1} + \text{cumsum}(\delta_{g,1:{K-2}})] \\
& c_{g1} \sim \text{Normal}(0 , 3.5^2)\\
& \delta_{g,1:{K-2}} \sim \text{Normal}(0 , 2.5^2)\\
& \beta \sim \text{S2Z-Normal}(0, 1)\\
& \gamma \sim \text{MVN-Normal}(0, \sigma_\gamma L L^T \sigma_\gamma^T)\\
& L \sim \text{LKJ}(2)\\
& \sigma_\gamma \sim \text{Cauchy}(0,1)
\end{align*}

Fitting this model in a Bayesian setting is however challenging due to the unidentifiability introduced between the $\beta_{g}$ and $\gamma_j$. I tried this, but opted against pursueing this approach further, as it won't scale and well and remains partly unidentifiable.


```{r ordered_logistic_single_mvn_stan_code, include=FALSE, eval=TRUE, echo=FALSE, tidy=FALSE, cache=TRUE}
file.prefix <- 'parentalmh_ordered_categorical_'

# this is a model with correlation among outcomes but without unit effects 
ordered_categorical_logit_txt_m4 <- "
data
{
  int<lower=1> N; // number of observations
  int<lower=3> K; // number of categories
  int<lower=2> Q; // number of questions
  int<lower=2> P; // number of groups
  array[N] int<lower=1, upper=K> y; // observations
  array [N] int<lower=1,upper=P> group_of_obs;
  array [N] int<lower=1,upper=Q> question_of_obs;
}
transformed data
{
    real s2z_sd_groups;
    vector[Q] zeros_Q_vector;
    
    s2z_sd_groups = inv(sqrt(1. - inv(P)));
    zeros_Q_vector = rep_vector(0.0, Q);
}
parameters
{
  sum_to_zero_vector[P] beta_group;
  
  // the cutpoints in logit space, using the 'ordered' variable type
  real cut_point_1;
  matrix<lower=0>[K - 2,P] cut_point_gaps_groups;
  
  // parameters for correlation in responses
  cholesky_factor_corr[Q] questions_L_omega;
  vector<lower=0>[Q] questions_L_sigma;
  vector[Q] questions_z;
}
transformed parameters
{
  vector[Q] questions_mvn;
  array[P] ordered[K-1] cut_points;
  
  questions_mvn = diag_pre_multiply(questions_L_sigma, questions_L_omega) * questions_z;
  
  for (p in 1:P)
  {
    cut_points[p] = 
      append_row(
        rep_vector(cut_point_1, 1), 
        rep_vector(cut_point_1, K-2) + cumulative_sum(cut_point_gaps_groups[,p])
        );
  }
}
model
{
  // likelihood, cannot be vectorized
  for (n in 1:N) {
    y[n] ~ ordered_logistic(
      beta_group[group_of_obs[n]] + questions_mvn[question_of_obs[n]], 
      cut_points[group_of_obs[n]]);
  }
  
  // priors
  beta_group ~ normal(0, s2z_sd_groups); 
  cut_point_1 ~ normal(0, 3.5); 
  to_vector(cut_point_gaps_groups) ~ normal(0, 2.5);
  questions_L_sigma ~ cauchy(0,1);
  questions_L_omega ~ lkj_corr_cholesky(2.0);
  questions_z ~ std_normal();
}
generated quantities
{
  matrix [K,P] ordered_prob;
  array [N] int<lower=0> ypred;
  array [N] real log_lik;
  corr_matrix[Q] questions_omega;
  
  questions_omega = multiply_lower_tri_self_transpose(questions_L_omega);
  
  for (p in 1:P) {
    ordered_prob[1,p]       = 1 - inv_logit( beta_group[p] - cut_points[p][1] );
    ordered_prob[2:(K-1),p] = inv_logit( beta_group[p] - cut_points[p][1:(K-2)] ) 
                              - 
                              inv_logit( beta_group[p] - cut_points[p][2:(K-1)] );
    ordered_prob[K,p]       = inv_logit( beta_group[p] - cut_points[p][K-1] );
  }
    
  for( n in 1:N ) {
    ypred[n] = ordered_logistic_rng( 
                beta_group[group_of_obs[n]] + questions_mvn[question_of_obs[n]], 
                cut_points[group_of_obs[n]]
                ); 
    log_lik[n] =  ordered_logistic_lpmf( y[n] |
                    beta_group[group_of_obs[n]] + questions_mvn[question_of_obs[n]], 
                    cut_points[group_of_obs[n]]);
  }
}
"

# compile the model
ordered_categorical_logit_filename_m4 <- cmdstanr::write_stan_file(
  gsub('\t',' ', ordered_categorical_logit_txt_m4),
  dir = dir.out,
  basename = NULL,
  force_overwrite = FALSE,
  hash_salt = ""
)

# compile Stan model
ordered_categorical_logit_compiled_m4 <- cmdstanr::cmdstan_model(ordered_categorical_logit_filename_m4, force_recompile = TRUE)
```

```{r ordered_logistic_single_mvn_run_model, include=FALSE, eval=TRUE, echo=FALSE, message=FALSE, tidy=FALSE, cache=TRUE}
# define data in format needed for model specification
stan_data <- list()
stan_data$N <- nrow(dcat1)
stan_data$P <- max(dcat1$group)
stan_data$Q <- max(dcat1$vid)
stan_data$K <- length(unique(dcat1$value))
stan_data$y <- dcat1$value
stan_data$group_of_obs <- dcat1$group
stan_data$question_of_obs <- dcat1$vid
# sample
ordered_categorical_logit_fit_m4 <- ordered_categorical_logit_compiled_m4$sample(
  data = stan_data,
  seed = 123,
  chains = 2,
  parallel_chains = 2,
  iter_warmup = 5e2,
  iter_sampling = 15e2,
  refresh = 500,
  save_warmup = TRUE
)

# save output to RDS
ordered_categorical_logit_fit_m4$save_object(file = file.path(dir.out, paste0(file.prefix,"m4_stan.rds")))
```
```{r ordered_logistic_single_mvn_load_model, include=FALSE, eval=TRUE, echo=FALSE, tidy=FALSE, cache=TRUE}
ordered_categorical_logit_fit_m4 <- readRDS(file = file.path(dir.out, paste0(file.prefix,'m4_stan.rds')))
```


```{r ordered_logistic_single_mvn_check mixing convergence, include=FALSE, eval=TRUE, echo=FALSE, tidy=FALSE, cache=TRUE}
#	check convergence and check mixing
#	get 95% credible intervals
tmp <- ordered_categorical_logit_fit_m4$summary(
  variables = c('beta_group','cut_point_1','cut_point_gaps_groups','questions_L_sigma','questions_L_omega','questions_z'),
  posterior::default_summary_measures(),
  posterior::default_convergence_measures(),
  extra_quantiles = ~posterior::quantile2(., probs = c(.0275, .975))
)
tmp <- as.data.table(tmp)
tmp
  
# parameter with lowest ess_bulk
worst_var <- tmp$variable[ which.min(tmp$ess_bulk) ]

# extract samples
po <- ordered_categorical_logit_fit_m4$draws(
  variables = c("lp__", worst_var),
  inc_warmup = TRUE,
  format = "draws_array"
  )

# make worst trace plot
p <- bayesplot:::mcmc_trace(po,  
                            pars = c("lp__",worst_var), 
                            n_warmup = 500,
                            facet_args = list(nrow = 2)
                            )
p <- p + theme_bw()
ggsave(file = file.path(dir.out, paste0(file.prefix,'m4_worsttrace.png')), 
       plot = p, 
       h = 10, 
       w = 8
       )

# make intervals plot
po <- ordered_categorical_logit_fit_m4$draws(
  variables = c('beta_group','cut_point_1','cut_point_gaps_groups','questions_mvn','questions_omega'),
  inc_warmup = FALSE,
  format = "draws_array"
  )

color_scheme_set("teal")
p <- bayesplot::mcmc_intervals(po, prob = 0.5, prob_outer = 0.95, outer_size = 1, point_size = 2) +
  theme_bw()
ggsave(file = file.path(dir.out, paste0(file.prefix,'m4_intervals.png')), 
       plot = p, 
       h = 7, 
       w = 8, 
       limitsize = FALSE
       )

po <- ordered_categorical_logit_fit_m4$draws(
  variables = c('lp__','ordered_prob[1,1]','ordered_prob[2,1]','ordered_prob[3,1]','ordered_prob[4,1]',
                'beta_group[1]','cut_point_1','cut_point_gaps_groups','questions_mvn'),
  inc_warmup = FALSE,
  format = "draws_array"
  )

bayesplot::color_scheme_set('viridisC')
p <- bayesplot::mcmc_pairs(po, 
                           diag_fun = "dens", 
                           off_diag_fun = "hex"
                           ) 
ggsave(file = file.path(dir.out, paste0(file.prefix,'m4_pairsplot.png')), 
       plot = p, 
       h = 50, 
       w = 50, 
       limitsize = FALSE
       )
bayesplot::color_scheme_set('brewer-RdYlBu')


po <- ordered_categorical_logit_fit_m4$draws(
  variables = c('lp__','questions_mvn','questions_z','questions_omega[2,1]','questions_omega[3,1]','questions_omega[4,1]',
                'questions_omega[3,2]','questions_omega[4,2]','questions_omega[4,3]'),
  inc_warmup = FALSE,
  format = "draws_array"
  )

bayesplot::color_scheme_set('viridisC')
p <- bayesplot::mcmc_pairs(po, 
                           diag_fun = "dens", 
                           off_diag_fun = "hex"
                           ) 
ggsave(file = file.path(dir.out, paste0(file.prefix,'m4_pairsplot_questions.png')), 
       plot = p, 
       h = 50, 
       w = 50, 
       limitsize = FALSE
       )
bayesplot::color_scheme_set('brewer-RdYlBu')

# make posterior predictive check

# create median and 95\% credible intervals
po <- ordered_categorical_logit_fit_m4$draws(
  variables = c('ypred'),
  inc_warmup = FALSE,
  format = "draws_df"
  )
po <- as.data.table(po)
po <- data.table::melt(po, id.vars = c('.draw','.chain','.iteration'), value.name = 'ypred')
po[, oid := as.integer(gsub('ypred\\[([0-9]+)\\]','\\1',variable)) ]
pos <- 
  po[,
     list( summary_value = quantile(ypred, prob = c(0.025, 0.25, 0.5, 0.75, 0.975)),
           summary_name = c('q_lower','iqr_lower','median','iqr_upper','q_upper') 
           ),
     by = 'oid'
     ]
pos <- 
  data.table::dcast(pos,
                    oid ~ summary_name, 
                    value.var = 'summary_value'
                    )
pos <- merge(pos, dcat1, by = 'oid')
pos[, IN_PPI := value >= q_lower & value <= q_upper]
pos[, mean(IN_PPI)]

# plot posterior predictive check
p <- ggplot(pos, aes(x = oid, group = oid)) + 
  geom_boxplot( aes( ymin = q_lower,
                     lower = iqr_lower,
                     middle = median,
                     upper = iqr_upper,
                     ymax = q_upper),
                stat = 'identity') +
  geom_point( aes(y = value, colour = IN_PPI ) ) +
  facet_grid(variable ~ arm_label+time_label, scales = 'free') +
  scale_x_discrete() +
  scale_y_continuous() +
  ggsci::scale_color_npg() +
  labs(x = '', 
       y = 'outcome', 
       colour = 'within\n95% posterior\nprediction\ninterval') +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 45,vjust = 1,hjust = 1))
ggsave(file = file.path(dir.out, paste0(file.prefix,'m4_ppcheck.png')), 
       p, 
       w = 12, 
       h = 12)
```

```{r ordered_logistic_single_mvn_extract_class_probs, include=FALSE, eval=TRUE, echo=FALSE, tidy=FALSE, cache=TRUE}
# extract model probabilities
po <- ordered_categorical_logit_fit_m4$draws(
  variables = c('ordered_prob'),
  inc_warmup = FALSE,
  format = "draws_df"
  )
po <- as.data.table(po)
setnames(po, colnames(po), gsub('\\]','',gsub('\\[','_',colnames(po))))
po <- data.table::melt(po, id.vars = c('.draw','.chain','.iteration'), value.name = 'prob')
po[, value := as.integer(gsub('ordered_prob_([0-9]),([0-9])','\\1',variable)) ]
po[, group := as.integer(gsub('ordered_prob_([0-9]),([0-9])','\\2',variable)) ]

tmp <- unique( subset(dcat1, select = c(group, arm, arm_label, time, time_label, value, value_label)) )
po <- merge(po, tmp, by = c('group','value'))
```

```{r ordered_logistic_single_mvn_make_plots, include=FALSE, eval=TRUE, echo=FALSE, tidy=FALSE, cache=TRUE}
pos <- po[, 
          list( summary_value = quantile(prob, prob = c(0.025, 0.25, 0.5, 0.75, 0.975)),
                summary_name = c('q_lower','iqr_lower','median','iqr_upper','q_upper') 
                ), 
          by = c('group','arm','arm_label','time','time_label','value','value_label')
          ]
pos <- data.table::dcast(pos, group + arm + arm_label + time + time_label + value + value_label ~ summary_name, value.var = 'summary_value')

tmp <- dcat1[, list(n = length(pid)), by = c('arm_label','time_label','value_label','value')]
tmp2 <- tmp[, list(total = sum(n)), by = c('arm_label','time_label')]
tmp <- merge(tmp, tmp2, by = c('arm_label','time_label'))
tmp[, p_emp := n/total]
pos <- merge(pos, subset(tmp, select = -c(value, n, total)), by = c('arm_label','time_label','value_label'))

p <- ggplot(data = pos, aes( x = value_label, fill = interaction(arm_label,time_label))) +
  geom_bar(aes( y = p_emp), 
           stat = 'identity', 
           position = position_dodge()) +
  geom_boxplot( aes(ymin = q_lower, lower = iqr_lower, middle = median, upper = iqr_upper, ymax = q_upper), 
                 position = position_dodge(0.9),
                stat = 'identity',
                alpha = 0,
                width = 0.3,
                outliers = FALSE) +
  scale_fill_npg() +
  scale_y_continuous(labels = scales::percent) +
  labs(x = '\nParental mental health\nPHQ4 outcome', y = 'group-level\nposterior probability', fill = '') +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 45,vjust = 1,hjust = 1),
        legend.position = 'bottom'
        )
ggsave(file = file.path(dir.out, paste0(file.prefix,'m4_probs_barplot.png')), 
       plot = p, 
       h = 8, 
       w = 8
       )
```

```{r ordered_logistic_single_mvn_plot, include=FALSE, eval=TRUE, echo=FALSE, tidy=FALSE, fig.align='center', out.width='80%'}
knitr::include_graphics( file.path(dir.out, paste0(file.prefix,'m4_probs_barplot.png')) )
```


```{r ordered_logistic_single_mvn_make_plots_2, include=FALSE, eval=TRUE, echo=FALSE, tidy=FALSE, cache=TRUE}
# Frequency plot
tmp <- dcat1[, list(n = length(pid)), by = c('arm_label','time_label','variable','value_label', 'value')]
tmp2 <- tmp[, list(total = sum(n)), by = c('arm_label','time_label','variable')]
tmp <- merge(tmp, tmp2, by = c('arm_label','time_label','variable'))
tmp[, p_emp_by_variable := n/total]


p <- ggplot(tmp, aes(x = value_label)) +
  geom_col(aes(fill = variable, y = p_emp_by_variable), position = position_dodge()) +
  geom_point(data = pos, aes(y = median), pch = 18, size = 2, colour = 'black') +
  geom_linerange(data = pos, aes(ymin = q_lower, ymax = q_upper), colour = 'black') +
  scale_fill_bmj() +
  scale_y_continuous(labels = scales::percent) +
  facet_grid(arm_label ~ time_label) +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 45,vjust = 1,hjust = 1),
        legend.position = 'bottom'
        ) +
  labs(x = '', x = 'Likert scale', y = 'proportion of outcomes', fill = '')
ggsave(file = file.path(dir.out, paste0(file.prefix,'m4_frequencies_v2.png')), 
       plot = p, 
       h = 10, 
       w = 8
       )
```

```{r ordered_logistic_single_mvn_plot_2, include=FALSE, eval=TRUE, echo=FALSE, tidy=FALSE, fig.align='center', out.width='80%'}
knitr::include_graphics( file.path(dir.out, paste0(file.prefix,'m4_frequencies_v2.png')) )
```



## Textbook model with latent S2Z correlations

I was able to make more progress by considering fixed or random effects for each item response that are a priori independent of each other, so that the correlations are not explicitly tracked as a distinct model parameter. The primary insight here is that the joint posterior density of the response-level random effects may nonetheless capture correlations, and we can compute the correlations from the Monte Carlo samples of the joint posterior density. Additionally, we can ensure parameter identifiability by placing sum-to-zero priors on the iid response-level random effects. This model is bound to sample very well.

Here is the fixed effects version, with baseline fixed effect parameters for each survey item. I started with the textbook Ordered Categorical model that does not have group-specific cutpoints:

\begin{align*}
& \text{Prob}(Y_{ij} = k) = \text{Ordered-Logistic}(k; \eta_{ij}, c) \\
& \eta_{ij}= \beta_{G(i)} + \gamma_j\\
& c = [c_{1}, c_{1} + \text{cumsum}(\delta_{1:{K-2}})] \\
& c_{1} \sim \text{Normal}(0 , 3.5^2)\\
& \delta_{1:{K-2}} \sim \text{Normal}(0 , 2.5^2)\\
& \beta \sim \text{S2Z-Normal}(0, 1)\\
& \gamma \sim \text{S2Z-Normal}(0,1)
\end{align*}

Here is the corresponding Stan code:

```{r ordered_logistic_a_s2z_stan_code, include=TRUE, eval=TRUE, echo=TRUE, tidy=FALSE, cache=TRUE}
file.prefix <- 'parentalmh_ordered_categorical_'

# this is a model with correlation among outcomes but without unit effects 
ordered_categorical_logit_txt_m4a <- "
data
{
  int<lower=1> N; // number of observations
  int<lower=3> K; // number of categories
  int<lower=2> Q; // number of questions
  int<lower=2> P; // number of groups
  array[N] int<lower=1, upper=K> y; // observations
  array [N] int<lower=1,upper=P> group_of_obs;
  array [N] int<lower=1,upper=Q> question_of_obs;
}
transformed data
{
    real s2z_sd_groups;
    real s2z_sd_questions;
    
    s2z_sd_groups = inv(sqrt(1. - inv(P)));
    s2z_sd_questions = inv(sqrt(1. - inv(Q)));
}
parameters
{
  sum_to_zero_vector[P] beta_group;
  
  // parameters for correlation in responses
  sum_to_zero_vector[Q] beta_questions;
  
  // the cutpoints in logit space, using the 'ordered' variable type
  real cut_point_1;
  vector<lower=0>[K - 2] cut_point_gaps_groups;
}
transformed parameters
{
  ordered[K-1] cut_points;
  cut_points = 
      append_row(
        cut_point_1, 
        rep_vector(cut_point_1, K-2) + cumulative_sum(cut_point_gaps_groups)
        );
}
model
{
  // likelihood, cannot be vectorized
  for (n in 1:N) {
    y[n] ~ ordered_logistic(
      beta_group[group_of_obs[n]] + beta_questions[question_of_obs[n]], 
      cut_points);
  }
  
  // priors
  beta_group ~ normal(0, s2z_sd_groups); 
  cut_point_1 ~ normal(0, 3.5); 
  cut_point_gaps_groups ~ normal(0, 2.5);
  beta_questions ~ normal(0, s2z_sd_questions); 
}
generated quantities
{
  matrix [K,P] ordered_prob;
  array [K,P,Q] real<lower=0, upper=1> ordered_prob_by_question;
  array [N] int<lower=0> ypred;
  array [N] real log_lik;
  
  for (p in 1:P) {
    ordered_prob[1,p]       = 1 - inv_logit( beta_group[p] - cut_points[1] );
    ordered_prob[2:(K-1),p] = inv_logit( beta_group[p] - cut_points[1:(K-2)] ) 
                              - 
                              inv_logit( beta_group[p] - cut_points[2:(K-1)] );
    ordered_prob[K,p]       = inv_logit( beta_group[p] - cut_points[K-1] );
  }
  
  for (q in 1:Q) {
    for (p in 1:P) {
      ordered_prob_by_question[1,p,q]       = 1 - inv_logit( beta_group[p] + beta_questions[q] - cut_points[1] );
      ordered_prob_by_question[2:(K-1),p,q] = to_array_1d( inv_logit( beta_group[p] + beta_questions[q] - cut_points[1:(K-2)] ) 
                                                           - 
                                                           inv_logit( beta_group[p] + beta_questions[q] - cut_points[2:(K-1)] )
                                                           );
      ordered_prob_by_question[K,p,q]       = inv_logit( beta_group[p] + beta_questions[q] - cut_points[K-1] );
    }
  }
    
  for( n in 1:N ) {
    ypred[n] = ordered_logistic_rng( 
                beta_group[group_of_obs[n]] + beta_questions[question_of_obs[n]], 
                cut_points
                ); 
    log_lik[n] =  ordered_logistic_lpmf( y[n] |
                    beta_group[group_of_obs[n]] + beta_questions[question_of_obs[n]], 
                    cut_points);
  }
}
"

# compile the model
ordered_categorical_logit_filename_m4a <- cmdstanr::write_stan_file(
  gsub('\t',' ', ordered_categorical_logit_txt_m4a),
  dir = dir.out,
  basename = NULL,
  force_overwrite = FALSE,
  hash_salt = ""
)

# compile Stan model
ordered_categorical_logit_compiled_m4a <- cmdstanr::cmdstan_model(ordered_categorical_logit_filename_m4a, force_recompile = TRUE)
```


```{r ordered_logistic_a_s2z_run_model, include=TRUE, eval=TRUE, echo=TRUE, message=FALSE, tidy=FALSE, cache=TRUE}
# define data in format needed for model specification
stan_data <- list()
stan_data$N <- nrow(dcat1)
stan_data$P <- max(dcat1$group)
stan_data$Q <- max(dcat1$vid)
stan_data$K <- length(unique(dcat1$value))
stan_data$y <- dcat1$value
stan_data$group_of_obs <- dcat1$group
stan_data$question_of_obs <- dcat1$vid
# sample
ordered_categorical_logit_fit_m4a <- ordered_categorical_logit_compiled_m4a$sample(
  data = stan_data,
  seed = 123,
  chains = 2,
  parallel_chains = 2,
  iter_warmup = 5e2,
  iter_sampling = 15e2,
  refresh = 500,
  save_warmup = TRUE
)

# save output to RDS
ordered_categorical_logit_fit_m4a$save_object(file = file.path(dir.out, paste0(file.prefix,"m4a_stan.rds")))
```
```{r ordered_logistic_a_s2z_load_model, include=FALSE, eval=TRUE, echo=FALSE, tidy=FALSE, cache=TRUE}
ordered_categorical_logit_fit_m4a <- readRDS(file = file.path(dir.out, paste0(file.prefix,'m4a_stan.rds')))
```


```{r ordered_logistic_a_s2z_check mixing convergence, include=FALSE, eval=TRUE, echo=FALSE, tidy=FALSE, cache=TRUE}
#	check convergence and check mixing
#	get 95% credible intervals
tmp <- ordered_categorical_logit_fit_m4a$summary(
  variables = c('beta_group','cut_point_1','cut_point_gaps_groups','beta_questions'),
  posterior::default_summary_measures(),
  posterior::default_convergence_measures(),
  extra_quantiles = ~posterior::quantile2(., probs = c(.0275, .975))
)
tmp <- as.data.table(tmp)
tmp
  
# parameter with lowest ess_bulk
worst_var <- tmp$variable[ which.min(tmp$ess_bulk) ]

# extract samples
po <- ordered_categorical_logit_fit_m4a$draws(
  variables = c("lp__", worst_var),
  inc_warmup = TRUE,
  format = "draws_array"
  )

# make worst trace plot
p <- bayesplot:::mcmc_trace(po,  
                            pars = c("lp__",worst_var), 
                            n_warmup = 500,
                            facet_args = list(nrow = 2)
                            )
p <- p + theme_bw()
ggsave(file = file.path(dir.out, paste0(file.prefix,'m4a_worsttrace.png')), 
       plot = p, 
       h = 10, 
       w = 8
       )

# make intervals plot
po <- ordered_categorical_logit_fit_m4a$draws(
  variables = c('beta_group','cut_point_1','cut_point_gaps_groups','beta_questions'),
  inc_warmup = FALSE,
  format = "draws_array"
  )

color_scheme_set("teal")
p <- bayesplot::mcmc_intervals(po, prob = 0.5, prob_outer = 0.95, outer_size = 1, point_size = 2) +
  theme_bw()
ggsave(file = file.path(dir.out, paste0(file.prefix,'m4a_intervals.png')), 
       plot = p, 
       h = 7, 
       w = 8, 
       limitsize = FALSE
       )

po <- ordered_categorical_logit_fit_m4a$draws(
  variables = c('lp__','ordered_prob[1,1]','ordered_prob[2,1]','ordered_prob[3,1]','ordered_prob[4,1]',
                'beta_group[1]','cut_point_1','cut_point_gaps_groups','beta_questions'),
  inc_warmup = FALSE,
  format = "draws_array"
  )

bayesplot::color_scheme_set('viridisC')
p <- bayesplot::mcmc_pairs(po, 
                           diag_fun = "dens", 
                           off_diag_fun = "hex"
                           ) 
ggsave(file = file.path(dir.out, paste0(file.prefix,'m4a_pairsplot.png')), 
       plot = p, 
       h = 50, 
       w = 50, 
       limitsize = FALSE
       )
bayesplot::color_scheme_set('brewer-RdYlBu')


po <- ordered_categorical_logit_fit_m4a$draws(
  variables = c('lp__','beta_questions'),
  inc_warmup = FALSE,
  format = "draws_array"
  )

bayesplot::color_scheme_set('viridisC')
p <- bayesplot::mcmc_pairs(po, 
                           diag_fun = "dens", 
                           off_diag_fun = "hex"
                           ) 
ggsave(file = file.path(dir.out, paste0(file.prefix,'m4a_pairsplot_questions.png')), 
       plot = p, 
       h = 20, 
       w = 20, 
       limitsize = FALSE
       )
bayesplot::color_scheme_set('brewer-RdYlBu')

# make posterior predictive check

# create median and 95\% credible intervals
po <- ordered_categorical_logit_fit_m4a$draws(
  variables = c('ypred'),
  inc_warmup = FALSE,
  format = "draws_df"
  )
po <- as.data.table(po)
po <- data.table::melt(po, id.vars = c('.draw','.chain','.iteration'), value.name = 'ypred')
po[, oid := as.integer(gsub('ypred\\[([0-9]+)\\]','\\1',variable)) ]
pos <- 
  po[,
     list( summary_value = quantile(ypred, prob = c(0.025, 0.25, 0.5, 0.75, 0.975)),
           summary_name = c('q_lower','iqr_lower','median','iqr_upper','q_upper') 
           ),
     by = 'oid'
     ]
pos <- 
  data.table::dcast(pos,
                    oid ~ summary_name, 
                    value.var = 'summary_value'
                    )
pos <- merge(pos, dcat1, by = 'oid')
pos[, IN_PPI := value >= q_lower & value <= q_upper]
pos[, mean(IN_PPI)]

# plot posterior predictive check
p <- ggplot(pos, aes(x = oid, group = oid)) + 
  geom_boxplot( aes( ymin = q_lower,
                     lower = iqr_lower,
                     middle = median,
                     upper = iqr_upper,
                     ymax = q_upper),
                stat = 'identity') +
  geom_point( aes(y = value, colour = IN_PPI ) ) +
  facet_grid(variable ~ arm_label+time_label, scales = 'free') +
  scale_x_discrete() +
  scale_y_continuous() +
  ggsci::scale_color_npg() +
  labs(x = '', 
       y = 'outcome', 
       colour = 'within\n95% posterior\nprediction\ninterval') +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 45,vjust = 1,hjust = 1))
ggsave(file = file.path(dir.out, paste0(file.prefix,'m4a_ppcheck.png')), 
       p, 
       w = 12, 
       h = 12)
```



```{r ordered_logistic_a_s2z_latent_eta, include=FALSE, eval=TRUE, echo=FALSE, tidy=FALSE, cache=TRUE}
# extract model probabilities
po <- ordered_categorical_logit_fit_m4a$draws(
  variables = c('beta_group','beta_questions'),
  inc_warmup = FALSE,
  format = "draws_df"
  )
po <- as.data.table(po)
po <- data.table::melt(po, id.vars = c('.draw','.chain','.iteration'), value.name = 'prob')
po[, value := as.integer(gsub('(.*)\\[([0-9])\\]$','\\2',variable)) ]
po[, variable := gsub('(.*)\\[([0-9])\\]$','\\1',variable) ]
tmp <- subset(po, variable == 'beta_group')
tmp <- data.table::dcast(tmp, .draw ~ paste0('beta_group_',value), value.var = 'prob')
po <- merge(subset(po, variable == 'beta_questions'), tmp, by = c('.draw'))
po[, eta_group_1 := beta_group_1 + prob]
po[, eta_group_2 := beta_group_2 + prob]
po[, eta_group_3 := beta_group_3 + prob]
po[, eta_group_4 := beta_group_4 + prob]
setnames(po, 'value', 'vid')
po <- subset(po, select = -c(variable, prob, .chain, .iteration, beta_group_1, beta_group_2, beta_group_3, beta_group_4))
po <- data.table::melt(po, id.vars = c('.draw','vid'), value.name = 'eta')
po[, group := as.integer(gsub('(.*)_([0-9])$','\\2',variable)) ]
po <- subset(po, select = -variable)
tmp <- unique( subset(dcat1, select = c(group, arm, arm_label, time, time_label, variable, vid)) )
po <- merge(po, tmp, by = c('group','vid'))
po <- data.table::dcast(po, .draw + arm + arm_label + time + time_label + group ~ variable, value.var = 'eta')
p <- GGally::ggpairs(po, 
                columns = c('PHQ4_down','PHQ4_interest','PHQ4_nervous','PHQ4_worry'),
                ggplot2::aes(colour = interaction(arm_label, time_label)),
                lower = list(continuous = ggally_density),
                diag = list(continuous = 'blankDiag')
                )
p <- p + scale_fill_npg() + 
  theme_bw()
ggsave(file = file.path(dir.out, paste0(file.prefix,'m4a_eta_pairsplot.png')), 
       plot = p, 
       h = 10, 
       w = 10
       )
```

We can illustrate the posterior correlations by plotting the shape of the posterior density of the linear predictor $\eta_{ij}$ across the four item responses. It is hard to plot a 4D density, but we can easily visualise all pairwise combinations of the 4D density across the four groups (shown in colours, lower triangular part). We clearly see that the model estimates polychoric correlations at a similar level as we observe in the data (upper triangular part).


```{r ordered_logistic_a_s2z_latent_eta_plot, include=TRUE, eval=TRUE, echo=FALSE, tidy=FALSE, fig.align='center', out.width='80%'}
knitr::include_graphics( file.path(dir.out, paste0(file.prefix,'m4a_eta_pairsplot.png')) )
```


```{r ordered_logistic_a_s2z_extract_class_probs, include=FALSE, eval=TRUE, echo=FALSE, tidy=FALSE, cache=TRUE}
# extract model probabilities
po <- ordered_categorical_logit_fit_m4a$draws(
  variables = c('ordered_prob'),
  inc_warmup = FALSE,
  format = "draws_df"
  )
po <- as.data.table(po)
setnames(po, colnames(po), gsub('\\]','',gsub('\\[','_',colnames(po))))
po <- data.table::melt(po, id.vars = c('.draw','.chain','.iteration'), value.name = 'prob')
po[, value := as.integer(gsub('ordered_prob_([0-9]),([0-9])','\\1',variable)) ]
po[, group := as.integer(gsub('ordered_prob_([0-9]),([0-9])','\\2',variable)) ]

tmp <- unique( subset(dcat1, select = c(group, arm, arm_label, time, time_label, value, value_label)) )
po <- merge(po, tmp, by = c('group','value'))
```

```{r ordered_logistic_a_s2z_make_plots, include=FALSE, eval=TRUE, echo=FALSE, tidy=FALSE, cache=TRUE}
pos <- po[, 
          list( summary_value = quantile(prob, prob = c(0.025, 0.25, 0.5, 0.75, 0.975)),
                summary_name = c('q_lower','iqr_lower','median','iqr_upper','q_upper') 
                ), 
          by = c('group','arm','arm_label','time','time_label','value','value_label')
          ]
pos <- data.table::dcast(pos, group + arm + arm_label + time + time_label + value + value_label ~ summary_name, value.var = 'summary_value')

tmp <- dcat1[, list(n = length(pid)), by = c('arm_label','time_label','value_label','value')]
tmp2 <- tmp[, list(total = sum(n)), by = c('arm_label','time_label')]
tmp <- merge(tmp, tmp2, by = c('arm_label','time_label'))
tmp[, p_emp := n/total]
pos <- merge(pos, subset(tmp, select = -c(value, n, total)), by = c('arm_label','time_label','value_label'))

p <- ggplot(data = pos, aes( x = value_label, fill = interaction(arm_label,time_label))) +
  geom_bar(aes( y = p_emp), 
           stat = 'identity', 
           position = position_dodge()) +
  geom_boxplot( aes(ymin = q_lower, lower = iqr_lower, middle = median, upper = iqr_upper, ymax = q_upper), 
                 position = position_dodge(0.9),
                stat = 'identity',
                alpha = 0,
                width = 0.3,
                outliers = FALSE) +
  scale_fill_npg() +
  scale_y_continuous(labels = scales::percent) +
  labs(x = '\nParental mental health\nPHQ4 outcome', y = 'group-level\nposterior probability', fill = '') +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 45,vjust = 1,hjust = 1),
        legend.position = 'bottom'
        )
ggsave(file = file.path(dir.out, paste0(file.prefix,'m4a_probs_barplot.png')), 
       plot = p, 
       h = 8, 
       w = 8
       )
```

This is the model fit (in black) relative to the aggregated survey items. Of course, this textbook model has the same deficiencies as the simpler textbook model further above. 

```{r ordered_logistic_a_s2z_plot, include=TRUE, eval=TRUE, echo=FALSE, tidy=FALSE, fig.align='center', out.width='80%'}
knitr::include_graphics( file.path(dir.out, paste0(file.prefix,'m4a_probs_barplot.png')) )
```

```{r ordered_logistic_a_s2z_make_plots_2, include=FALSE, eval=TRUE, echo=FALSE, tidy=FALSE, cache=TRUE}
# extract model probabilities
po <- ordered_categorical_logit_fit_m4a$draws(
  variables = c('ordered_prob_by_question'),
  inc_warmup = FALSE,
  format = "draws_df"
  )
po <- as.data.table(po)
setnames(po, colnames(po), gsub('\\]','',gsub('\\[','_',colnames(po))))
po <- data.table::melt(po, id.vars = c('.draw','.chain','.iteration'), value.name = 'prob')
po[, value := as.integer(gsub('ordered_prob_([0-9]),([0-9])','\\1',variable)) ]
po[, group := as.integer(gsub('ordered_prob_([0-9]),([0-9])','\\2',variable)) ]

tmp <- unique( subset(dcat1, select = c(group, arm, arm_label, time, time_label, value, value_label)) )
po <- merge(po, tmp, by = c('group','value'))


# Frequency plot
tmp <- dcat1[, list(n = length(pid)), by = c('arm_label','time_label','variable','value_label', 'value')]
tmp2 <- tmp[, list(total = sum(n)), by = c('arm_label','time_label','variable')]
tmp <- merge(tmp, tmp2, by = c('arm_label','time_label','variable'))
tmp[, p_emp_by_variable := n/total]


p <- ggplot(tmp, aes(x = value_label)) +
  geom_col(aes(fill = variable, y = p_emp_by_variable), position = position_dodge()) +
  geom_point(data = pos, aes(y = median), pch = 18, size = 2, colour = 'black') +
  geom_linerange(data = pos, aes(ymin = q_lower, ymax = q_upper), colour = 'black') +
  scale_fill_bmj() +
  scale_y_continuous(labels = scales::percent) +
  facet_grid(arm_label ~ time_label) +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 45,vjust = 1,hjust = 1),
        legend.position = 'bottom'
        ) +
  labs(x = '', x = 'Likert scale', y = 'proportion of outcomes', fill = '')
ggsave(file = file.path(dir.out, paste0(file.prefix,'m4a_frequencies_v2.png')), 
       plot = p, 
       h = 10, 
       w = 8
       )
```

```{r ordered_logistic_a_s2z_plot_2, include=FALSE, eval=FALSE, echo=FALSE, tidy=FALSE, fig.align='center', out.width='80%'}
knitr::include_graphics( file.path(dir.out, paste0(file.prefix,'m4a_frequencies_v2.png')) )
```

## Multi-group model with S2Z correlations

We can of course easily extend the textbook model as elaborated further above,

\begin{align*}
& \text{Prob}(Y_{ij} = k) = \text{Ordered-Logistic}(k; \eta_{ij}, c_{G(i)}) \\
& \eta_{ij}= \beta_{G(i)} + \gamma_j\\
& c_g = [c_{g1}, c_{g1} + \text{cumsum}(\delta_{g,1:{K-2}})] \\
& c_{g1} \sim \text{Normal}(0 , 3.5^2)\\
& \delta_{g,1:{K-2}} \sim \text{Normal}(0 , 2.5^2)\\
& \beta \sim \text{S2Z-Normal}(0, 1)\\
& \gamma \sim \text{S2Z-Normal}(0,1)
\end{align*}

Here is the Stan code:

```{r ordered_logistic_s2z_stan_code, include=TRUE, eval=TRUE, echo=TRUE, tidy=FALSE, cache=TRUE}
file.prefix <- 'parentalmh_ordered_categorical_'

# this is a model with correlation among outcomes but without unit effects 
ordered_categorical_logit_txt_m4 <- "
data
{
  int<lower=1> N; // number of observations
  int<lower=3> K; // number of categories
  int<lower=2> Q; // number of questions
  int<lower=2> P; // number of groups
  array[N] int<lower=1, upper=K> y; // observations
  array [N] int<lower=1,upper=P> group_of_obs;
  array [N] int<lower=1,upper=Q> question_of_obs;
}
transformed data
{
    real s2z_sd_groups;
    real s2z_sd_questions;
    
    s2z_sd_groups = inv(sqrt(1. - inv(P)));
    s2z_sd_questions = inv(sqrt(1. - inv(Q)));
}
parameters
{
  sum_to_zero_vector[P] beta_group;
  
  // parameters for correlation in responses
  sum_to_zero_vector[Q] beta_questions;
  
  // the cutpoints in logit space, using the 'ordered' variable type
  real cut_point_1;
  matrix<lower=0>[K - 2,P] cut_point_gaps_groups;
}
transformed parameters
{
  array[P] ordered[K-1] cut_points;
  for (p in 1:P)
  {
    cut_points[p] = 
      append_row(
        rep_vector(cut_point_1, 1), 
        rep_vector(cut_point_1, K-2) + cumulative_sum(cut_point_gaps_groups[,p])
        );
  }
}
model
{
  // likelihood, cannot be vectorized
  for (n in 1:N) {
    y[n] ~ ordered_logistic(
      beta_group[group_of_obs[n]] + beta_questions[question_of_obs[n]], 
      cut_points[group_of_obs[n]]);
  }
  
  // priors
  beta_group ~ normal(0, s2z_sd_groups); 
  cut_point_1 ~ normal(0, 3.5); 
  to_vector(cut_point_gaps_groups) ~ normal(0, 2.5);
  beta_questions ~ normal(0, s2z_sd_questions); 
}
generated quantities
{
  matrix [K,P] ordered_prob;
  array [K,P,Q] real<lower=0, upper=1> ordered_prob_by_question;
  array [N] int<lower=0> ypred;
  array [N] real log_lik;
  
  for (p in 1:P) {
    ordered_prob[1,p]       = 1 - inv_logit( beta_group[p] - cut_points[p][1] );
    ordered_prob[2:(K-1),p] = inv_logit( beta_group[p] - cut_points[p][1:(K-2)] ) 
                              - 
                              inv_logit( beta_group[p] - cut_points[p][2:(K-1)] );
    ordered_prob[K,p]       = inv_logit( beta_group[p] - cut_points[p][K-1] );
  }
  
  for (q in 1:Q) {
    for (p in 1:P) {
      ordered_prob_by_question[1,p,q]       = 1 - inv_logit( beta_group[p] + beta_questions[q] - cut_points[p][1] );
      ordered_prob_by_question[2:(K-1),p,q] = to_array_1d( inv_logit( beta_group[p] + beta_questions[q] - cut_points[p][1:(K-2)] ) 
                                                           - 
                                                           inv_logit( beta_group[p] + beta_questions[q] - cut_points[p][2:(K-1)] )
                                                           );
      ordered_prob_by_question[K,p,q]       = inv_logit( beta_group[p] + beta_questions[q] - cut_points[p][K-1] );
    }
  }
    
  for( n in 1:N ) {
    ypred[n] = ordered_logistic_rng( 
                beta_group[group_of_obs[n]] + beta_questions[question_of_obs[n]], 
                cut_points[group_of_obs[n]]
                ); 
    log_lik[n] =  ordered_logistic_lpmf( y[n] |
                    beta_group[group_of_obs[n]] + beta_questions[question_of_obs[n]], 
                    cut_points[group_of_obs[n]]);
  }
}
"

# compile the model
ordered_categorical_logit_filename_m4 <- cmdstanr::write_stan_file(
  gsub('\t',' ', ordered_categorical_logit_txt_m4),
  dir = dir.out,
  basename = NULL,
  force_overwrite = FALSE,
  hash_salt = ""
)

# compile Stan model
ordered_categorical_logit_compiled_m4 <- cmdstanr::cmdstan_model(ordered_categorical_logit_filename_m4, force_recompile = TRUE)
```

Let's run this model too:

```{r ordered_logistic_s2z_run_model, include=TRUE, eval=TRUE, echo=TRUE, message=FALSE, tidy=FALSE, cache=TRUE}
# define data in format needed for model specification
stan_data <- list()
stan_data$N <- nrow(dcat1)
stan_data$P <- max(dcat1$group)
stan_data$Q <- max(dcat1$vid)
stan_data$K <- length(unique(dcat1$value))
stan_data$y <- dcat1$value
stan_data$group_of_obs <- dcat1$group
stan_data$question_of_obs <- dcat1$vid
# sample
ordered_categorical_logit_fit_m4 <- ordered_categorical_logit_compiled_m4$sample(
  data = stan_data,
  seed = 123,
  chains = 2,
  parallel_chains = 2,
  iter_warmup = 5e2,
  iter_sampling = 15e2,
  refresh = 500,
  save_warmup = TRUE
)

# save output to RDS
ordered_categorical_logit_fit_m4$save_object(file = file.path(dir.out, paste0(file.prefix,"m4_stan.rds")))
```
```{r ordered_logistic_s2z_load_model, include=FALSE, eval=TRUE, echo=FALSE, tidy=FALSE, cache=TRUE}
ordered_categorical_logit_fit_m4 <- readRDS(file = file.path(dir.out, paste0(file.prefix,'m4_stan.rds')))
```


```{r ordered_logistic_s2z_check mixing convergence, include=FALSE, eval=TRUE, echo=FALSE, tidy=FALSE, cache=TRUE}
#	check convergence and check mixing
#	get 95% credible intervals
tmp <- ordered_categorical_logit_fit_m4$summary(
  variables = c('beta_group','cut_point_1','cut_point_gaps_groups','beta_questions'),
  posterior::default_summary_measures(),
  posterior::default_convergence_measures(),
  extra_quantiles = ~posterior::quantile2(., probs = c(.0275, .975))
)
tmp <- as.data.table(tmp)
tmp
  
# parameter with lowest ess_bulk
worst_var <- tmp$variable[ which.min(tmp$ess_bulk) ]

# extract samples
po <- ordered_categorical_logit_fit_m4$draws(
  variables = c("lp__", worst_var),
  inc_warmup = TRUE,
  format = "draws_array"
  )

# make worst trace plot
p <- bayesplot:::mcmc_trace(po,  
                            pars = c("lp__",worst_var), 
                            n_warmup = 500,
                            facet_args = list(nrow = 2)
                            )
p <- p + theme_bw()
ggsave(file = file.path(dir.out, paste0(file.prefix,'m4_worsttrace.png')), 
       plot = p, 
       h = 10, 
       w = 8
       )

# make intervals plot
po <- ordered_categorical_logit_fit_m4$draws(
  variables = c('beta_group','cut_point_1','cut_point_gaps_groups','beta_questions'),
  inc_warmup = FALSE,
  format = "draws_array"
  )

color_scheme_set("teal")
p <- bayesplot::mcmc_intervals(po, prob = 0.5, prob_outer = 0.95, outer_size = 1, point_size = 2) +
  theme_bw()
ggsave(file = file.path(dir.out, paste0(file.prefix,'m4_intervals.png')), 
       plot = p, 
       h = 7, 
       w = 8, 
       limitsize = FALSE
       )

po <- ordered_categorical_logit_fit_m4$draws(
  variables = c('lp__','ordered_prob[1,1]','ordered_prob[2,1]','ordered_prob[3,1]','ordered_prob[4,1]',
                'beta_group[1]','cut_point_1','cut_point_gaps_groups','beta_questions'),
  inc_warmup = FALSE,
  format = "draws_array"
  )

bayesplot::color_scheme_set('viridisC')
p <- bayesplot::mcmc_pairs(po, 
                           diag_fun = "dens", 
                           off_diag_fun = "hex"
                           ) 
ggsave(file = file.path(dir.out, paste0(file.prefix,'m4_pairsplot.png')), 
       plot = p, 
       h = 50, 
       w = 50, 
       limitsize = FALSE
       )
bayesplot::color_scheme_set('brewer-RdYlBu')


po <- ordered_categorical_logit_fit_m4$draws(
  variables = c('lp__','beta_questions'),
  inc_warmup = FALSE,
  format = "draws_array"
  )

bayesplot::color_scheme_set('viridisC')
p <- bayesplot::mcmc_pairs(po, 
                           diag_fun = "dens", 
                           off_diag_fun = "hex"
                           ) 
ggsave(file = file.path(dir.out, paste0(file.prefix,'m4_pairsplot_questions.png')), 
       plot = p, 
       h = 20, 
       w = 20, 
       limitsize = FALSE
       )
bayesplot::color_scheme_set('brewer-RdYlBu')

# make posterior predictive check

# create median and 95\% credible intervals
po <- ordered_categorical_logit_fit_m4$draws(
  variables = c('ypred'),
  inc_warmup = FALSE,
  format = "draws_df"
  )
po <- as.data.table(po)
po <- data.table::melt(po, id.vars = c('.draw','.chain','.iteration'), value.name = 'ypred')
po[, oid := as.integer(gsub('ypred\\[([0-9]+)\\]','\\1',variable)) ]
pos <- 
  po[,
     list( summary_value = quantile(ypred, prob = c(0.025, 0.25, 0.5, 0.75, 0.975)),
           summary_name = c('q_lower','iqr_lower','median','iqr_upper','q_upper') 
           ),
     by = 'oid'
     ]
pos <- 
  data.table::dcast(pos,
                    oid ~ summary_name, 
                    value.var = 'summary_value'
                    )
pos <- merge(pos, dcat1, by = 'oid')
pos[, IN_PPI := value >= q_lower & value <= q_upper]
pos[, mean(IN_PPI)]

# plot posterior predictive check
p <- ggplot(pos, aes(x = oid, group = oid)) + 
  geom_boxplot( aes( ymin = q_lower,
                     lower = iqr_lower,
                     middle = median,
                     upper = iqr_upper,
                     ymax = q_upper),
                stat = 'identity') +
  geom_point( aes(y = value, colour = IN_PPI ) ) +
  facet_grid(variable ~ arm_label+time_label, scales = 'free') +
  scale_x_discrete() +
  scale_y_continuous() +
  ggsci::scale_color_npg() +
  labs(x = '', 
       y = 'outcome', 
       colour = 'within\n95% posterior\nprediction\ninterval') +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 45,vjust = 1,hjust = 1))
ggsave(file = file.path(dir.out, paste0(file.prefix,'m4_ppcheck.png')), 
       p, 
       w = 12, 
       h = 12)
```



```{r ordered_logistic_s2z_latent_eta, include=FALSE, eval=TRUE, echo=FALSE, tidy=FALSE, cache=TRUE}
# extract model probabilities
po <- ordered_categorical_logit_fit_m4$draws(
  variables = c('beta_group','beta_questions'),
  inc_warmup = FALSE,
  format = "draws_df"
  )
po <- as.data.table(po)
po <- data.table::melt(po, id.vars = c('.draw','.chain','.iteration'), value.name = 'prob')
po[, value := as.integer(gsub('(.*)\\[([0-9])\\]$','\\2',variable)) ]
po[, variable := gsub('(.*)\\[([0-9])\\]$','\\1',variable) ]
tmp <- subset(po, variable == 'beta_group')
tmp <- data.table::dcast(tmp, .draw ~ paste0('beta_group_',value), value.var = 'prob')
po <- merge(subset(po, variable == 'beta_questions'), tmp, by = c('.draw'))
po[, eta_group_1 := beta_group_1 + prob]
po[, eta_group_2 := beta_group_2 + prob]
po[, eta_group_3 := beta_group_3 + prob]
po[, eta_group_4 := beta_group_4 + prob]
setnames(po, 'value', 'vid')
po <- subset(po, select = -c(variable, prob, .chain, .iteration, beta_group_1, beta_group_2, beta_group_3, beta_group_4))
po <- data.table::melt(po, id.vars = c('.draw','vid'), value.name = 'eta')
po[, group := as.integer(gsub('(.*)_([0-9])$','\\2',variable)) ]
po <- subset(po, select = -variable)
tmp <- unique( subset(dcat1, select = c(group, arm, arm_label, time, time_label, variable, vid)) )
po <- merge(po, tmp, by = c('group','vid'))
po <- data.table::dcast(po, .draw + arm + arm_label + time + time_label + group ~ variable, value.var = 'eta')
p <- GGally::ggpairs(po, 
                columns = c('PHQ4_down','PHQ4_interest','PHQ4_nervous','PHQ4_worry'),
                ggplot2::aes(colour = interaction(arm_label, time_label)),
                lower = list(continuous = ggally_density),
                diag = list(continuous = 'blankDiag')
                )
p <- p + scale_fill_npg() + 
  theme_bw()
ggsave(file = file.path(dir.out, paste0(file.prefix,'m4_eta_pairsplot.png')), 
       plot = p, 
       h = 10, 
       w = 10
       )
```

```{r ordered_logistic_s2z_latent_eta_plot, include=FALSE, eval=FALSE, echo=FALSE, tidy=FALSE, fig.align='center', out.width='80%'}
knitr::include_graphics( file.path(dir.out, paste0(file.prefix,'m4_eta_pairsplot.png')) )
```


```{r ordered_logistic_s2z_extract_class_probs, include=FALSE, eval=TRUE, echo=FALSE, tidy=FALSE, cache=TRUE}
# extract model probabilities
po <- ordered_categorical_logit_fit_m4$draws(
  variables = c('ordered_prob'),
  inc_warmup = FALSE,
  format = "draws_df"
  )
po <- as.data.table(po)
setnames(po, colnames(po), gsub('\\]','',gsub('\\[','_',colnames(po))))
po <- data.table::melt(po, id.vars = c('.draw','.chain','.iteration'), value.name = 'prob')
po[, value := as.integer(gsub('ordered_prob_([0-9]),([0-9])','\\1',variable)) ]
po[, group := as.integer(gsub('ordered_prob_([0-9]),([0-9])','\\2',variable)) ]

tmp <- unique( subset(dcat1, select = c(group, arm, arm_label, time, time_label, value, value_label)) )
po <- merge(po, tmp, by = c('group','value'))

pos <- po[, 
          list( summary_value = quantile(prob, prob = c(0.025, 0.25, 0.5, 0.75, 0.975)),
                summary_name = c('q_lower','iqr_lower','median','iqr_upper','q_upper') 
                ), 
          by = c('group','arm','arm_label','time','time_label','value','value_label')
          ]
pos <- data.table::dcast(pos, group + arm + arm_label + time + time_label + value + value_label ~ summary_name, value.var = 'summary_value')

tmp <- dcat1[, list(n = length(pid)), by = c('arm_label','time_label','value_label','value')]
tmp2 <- tmp[, list(total = sum(n)), by = c('arm_label','time_label')]
tmp <- merge(tmp, tmp2, by = c('arm_label','time_label'))
tmp[, p_emp := n/total]
pos <- merge(pos, subset(tmp, select = -c(value, n, total)), by = c('arm_label','time_label','value_label'))

p <- ggplot(data = pos, aes( x = value_label, fill = interaction(arm_label,time_label))) +
  geom_bar(aes( y = p_emp), 
           stat = 'identity', 
           position = position_dodge()) +
  geom_boxplot( aes(ymin = q_lower, lower = iqr_lower, middle = median, upper = iqr_upper, ymax = q_upper), 
                 position = position_dodge(0.9),
                stat = 'identity',
                alpha = 0,
                width = 0.3,
                outliers = FALSE) +
  scale_fill_npg() +
  scale_y_continuous(labels = scales::percent) +
  labs(x = '\nParental mental health\nPHQ4 outcome', y = 'group-level\nposterior probability', fill = '') +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 45,vjust = 1,hjust = 1),
        legend.position = 'bottom'
        )
ggsave(file = file.path(dir.out, paste0(file.prefix,'m4_probs_barplot.png')), 
       plot = p, 
       h = 8, 
       w = 8
       )
```

This is the model fit (in black) relative to the aggregated survey items. The model is flexible enough to capture the proportions for each Likert outcome and each group exactly, and matches the polychoric correlations between item responses (not shown, plot is essentially as for the textbook model above).

```{r ordered_logistic_s2z_plot, include=TRUE, eval=TRUE, echo=FALSE, tidy=FALSE, fig.align='center', out.width='80%'}
knitr::include_graphics( file.path(dir.out, paste0(file.prefix,'m4_probs_barplot.png')) )
```

```{r ordered_logistic_s2z_make_plots_2, include=FALSE, eval=TRUE, echo=FALSE, tidy=FALSE, cache=TRUE}
# extract model probabilities
po <- ordered_categorical_logit_fit_m4$draws(
  variables = c('ordered_prob_by_question'),
  inc_warmup = FALSE,
  format = "draws_df"
  )
po <- as.data.table(po)
setnames(po, colnames(po), gsub('\\]','',gsub('\\[','_',colnames(po))))
po <- data.table::melt(po, id.vars = c('.draw','.chain','.iteration'), value.name = 'prob')
po[, value := as.integer(gsub('ordered_prob_([0-9]),([0-9])','\\1',variable)) ]
po[, group := as.integer(gsub('ordered_prob_([0-9]),([0-9])','\\2',variable)) ]

tmp <- unique( subset(dcat1, select = c(group, arm, arm_label, time, time_label, value, value_label)) )
po <- merge(po, tmp, by = c('group','value'))


# Frequency plot
tmp <- dcat1[, list(n = length(pid)), by = c('arm_label','time_label','variable','value_label', 'value')]
tmp2 <- tmp[, list(total = sum(n)), by = c('arm_label','time_label','variable')]
tmp <- merge(tmp, tmp2, by = c('arm_label','time_label','variable'))
tmp[, p_emp_by_variable := n/total]


p <- ggplot(tmp, aes(x = value_label)) +
  geom_col(aes(fill = variable, y = p_emp_by_variable), position = position_dodge()) +
  geom_point(data = pos, aes(y = median), pch = 18, size = 2, colour = 'black') +
  geom_linerange(data = pos, aes(ymin = q_lower, ymax = q_upper), colour = 'black') +
  scale_fill_bmj() +
  scale_y_continuous(labels = scales::percent) +
  facet_grid(arm_label ~ time_label) +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 45,vjust = 1,hjust = 1),
        legend.position = 'bottom'
        ) +
  labs(x = '', x = 'Likert scale', y = 'proportion of outcomes', fill = '')
ggsave(file = file.path(dir.out, paste0(file.prefix,'m4_frequencies_v2.png')), 
       plot = p, 
       h = 10, 
       w = 8
       )
```

```{r ordered_logistic_s2z_plot_2, include=FALSE, eval=FALSE, echo=FALSE, tidy=FALSE, fig.align='center', out.width='80%'}
knitr::include_graphics( file.path(dir.out, paste0(file.prefix,'m4_frequencies_v2.png')) )
```

Let us also quantify that accounting for polychoric correlations 
indeed improves model fit in terms of Bayes-LOO-ELPD:

```{r PHQ4_s2z_Compare Bayesian LOO approximations v2, include=FALSE, eval=TRUE, echo=FALSE, message=FALSE, tidy=FALSE, cache=TRUE}
poisson_log_likert_fit_m0 <- readRDS(file = file.path(dir.out, 'parentalmh_poisson_likert_m0_stan.rds'))
poisson_log_likert_fit_m1 <- readRDS(file = file.path(dir.out, 'parentalmh_poisson_likert_m1_stan.rds'))
ordered_categorical_logit_fit_m0 <- readRDS(file = file.path(dir.out, 'parentalmh_ordered_categorical_m0_stan.rds'))
ordered_categorical_logit_fit_m0a <- readRDS(file = file.path(dir.out, 'parentalmh_ordered_categorical_m0a_stan.rds'))
ordered_categorical_logit_fit <- readRDS(file = file.path(dir.out, 'parentalmh_ordered_categorical_m1_stan.rds'))
ordered_categorical_logit_fit_m4a <- readRDS(file = file.path(dir.out, 'parentalmh_ordered_categorical_m4a_stan.rds'))
ordered_categorical_logit_fit_m4 <- readRDS(file = file.path(dir.out, 'parentalmh_ordered_categorical_m4_stan.rds'))

poisson_log_likert_fit_m0_loo <- poisson_log_likert_fit_m0$loo()
poisson_log_likert_fit_m1_loo <- poisson_log_likert_fit_m1$loo()
ordered_categorical_logit_fit_m0_loo <- ordered_categorical_logit_fit_m0$loo()
ordered_categorical_logit_fit_m0a_loo <- ordered_categorical_logit_fit_m0a$loo()
ordered_categorical_logit_fit_loo <- ordered_categorical_logit_fit$loo()
ordered_categorical_logit_fit_m4a_loo <- ordered_categorical_logit_fit_m4a$loo()
ordered_categorical_logit_fit_m4_loo <- ordered_categorical_logit_fit_m4$loo()
```

```{r PHQ4_s2z_Compare Bayesian LOO approximations table v2, include=TRUE, eval=TRUE, echo=TRUE, tidy=FALSE, cache=TRUE}
# Model 1 - Poisson no participant effects
# Model 2 - Poisson with participant random effects
# Model 3 - Textbook Ordered Cat, no participant effects
# Model 4 - Multi-group Ordered Cat, no participant effects
# Model 5 - Multi-group Ordered Cat, with participant random effects
# Model 6 - Textbook Ordered Cat with response correlations, no participant effects
# Model 7 - Multi-group Ordered Cat with response correlations, no participant effects
comp <- loo::loo_compare(poisson_log_likert_fit_m0_loo,
                         poisson_log_likert_fit_m1_loo,
                         ordered_categorical_logit_fit_m0a_loo,
                         ordered_categorical_logit_fit_m0_loo, 
                         ordered_categorical_logit_fit_loo,
                         ordered_categorical_logit_fit_m4a_loo,
                         ordered_categorical_logit_fit_m4_loo
                         )
print(comp, simplify = FALSE)
```

# PHQ4 - Ordered Cat Latent Factor model

## Textbook model

I can summarize all above outcomes with a single sentence: each improvement has brought
us closer to a standard latent factor model. So let us investigate a standard latent factor model
too. I will start with the textbook version, one cutpoint vector across groups. We don't expect 
that this model fits well, but it is important to understand if we could get satisfactory 
outputs just with BRMS.

\begin{align*}
& \text{Prob}(Y_{ij} = k) = \text{Ordered-Logistic}(k; \eta_{ij}, c) \\
& \eta_{ij}= \gamma_j + \lambda_j * (\beta_{G(i)} + \beta^{\text{RE}}_i)\\
& \gamma \sim \text{S2Z-Normal}(0,1)\\
& \lambda_1 = 1\\
& \lambda_{2:J} \sim \text{Normal}(0,1)\\
& \beta \sim \text{S2Z-Normal}(0, 1)\\
& \beta^{\text{RE}} \sim \text{S2Z-Normal}(0,1)\\
& c = [c_{1}, c_{1} + \text{cumsum}(\delta_{1:{K-2}})] \\
& c_{1} \sim \text{Normal}(0 , 3.5^2)\\
& \delta_{1:{K-2}} \sim \text{Normal}(0 , 2.5^2)\\
\end{align*}

The key points of the latent factor model are: (1) we consider each participant is 
now associated with its own latent factor $\beta_{G(i)} + \beta^{\text{RE}}_i$. These
latent factors are modeled through participant-level random effects around a group mean, as
further above. (2) We also retain the baseline item effects $\gamma_j$ to capture correlations
across items as above. These item effects also enable us to explain data where latent factors 
are not needed at all to explain the data. (3) We only added the factor loadings $\lambda_j$. 
These essentially provide the model with the ability to link the categorical probabilities
to the latent factors through linear functions that also have a slope and are not just 
horizontal. So this is pretty much the minimum complexity that one would like to 
consider. The first loading is set to 1 to ensure that the loadings and factors are 
identifiable.

Here is the Stan code:

```{r ordered_logistic_latent_factor-a_stan_code, include=TRUE, eval=TRUE, echo=TRUE, tidy=FALSE, cache=TRUE}
ordered_categorical_logit_txt_m5a <- "
data
{
  int<lower=1> N; // number of observations
  int<lower=3> K; // number of categories
  int<lower=2> Q; // number of questions
  int<lower=2> P; // number of groups
  int<lower=1> U; // number of units
  array [N] int<lower=1, upper=K> y; // observations
  array [N] int<lower=1,upper=P> group_of_obs;
  array [N] int<lower=1,upper=Q> question_of_obs;
  array [N] int<lower=1,upper=U> unit_of_obs;
}
transformed data
{
    real s2z_sd_groups;
    real s2z_sd_unit;
    real s2z_sd_questions;
    s2z_sd_groups = inv(sqrt(1. - inv(P)));
    s2z_sd_unit = inv(sqrt(1. - inv(U)));
    s2z_sd_questions = inv(sqrt(1. - inv(Q)));
}
parameters
{
  sum_to_zero_vector[P] factor_group;
  sum_to_zero_vector[Q] beta_questions;
  vector<lower=0>[Q-1] loadings_questions_m1;
  sum_to_zero_vector[U] beta_unit;
  real<lower=0> beta_unit_sd;
  
  // the cutpoints in logit space, using the 'ordered' variable type
  real cut_point_1;
  vector<lower=0>[K - 2] cut_point_gaps_groups;
}
transformed parameters
{
  ordered[K-1] cut_points;
  vector<lower=0>[Q] loadings_questions;
  
  cut_points = 
      append_row(
        cut_point_1, 
        rep_vector(cut_point_1, K-2) + cumulative_sum(cut_point_gaps_groups)
        );
  
  loadings_questions = append_row(1.0, loadings_questions_m1);
}
model
{
  // likelihood, cannot be vectorized
  for (n in 1:N) {
    y[n] ~ ordered_logistic(
      beta_questions[question_of_obs[n]] + 
        loadings_questions[question_of_obs[n]] *
        (
          factor_group[group_of_obs[n]] + 
          beta_unit_sd * beta_unit[unit_of_obs[n]]
        ), 
      cut_points);
  }
  
  // priors
  factor_group ~ normal(0, s2z_sd_groups); 
  beta_unit ~ normal(0, s2z_sd_unit); 
  beta_questions ~ normal(0, s2z_sd_questions); 
  loadings_questions_m1 ~ lognormal(0,1);
  beta_unit_sd ~ exponential(2); 
  cut_point_1 ~ normal(0, 3.5); 
  cut_point_gaps_groups ~ normal(0, 2.5); 
}
generated quantities
{
  array [P,Q] real mean_eta;
  array [K,N] real<lower=0, upper=1> ordered_prob_by_obs;
  array [N] int<lower=0> ypred;
  array [N] real log_lik;
  vector[U] beta_unit_re;
  
  for (q in 1:Q) 
  {
    for (p in 1:P) 
    {
      mean_eta[p,q] = beta_questions[q] + loadings_questions[q] * factor_group[p];
    }
  }
  
  {
    real tmp_real;
    for (n in 1:N) 
    {
        tmp_real = beta_questions[question_of_obs[n]] + 
                    loadings_questions[question_of_obs[n]] *
                    (
                      factor_group[group_of_obs[n]] + 
                      beta_unit_sd * beta_unit[unit_of_obs[n]]
                    );
        ordered_prob_by_obs[1,n]       = 1 - inv_logit( tmp_real - cut_points[1] );
        ordered_prob_by_obs[2:(K-1),n] = to_array_1d( inv_logit( tmp_real - cut_points[1:(K-2)] ) 
                                                              - 
                                                              inv_logit( tmp_real - cut_points[2:(K-1)] )
                                                              );
        ordered_prob_by_obs[K,n]       = inv_logit( tmp_real - cut_points[K-1] );
    }
  }
  
  for( n in 1:N ) 
  {
    ypred[n] = ordered_logistic_rng( 
                beta_questions[question_of_obs[n]] + 
                  loadings_questions[question_of_obs[n]] *
                  (
                    factor_group[group_of_obs[n]] + 
                    beta_unit_sd * beta_unit[unit_of_obs[n]]
                  ), 
                cut_points
                ); 
    log_lik[n] =  ordered_logistic_lpmf( y[n] |
                    beta_questions[question_of_obs[n]] + 
                      loadings_questions[question_of_obs[n]] *
                      (
                        factor_group[group_of_obs[n]] + 
                        beta_unit_sd * beta_unit[unit_of_obs[n]]
                      ),
                    cut_points);
  }
  
  beta_unit_re = beta_unit_sd * beta_unit;
}
"

# compile the model
ordered_categorical_logit_m5a_filename <- cmdstanr::write_stan_file(
  gsub('\t',' ', ordered_categorical_logit_txt_m5a),
  dir = dir.out,
  basename = NULL,
  force_overwrite = FALSE,
  hash_salt = ""
)

# compile Stan model
ordered_categorical_logit_compiled_m5a <- cmdstanr::cmdstan_model(ordered_categorical_logit_m5a_filename)
```

Let's fit the model!

```{r ordered_logistic_latent_factor-a_run_model, include=TRUE, eval=TRUE, echo=TRUE, message=FALSE, tidy=FALSE, cache=TRUE}
file.prefix <- 'parentalmh_ordered_categorical_'

# define data in format needed for model specification
stan_data <- list()
stan_data$N <- nrow(dcat1)
stan_data$P <- max(dcat1$group)
stan_data$Q <- max(dcat1$vid)
stan_data$U <- max(dcat1$pid)
stan_data$K <- length(unique(dcat1$value))
stan_data$y <- dcat1$value
stan_data$group_of_obs <- dcat1$group
stan_data$question_of_obs <- dcat1$vid
stan_data$unit_of_obs <- dcat1$pid

# sample
ordered_categorical_logit_fit_m5a <- ordered_categorical_logit_compiled_m5a$sample(
  data = stan_data,
  seed = 123,
  chains = 2,
  parallel_chains = 2,
  iter_warmup = 5e2,
  iter_sampling = 15e2,
  refresh = 500,
  save_warmup = TRUE
)

# save output to RDS
ordered_categorical_logit_fit_m5a$save_object(file = file.path(dir.out, paste0(file.prefix,"m5a_stan.rds")))
```

```{r ordered_logistic_latent_factor-a_load_model, include=FALSE, eval=TRUE, echo=FALSE, tidy=FALSE, cache=TRUE}
ordered_categorical_logit_fit_m5 <- readRDS(file = file.path(dir.out, paste0(file.prefix,'m5a_stan.rds')))
```

```{r ordered_logistic_latent_factor-a_check_mixing_convergence, include=FALSE, eval=TRUE, echo=FALSE, tidy=FALSE, cache=TRUE}
#	check convergence and check mixing
#	get 95% credible intervals
tmp <- ordered_categorical_logit_fit_m5a$summary(
  variables = c('factor_group','beta_unit','beta_unit_sd','beta_questions',
                'loadings_questions_m1','cut_point_1','cut_point_gaps_groups'),
  posterior::default_summary_measures(),
  posterior::default_convergence_measures(),
  extra_quantiles = ~posterior::quantile2(., probs = c(.0275, .975))
)
tmp <- as.data.table(tmp)
tmp
  
# parameter with lowest ess_bulk
worst_var <- tmp$variable[ which.min(tmp$ess_bulk) ]

# extract samples
po <- ordered_categorical_logit_fit_m5a$draws(
  variables = c("lp__", worst_var),
  inc_warmup = TRUE,
  format = "draws_array"
  )

# make worst trace plot
p <- bayesplot:::mcmc_trace(po,  
                            pars = c("lp__",worst_var), 
                            n_warmup = 500,
                            facet_args = list(nrow = 2)
                            )
p <- p + theme_bw()
ggsave(file = file.path(dir.out, paste0(file.prefix,'m5a_worsttrace.png')), 
       plot = p, 
       h = 10, 
       w = 8
       )

# make intervals plot
po <- ordered_categorical_logit_fit_m5a$draws(
  variables = c('factor_group','beta_questions','loadings_questions',
                'cut_point_1','cut_point_gaps_groups',
                'beta_unit_sd','beta_unit'),
  inc_warmup = FALSE,
  format = "draws_array"
  )

color_scheme_set("teal")
p <- bayesplot::mcmc_intervals(po, prob = 0.5, prob_outer = 0.95, outer_size = 1, point_size = 2) +
  theme_bw()
ggsave(file = file.path(dir.out, paste0(file.prefix,'m5a_intervals.png')), 
       plot = p, 
       h = 40, 
       w = 8, 
       limitsize = FALSE
       )

# make pairs plot
po <- ordered_categorical_logit_fit_m5a$draws(
  variables = c('factor_group','cut_point_1','cut_point_gaps_groups'),
  inc_warmup = FALSE,
  format = "draws_array"
  )

bayesplot::color_scheme_set('viridisC')
p <- bayesplot::mcmc_pairs(po, 
                           diag_fun = "dens", 
                           off_diag_fun = "hex"
                           ) 
ggsave(file = file.path(dir.out, paste0(file.prefix,'m5a_pairsplot.png')), 
       plot = p, 
       h = 30, 
       w = 30, 
       limitsize = FALSE
       )
bayesplot::color_scheme_set('brewer-RdYlBu')

# make posterior predictive check

# create median and 95\% credible intervals
po <- ordered_categorical_logit_fit_m5a$draws(
  variables = c('ypred'),
  inc_warmup = FALSE,
  format = "draws_df"
  )
po <- as.data.table(po)
po <- data.table::melt(po, id.vars = c('.draw','.chain','.iteration'), value.name = 'ypred')
po[, oid := as.integer(gsub('ypred\\[([0-9]+)\\]','\\1',variable)) ]
pos <- 
  po[,
     list( summary_value = quantile(ypred, prob = c(0.025, 0.25, 0.5, 0.75, 0.975)),
           summary_name = c('q_lower','iqr_lower','median','iqr_upper','q_upper') 
           ),
     by = 'oid'
     ]
pos <- 
  data.table::dcast(pos,
                    oid ~ summary_name, 
                    value.var = 'summary_value'
                    )
pos <- merge(pos, dcat1, by = 'oid')
pos[, IN_PPI := value >= q_lower & value <= q_upper]

# plot posterior predictive check
p <- ggplot(pos, aes(x = oid, group = oid)) + 
  geom_boxplot( aes( ymin = q_lower,
                     lower = iqr_lower,
                     middle = median,
                     upper = iqr_upper,
                     ymax = q_upper),
                stat = 'identity') +
  geom_point( aes(y = value, colour = IN_PPI ) ) +
  facet_grid(variable ~ arm_label+time_label, scales = 'free') +
  scale_x_discrete() +
  scale_y_continuous() +
  ggsci::scale_color_npg() +
  labs(x = '', 
       y = 'outcome', 
       colour = 'within\n95% posterior\nprediction\ninterval') +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 45,vjust = 1,hjust = 1))
ggsave(file = file.path(dir.out, paste0(file.prefix,'m5a_ppcheck.png')), 
       p, 
       w = 12, 
       h = 12)
```

```{r ordered_logistic_latent_factor-a_extract_participant_effects, include=FALSE, eval=TRUE, echo=FALSE, tidy=FALSE, cache=TRUE}
# extract participant effects
po <- ordered_categorical_logit_fit_m5a$draws(
  variables = c('beta_unit_sd','beta_unit'),
  inc_warmup = FALSE,
  format = "draws_df"
  )
po <- as.data.table(po)
po <- data.table::melt(po, id.vars = c('.draw','.chain','.iteration'))
tmp <- subset(po, variable == 'beta_unit_sd')
tmp <- data.table::dcast(tmp, .draw ~ variable, value.var = 'value')
po <- subset(po, variable != 'beta_unit_sd')
po <- merge(po, tmp, by = '.draw')
po[, pid := as.integer(gsub('beta_unit\\[([0-9]+)\\]','\\1',variable)) ]
po[, value := value * beta_unit_sd]

pos <- po[, 
          list( summary_value = quantile(value, prob = c(0.025, 0.25, 0.5, 0.75, 0.975)),
                summary_name = c('q_lower','iqr_lower','median','iqr_upper','q_upper') 
                ), 
          by = c('pid')
          ]
pos <- data.table::dcast(pos, pid ~ summary_name, value.var = 'summary_value')
tmp <- unique(subset(dcat1, select = c(pid, arm_label, time_label)))
pos <- merge(pos, tmp, by = c('pid'))
tmp <- dcat1[, list(avg_value = mean(value)), by = 'pid']
tmp[, avg_value_jitter := avg_value + runif(nrow(tmp), min = -1/8, max = 1/8)]
pos <- merge(pos, tmp, by = 'pid')

tmp <- pos[, list(avg_effect = mean(median)), by = c('arm_label','time_label')]

p <- ggplot(data = pos, aes( x = avg_value_jitter)) +
  geom_hline(data = tmp, aes( yintercept = avg_effect), colour = 'grey50') +
  geom_linerange(aes( ymin = q_lower, ymax = q_upper), 
           stat = 'identity') +
  geom_point(aes( y = median)) +
  labs(x = '\naverage response of participant', y = 'participant effect') +
  theme_bw() +
  facet_grid(arm_label ~ time_label, scales = 'free') +
  theme(axis.text.x = element_text(angle = 45,vjust = 1,hjust = 1),
        legend.position = 'bottom'
        )
ggsave(file = file.path(dir.out, paste0(file.prefix,'m5a_participant_effects.png')), 
       plot = p, 
       h = 8, 
       w = 12
       )
```

Here are the participant effects around the group mean latent factors, showing the same behaviour as before. This demonstrates that (1) the added baseline response effects are not able to explain the data without latent traits, and (2) there is substantial individual-level heterogeneity around the group mean latent factors.

```{r ordered_logistic_latent_factor-a_participant_effect_plot, include=TRUE, eval=TRUE, echo=FALSE, tidy=FALSE, fig.align='center', out.width='80%'}
knitr::include_graphics( file.path(dir.out, paste0(file.prefix,'m5a_participant_effects.png')) )
```

```{r ordered_logistic_latent_factor-a_latent_eta, include=FALSE, eval=TRUE, echo=FALSE, tidy=FALSE, cache=TRUE}
# extract model probabilities
po <- ordered_categorical_logit_fit_m5a$draws(
  variables = c('mean_eta'),
  inc_warmup = FALSE,
  format = "draws_df"
  )
po <- as.data.table(po)
po <- data.table::melt(po, id.vars = c('.draw','.chain','.iteration'), value.name = 'mean_eta')
po[, group := as.integer(gsub('(.*)\\[([0-9]),([0-9])\\]$','\\2',variable)) ]
po[, vid := as.integer(gsub('(.*)\\[([0-9]),([0-9])\\]$','\\3',variable)) ]
set(po, NULL, 'variable', NULL)
tmp <- unique( subset(dcat1, select = c(group, arm, arm_label, time, time_label, variable, vid)) )
po <- merge(po, tmp, by = c('group','vid'))
po <- data.table::dcast(po, .draw + arm + arm_label + time + time_label + group ~ variable, value.var = 'mean_eta')
p <- GGally::ggpairs(po, 
                columns = c('PHQ4_down','PHQ4_interest','PHQ4_nervous','PHQ4_worry'),
                ggplot2::aes(colour = interaction(arm_label, time_label)),
                lower = list(continuous = ggally_density),
                diag = list(continuous = 'blankDiag')
                )
p <- p + scale_fill_npg() + 
  theme_bw()
ggsave(file = file.path(dir.out, paste0(file.prefix,'m5a_eta_pairsplot.png')), 
       plot = p, 
       h = 10, 
       w = 10
       )
```

Here is the polychoric correlation plot. We ignore the $\beta_i^{\text{RND}}$ latent factor
random effects, and can then plot the group-mean multivariate structure of the joint probabilities in logit-space:

```{r ordered_logistic_latent_factor-a_latent_eta_plot, include=TRUE, eval=TRUE, echo=FALSE, tidy=FALSE, fig.align='center', out.width='80%'}
knitr::include_graphics( file.path(dir.out, paste0(file.prefix,'m5a_eta_pairsplot.png')) )
```

```{r ordered_logistic_latent_factor-a_extract_class_probs_by_n, include=FALSE, eval=TRUE, echo=FALSE, tidy=FALSE, cache=TRUE}
po <- ordered_categorical_logit_fit_m5a$draws(
  variables = c('ordered_prob_by_obs'),
  inc_warmup = FALSE,
  format = "draws_df"
  )
po <- as.data.table(po)
po <- data.table::melt(po, id.vars = c('.draw','.chain','.iteration'), value.name = 'prob')
po[, value := as.integer(gsub('ordered_prob_by_obs\\[([0-9]),([0-9]+)\\]','\\1',variable)) ]
po[, oid := as.integer(gsub('ordered_prob_by_obs\\[([0-9]),([0-9]+)\\]','\\2',variable)) ]
set(po, NULL, 'variable', NULL)
tmp <- unique(subset(dcat1, select = c(oid, pid, vid, group )))
po <- merge(po, tmp, by = c('oid'))
po <- po[, list(prob = mean(prob)), by = c('.draw','group','value')]

pos <- po[, 
          list( summary_value = quantile(prob, prob = c(0.025, 0.25, 0.5, 0.75, 0.975)),
                summary_name = c('q_lower','iqr_lower','median','iqr_upper','q_upper') 
                ), 
          by = c('group','value')
          ]
pos <- data.table::dcast(pos, group + value ~ summary_name, value.var = 'summary_value')

tmp <- unique( subset(dcat1, select = c(group, arm, arm_label, time, time_label, value, value_label)) )
pos <- merge(pos, tmp, by = c('group','value'))

tmp <- dcat1[, list(n = length(pid)), by = c('arm_label','time_label','value_label','value')]
tmp2 <- tmp[, list(total = sum(n)), by = c('arm_label','time_label')]
tmp <- merge(tmp, tmp2, by = c('arm_label','time_label'))
tmp[, p_emp := n/total]
pos <- merge(pos, subset(tmp, select = -c(value, n, total)), by = c('arm_label','time_label','value_label'))

p <- ggplot(data = pos, aes( x = value_label, fill = interaction(arm_label,time_label))) +
  geom_bar(aes( y = p_emp), 
           stat = 'identity', 
           position = position_dodge()) +
  geom_point(aes( y = median),
             position = position_dodge(0.9), colour = 'black') +
  geom_boxplot( aes(ymin = q_lower, lower = iqr_lower, middle = median, upper = iqr_upper, ymax = q_upper), 
                position = position_dodge(0.9),
                stat = 'identity',
                alpha = 0,
                width = 0.3) +
  scale_fill_npg() +
  scale_y_continuous(labels = scales::percent) +
  labs(x = '\nParental mental health\nPHQ4 outcome', y = 'group-level\nposterior probability', fill = '') +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 45,vjust = 1,hjust = 1),
        legend.position = 'bottom'
        )
ggsave(file = file.path(dir.out, paste0(file.prefix,'m5a_probs_barplot.png')), 
       plot = p, 
       h = 8, 
       w = 8
       )
```

Here is the model fit (in black) relative to the aggregated survey items. As before, the fit is not great relative to our primary target statistics:

```{r ordered_logistic_latent_factor-a_plot, include=TRUE, eval=TRUE, echo=FALSE, tidy=FALSE, fig.align='center', out.width='80%'}
knitr::include_graphics( file.path(dir.out, paste0(file.prefix,'m5a_probs_barplot.png')) )
```


```{r ordered_logistic_latent_factor-a_extract_class_probs_by_question, include=FALSE, eval=TRUE, echo=FALSE, tidy=FALSE, cache=TRUE}
po <- ordered_categorical_logit_fit_m5a$draws(
  variables = c('ordered_prob_by_obs'),
  inc_warmup = FALSE,
  format = "draws_df"
  )
po <- as.data.table(po)
po <- data.table::melt(po, id.vars = c('.draw','.chain','.iteration'), value.name = 'prob')
po[, value := as.integer(gsub('ordered_prob_by_obs\\[([0-9]),([0-9]+)\\]','\\1',variable)) ]
po[, oid := as.integer(gsub('ordered_prob_by_obs\\[([0-9]),([0-9]+)\\]','\\2',variable)) ]
set(po, NULL, 'variable', NULL)
tmp <- unique(subset(dcat1, select = c(oid, pid, vid, group )))
po <- merge(po, tmp, by = c('oid'))
po <- po[, list(prob = mean(prob)), by = c('.draw','group','vid','value')]

pos <- po[, 
          list( summary_value = quantile(prob, prob = c(0.025, 0.25, 0.5, 0.75, 0.975)),
                summary_name = c('q_lower','iqr_lower','median','iqr_upper','q_upper') 
                ), 
          by = c('group','vid','value')
          ]
pos <- data.table::dcast(pos, group + vid + value ~ summary_name, value.var = 'summary_value')

tmp <- unique( subset(dcat1, select = c(group, arm, arm_label, time, time_label, variable, vid, value, value_label)) )
pos <- merge(pos, tmp, by = c('group','vid','value'))

tmp <- dcat1[, list(n = length(pid)), by = c('arm_label','time_label','variable','value_label','value')]
tmp2 <- tmp[, list(total = sum(n)), by = c('arm_label','time_label','variable')]
tmp <- merge(tmp, tmp2, by = c('arm_label','time_label','variable'))
tmp[, p_emp := n/total]
pos <- merge(pos, subset(tmp, select = -c(value, n, total)), by = c('arm_label','time_label','variable','value_label'))

p <- ggplot(pos, aes(x = value_label, group = interaction(variable, value_label))) +
  geom_col(aes(fill = variable, y = p_emp), 
           position = position_dodge(0.9)
           ) +
  geom_boxplot( aes(ymin = q_lower, lower = iqr_lower, middle = median, upper = iqr_upper, ymax = q_upper), 
                position = position_dodge(0.9),
                stat = 'identity',
                alpha = 0,
                width = 0.3) +
  scale_fill_bmj() +
  scale_y_continuous(labels = scales::percent) +
  facet_grid(arm_label ~ time_label) +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 45,vjust = 1,hjust = 1),
        legend.position = 'bottom'
        ) +
  labs(x = '', x = 'Likert scale', y = 'proportion of outcomes', fill = '')
ggsave(file = file.path(dir.out, paste0(file.prefix,'m5a_frequencies_v2.png')), 
       plot = p, 
       h = 10, 
       w = 8
       )
```

```{r ordered_logistic_latent_factor-a_extract_class_probs_by_question_plot, include=FALSE, eval=TRUE, echo=FALSE, tidy=FALSE, fig.align='center', out.width='80%'}
knitr::include_graphics( file.path(dir.out, paste0(file.prefix,'m5a_frequencies_v2.png')) )
```


## First multi-group model

Next up, I expanded the 1D latent factor model to include cutpoints for each group, 
as in the above multi-group models.

\begin{align*}
& \text{Prob}(Y_{ij} = k) = \text{Ordered-Logistic}(k; \eta_{ij}, c_{G(i)}) \\
& \eta_{ij}= \gamma_j + \lambda_j * (\beta_{G(i)} + \beta^{\text{RE}}_i)\\
& \gamma \sim \text{S2Z-Normal}(0,1)\\
& \lambda_1 = 1\\
& \lambda_{2:J} \sim \text{Normal}(0,1)\\
& \beta \sim \text{S2Z-Normal}(0, 1)\\
& \beta^{\text{RE}} \sim \text{S2Z-Normal}(0,1)\\
& c_g = [c_{g1}, c_{g1} + \text{cumsum}(\delta_{g,1:{K-2}})] \\
& c_{g1} \sim \text{Normal}(0 , 3.5^2)\\
& \delta_{g,1:{K-2}} \sim \text{Normal}(0 , 2.5^2)\\
\end{align*}

Here is the Stan code:

```{r ordered_logistic_latent_factor_stan_code, include=TRUE, eval=TRUE, echo=TRUE, tidy=FALSE, cache=TRUE}
ordered_categorical_logit_txt_m5 <- "
data
{
  int<lower=1> N; // number of observations
  int<lower=3> K; // number of categories
  int<lower=2> Q; // number of questions
  int<lower=2> P; // number of groups
  int<lower=1> U; // number of units
  array [N] int<lower=1, upper=K> y; // observations
  array [N] int<lower=1,upper=P> group_of_obs;
  array [N] int<lower=1,upper=Q> question_of_obs;
  array [N] int<lower=1,upper=U> unit_of_obs;
}
transformed data
{
    real s2z_sd_groups;
    real s2z_sd_unit;
    real s2z_sd_questions;
    s2z_sd_groups = inv(sqrt(1. - inv(P)));
    s2z_sd_unit = inv(sqrt(1. - inv(U)));
    s2z_sd_questions = inv(sqrt(1. - inv(Q)));
}
parameters
{
  sum_to_zero_vector[P] factor_group;
  sum_to_zero_vector[Q] beta_questions;
  vector<lower=0>[Q-1] loadings_questions_m1;
  sum_to_zero_vector[U] beta_unit;
  real<lower=0> beta_unit_sd;
  
  // the cutpoints in logit space, using the 'ordered' variable type
  real cut_point_1;
  matrix<lower=0>[K - 2,P] cut_point_gaps_groups;
}
transformed parameters
{
  array[P] ordered[K-1] cut_points;
  vector<lower=0>[Q] loadings_questions;
  
  for (p in 1:P)
  {
    cut_points[p] = 
      append_row(
        rep_vector(cut_point_1, 1), 
        rep_vector(cut_point_1, K-2) + cumulative_sum(cut_point_gaps_groups[,p])
        );
  }
  
  loadings_questions = append_row(1.0, loadings_questions_m1);
}
model
{
  // likelihood, cannot be vectorized
  for (n in 1:N) {
    y[n] ~ ordered_logistic(
      beta_questions[question_of_obs[n]] + 
        loadings_questions[question_of_obs[n]] *
        (
          factor_group[group_of_obs[n]] + 
          beta_unit_sd * beta_unit[unit_of_obs[n]]
        ), 
      cut_points[group_of_obs[n]]);
  }
  
  // priors
  factor_group ~ normal(0, s2z_sd_groups); 
  beta_unit ~ normal(0, s2z_sd_unit); 
  beta_questions ~ normal(0, s2z_sd_questions); 
  loadings_questions_m1 ~ lognormal(0,1);
  beta_unit_sd ~ exponential(2); // a +1 increase corresponds to ~ doubling of prob, 
                                 // although there is an issue in that for small probs, the same increment provides larger multipliers
                                 // gtools::inv.logit(c(-3,-2)) = 0.04742587 0.11920292
                                 // gtools::inv.logit(c(-1,0)) = 0.2689414 0.5000000
  cut_point_1 ~ normal(0, 3.5); // gtools::inv.logit(2*3.5) = 0.999
  to_vector(cut_point_gaps_groups) ~ normal(0, 2.5); // gtools::inv.logit(-3 + 2 = 1) = 0.26, empirically we know the largest next prob is ~ +0.20
}
generated quantities
{
  array [P,Q] real mean_eta;
  array [K,N] real<lower=0, upper=1> ordered_prob_by_obs;
  array [N] int<lower=0> ypred;
  array [N] real log_lik;
  vector[U] beta_unit_re;
  
  for (q in 1:Q) 
  {
    for (p in 1:P) 
    {
      mean_eta[p,q] = beta_questions[q] + loadings_questions[q] * factor_group[p];
    }
  }
  
  {
    real tmp_real;
    for (n in 1:N) 
    {
        tmp_real = beta_questions[question_of_obs[n]] + 
                    loadings_questions[question_of_obs[n]] *
                    (
                      factor_group[group_of_obs[n]] + 
                      beta_unit_sd * beta_unit[unit_of_obs[n]]
                    );
        ordered_prob_by_obs[1,n]       = 1 - inv_logit( tmp_real - cut_points[group_of_obs[n]][1] );
        ordered_prob_by_obs[2:(K-1),n] = to_array_1d( inv_logit( tmp_real - cut_points[group_of_obs[n]][1:(K-2)] ) 
                                                              - 
                                                              inv_logit( tmp_real - cut_points[group_of_obs[n]][2:(K-1)] )
                                                              );
        ordered_prob_by_obs[K,n]       = inv_logit( tmp_real - cut_points[group_of_obs[n]][K-1] );
    }
  }
  
  for( n in 1:N ) 
  {
    ypred[n] = ordered_logistic_rng( 
                beta_questions[question_of_obs[n]] + 
                  loadings_questions[question_of_obs[n]] *
                  (
                    factor_group[group_of_obs[n]] + 
                    beta_unit_sd * beta_unit[unit_of_obs[n]]
                  ), 
                cut_points[group_of_obs[n]]
                ); 
    log_lik[n] =  ordered_logistic_lpmf( y[n] |
                    beta_questions[question_of_obs[n]] + 
                      loadings_questions[question_of_obs[n]] *
                      (
                        factor_group[group_of_obs[n]] + 
                        beta_unit_sd * beta_unit[unit_of_obs[n]]
                      ),
                    cut_points[group_of_obs[n]]);
  }
  
  beta_unit_re = beta_unit_sd * beta_unit;
}
"

# compile the model
ordered_categorical_logit_m5_filename <- cmdstanr::write_stan_file(
  gsub('\t',' ', ordered_categorical_logit_txt_m5),
  dir = dir.out,
  basename = NULL,
  force_overwrite = FALSE,
  hash_salt = ""
)

# compile Stan model
ordered_categorical_logit_compiled_m5 <- cmdstanr::cmdstan_model(ordered_categorical_logit_m5_filename)
```

Let's fit the model!

```{r ordered_logistic_latent_factor_run_model, include=TRUE, eval=TRUE, echo=TRUE, message=FALSE, tidy=FALSE, cache=TRUE}
file.prefix <- 'parentalmh_ordered_categorical_'

# define data in format needed for model specification
stan_data <- list()
stan_data$N <- nrow(dcat1)
stan_data$P <- max(dcat1$group)
stan_data$Q <- max(dcat1$vid)
stan_data$U <- max(dcat1$pid)
stan_data$K <- length(unique(dcat1$value))
stan_data$y <- dcat1$value
stan_data$group_of_obs <- dcat1$group
stan_data$question_of_obs <- dcat1$vid
stan_data$unit_of_obs <- dcat1$pid

# sample
ordered_categorical_logit_fit_m5 <- ordered_categorical_logit_compiled_m5$sample(
  data = stan_data,
  seed = 123,
  chains = 2,
  parallel_chains = 2,
  iter_warmup = 5e2,
  iter_sampling = 15e2,
  refresh = 500,
  save_warmup = TRUE
)

# save output to RDS
ordered_categorical_logit_fit_m5$save_object(file = file.path(dir.out, paste0(file.prefix,"m5_stan.rds")))
```

```{r ordered_logistic_latent_factor_load_model, include=FALSE, eval=TRUE, echo=FALSE, tidy=FALSE, cache=TRUE}
ordered_categorical_logit_fit_m5 <- readRDS(file = file.path(dir.out, paste0(file.prefix,'m5_stan.rds')))
```

```{r ordered_logistic_latent_factor_check_mixing_convergence, include=FALSE, eval=TRUE, echo=FALSE, tidy=FALSE, cache=TRUE}
#	check convergence and check mixing
#	get 95% credible intervals
tmp <- ordered_categorical_logit_fit_m5$summary(
  variables = c('factor_group','beta_unit','beta_unit_sd','beta_questions',
                'loadings_questions_m1','cut_point_1','cut_point_gaps_groups'),
  posterior::default_summary_measures(),
  posterior::default_convergence_measures(),
  extra_quantiles = ~posterior::quantile2(., probs = c(.0275, .975))
)
tmp <- as.data.table(tmp)
tmp
  
# parameter with lowest ess_bulk
worst_var <- tmp$variable[ which.min(tmp$ess_bulk) ]

# extract samples
po <- ordered_categorical_logit_fit_m5$draws(
  variables = c("lp__", worst_var),
  inc_warmup = TRUE,
  format = "draws_array"
  )

# make worst trace plot
p <- bayesplot:::mcmc_trace(po,  
                            pars = c("lp__",worst_var), 
                            n_warmup = 500,
                            facet_args = list(nrow = 2)
                            )
p <- p + theme_bw()
ggsave(file = file.path(dir.out, paste0(file.prefix,'m5_worsttrace.png')), 
       plot = p, 
       h = 10, 
       w = 8
       )

# make intervals plot
po <- ordered_categorical_logit_fit_m5$draws(
  variables = c('factor_group','beta_questions','loadings_questions',
                'cut_point_1','cut_point_gaps_groups',
                'beta_unit_sd','beta_unit'),
  inc_warmup = FALSE,
  format = "draws_array"
  )

color_scheme_set("teal")
p <- bayesplot::mcmc_intervals(po, prob = 0.5, prob_outer = 0.95, outer_size = 1, point_size = 2) +
  theme_bw()
ggsave(file = file.path(dir.out, paste0(file.prefix,'m5_intervals.png')), 
       plot = p, 
       h = 40, 
       w = 8, 
       limitsize = FALSE
       )

# make pairs plot
po <- ordered_categorical_logit_fit_m5$draws(
  variables = c('factor_group','cut_point_1','cut_point_gaps_groups'),
  inc_warmup = FALSE,
  format = "draws_array"
  )

bayesplot::color_scheme_set('viridisC')
p <- bayesplot::mcmc_pairs(po, 
                           diag_fun = "dens", 
                           off_diag_fun = "hex"
                           ) 
ggsave(file = file.path(dir.out, paste0(file.prefix,'m5_pairsplot.png')), 
       plot = p, 
       h = 30, 
       w = 30, 
       limitsize = FALSE
       )
bayesplot::color_scheme_set('brewer-RdYlBu')

# make posterior predictive check

# create median and 95\% credible intervals
po <- ordered_categorical_logit_fit_m5$draws(
  variables = c('ypred'),
  inc_warmup = FALSE,
  format = "draws_df"
  )
po <- as.data.table(po)
po <- data.table::melt(po, id.vars = c('.draw','.chain','.iteration'), value.name = 'ypred')
po[, oid := as.integer(gsub('ypred\\[([0-9]+)\\]','\\1',variable)) ]
pos <- 
  po[,
     list( summary_value = quantile(ypred, prob = c(0.025, 0.25, 0.5, 0.75, 0.975)),
           summary_name = c('q_lower','iqr_lower','median','iqr_upper','q_upper') 
           ),
     by = 'oid'
     ]
pos <- 
  data.table::dcast(pos,
                    oid ~ summary_name, 
                    value.var = 'summary_value'
                    )
pos <- merge(pos, dcat1, by = 'oid')
pos[, IN_PPI := value >= q_lower & value <= q_upper]

# plot posterior predictive check
p <- ggplot(pos, aes(x = oid, group = oid)) + 
  geom_boxplot( aes( ymin = q_lower,
                     lower = iqr_lower,
                     middle = median,
                     upper = iqr_upper,
                     ymax = q_upper),
                stat = 'identity') +
  geom_point( aes(y = value, colour = IN_PPI ) ) +
  facet_grid(variable ~ arm_label+time_label, scales = 'free') +
  scale_x_discrete() +
  scale_y_continuous() +
  ggsci::scale_color_npg() +
  labs(x = '', 
       y = 'outcome', 
       colour = 'within\n95% posterior\nprediction\ninterval') +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 45,vjust = 1,hjust = 1))
ggsave(file = file.path(dir.out, paste0(file.prefix,'m5_ppcheck.png')), 
       p, 
       w = 12, 
       h = 12)
```

```{r ordered_logistic_latent_factor_extract_participant_effects, include=FALSE, eval=TRUE, echo=FALSE, tidy=FALSE, cache=TRUE}
# extract participant effects
po <- ordered_categorical_logit_fit_m5$draws(
  variables = c('beta_unit_sd','beta_unit'),
  inc_warmup = FALSE,
  format = "draws_df"
  )
po <- as.data.table(po)
po <- data.table::melt(po, id.vars = c('.draw','.chain','.iteration'))
tmp <- subset(po, variable == 'beta_unit_sd')
tmp <- data.table::dcast(tmp, .draw ~ variable, value.var = 'value')
po <- subset(po, variable != 'beta_unit_sd')
po <- merge(po, tmp, by = '.draw')
po[, pid := as.integer(gsub('beta_unit\\[([0-9]+)\\]','\\1',variable)) ]
po[, value := value * beta_unit_sd]

pos <- po[, 
          list( summary_value = quantile(value, prob = c(0.025, 0.25, 0.5, 0.75, 0.975)),
                summary_name = c('q_lower','iqr_lower','median','iqr_upper','q_upper') 
                ), 
          by = c('pid')
          ]
pos <- data.table::dcast(pos, pid ~ summary_name, value.var = 'summary_value')
tmp <- unique(subset(dcat1, select = c(pid, arm_label, time_label)))
pos <- merge(pos, tmp, by = c('pid'))
tmp <- dcat1[, list(avg_value = mean(value)), by = 'pid']
tmp[, avg_value_jitter := avg_value + runif(nrow(tmp), min = -1/8, max = 1/8)]
pos <- merge(pos, tmp, by = 'pid')

tmp <- pos[, list(avg_effect = mean(median)), by = c('arm_label','time_label')]

p <- ggplot(data = pos, aes( x = avg_value_jitter)) +
  geom_hline(data = tmp, aes( yintercept = avg_effect), colour = 'grey50') +
  geom_linerange(aes( ymin = q_lower, ymax = q_upper), 
           stat = 'identity') +
  geom_point(aes( y = median)) +
  labs(x = '\naverage response of participant', y = 'participant effect') +
  theme_bw() +
  facet_grid(arm_label ~ time_label, scales = 'free') +
  theme(axis.text.x = element_text(angle = 45,vjust = 1,hjust = 1),
        legend.position = 'bottom'
        )
ggsave(file = file.path(dir.out, paste0(file.prefix,'m5_participant_effects.png')), 
       plot = p, 
       h = 8, 
       w = 12
       )
```

Here are the participant effects around the group mean latent factors, showing the same behaviour as before. This demonstrates that (1) the added baseline response effects are not able to explain the data without latent traits, and (2) there is substantial individual-level heterogeneity around the group mean latent factors.

```{r ordered_logistic_latent_factor_participant_effect_plot, include=TRUE, eval=TRUE, echo=FALSE, tidy=FALSE, fig.align='center', out.width='80%'}
knitr::include_graphics( file.path(dir.out, paste0(file.prefix,'m5_participant_effects.png')) )
```

```{r ordered_logistic_latent_factor_latent_eta, include=FALSE, eval=TRUE, echo=FALSE, tidy=FALSE, cache=TRUE}
# extract model probabilities
po <- ordered_categorical_logit_fit_m5$draws(
  variables = c('mean_eta'),
  inc_warmup = FALSE,
  format = "draws_df"
  )
po <- as.data.table(po)
po <- data.table::melt(po, id.vars = c('.draw','.chain','.iteration'), value.name = 'mean_eta')
po[, group := as.integer(gsub('(.*)\\[([0-9]),([0-9])\\]$','\\2',variable)) ]
po[, vid := as.integer(gsub('(.*)\\[([0-9]),([0-9])\\]$','\\3',variable)) ]
set(po, NULL, 'variable', NULL)
tmp <- unique( subset(dcat1, select = c(group, arm, arm_label, time, time_label, variable, vid)) )
po <- merge(po, tmp, by = c('group','vid'))
po <- data.table::dcast(po, .draw + arm + arm_label + time + time_label + group ~ variable, value.var = 'mean_eta')
p <- GGally::ggpairs(po, 
                columns = c('PHQ4_down','PHQ4_interest','PHQ4_nervous','PHQ4_worry'),
                ggplot2::aes(colour = interaction(arm_label, time_label)),
                lower = list(continuous = ggally_density),
                diag = list(continuous = 'blankDiag')
                )
p <- p + scale_fill_npg() + 
  theme_bw()
ggsave(file = file.path(dir.out, paste0(file.prefix,'m5_eta_pairsplot.png')), 
       plot = p, 
       h = 10, 
       w = 10
       )
```

Here is the polychoric correlation plot. We ignore the $\beta_i^{\text{RND}}$ latent factor
random effects, and can then plot the group-mean multivariate structure of the joint probabilities in logit-space:

```{r ordered_logistic_latent_factor_latent_eta_plot, include=TRUE, eval=TRUE, echo=FALSE, tidy=FALSE, fig.align='center', out.width='80%'}
knitr::include_graphics( file.path(dir.out, paste0(file.prefix,'m5_eta_pairsplot.png')) )
```

```{r ordered_logistic_latent_factor_extract_class_probs_by_n, include=FALSE, eval=TRUE, echo=FALSE, tidy=FALSE, cache=TRUE}
po <- ordered_categorical_logit_fit_m5$draws(
  variables = c('ordered_prob_by_obs'),
  inc_warmup = FALSE,
  format = "draws_df"
  )
po <- as.data.table(po)
po <- data.table::melt(po, id.vars = c('.draw','.chain','.iteration'), value.name = 'prob')
po[, value := as.integer(gsub('ordered_prob_by_obs\\[([0-9]),([0-9]+)\\]','\\1',variable)) ]
po[, oid := as.integer(gsub('ordered_prob_by_obs\\[([0-9]),([0-9]+)\\]','\\2',variable)) ]
set(po, NULL, 'variable', NULL)
tmp <- unique(subset(dcat1, select = c(oid, pid, vid, group )))
po <- merge(po, tmp, by = c('oid'))
po <- po[, list(prob = mean(prob)), by = c('.draw','group','value')]

pos <- po[, 
          list( summary_value = quantile(prob, prob = c(0.025, 0.25, 0.5, 0.75, 0.975)),
                summary_name = c('q_lower','iqr_lower','median','iqr_upper','q_upper') 
                ), 
          by = c('group','value')
          ]
pos <- data.table::dcast(pos, group + value ~ summary_name, value.var = 'summary_value')

tmp <- unique( subset(dcat1, select = c(group, arm, arm_label, time, time_label, value, value_label)) )
pos <- merge(pos, tmp, by = c('group','value'))

tmp <- dcat1[, list(n = length(pid)), by = c('arm_label','time_label','value_label','value')]
tmp2 <- tmp[, list(total = sum(n)), by = c('arm_label','time_label')]
tmp <- merge(tmp, tmp2, by = c('arm_label','time_label'))
tmp[, p_emp := n/total]
pos <- merge(pos, subset(tmp, select = -c(value, n, total)), by = c('arm_label','time_label','value_label'))

p <- ggplot(data = pos, aes( x = value_label, fill = interaction(arm_label,time_label))) +
  geom_bar(aes( y = p_emp), 
           stat = 'identity', 
           position = position_dodge()) +
  geom_point(aes( y = median),
             position = position_dodge(0.9), colour = 'black') +
  geom_boxplot( aes(ymin = q_lower, lower = iqr_lower, middle = median, upper = iqr_upper, ymax = q_upper), 
                position = position_dodge(0.9),
                stat = 'identity',
                alpha = 0,
                width = 0.3) +
  scale_fill_npg() +
  scale_y_continuous(labels = scales::percent) +
  labs(x = '\nParental mental health\nPHQ4 outcome', y = 'group-level\nposterior probability', fill = '') +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 45,vjust = 1,hjust = 1),
        legend.position = 'bottom'
        )
ggsave(file = file.path(dir.out, paste0(file.prefix,'m5_probs_barplot.png')), 
       plot = p, 
       h = 8, 
       w = 8
       )
```

Here is the model fit (in black) relative to the aggregated survey items. Hoorary, we retain a good fit relative to our primary target statistics:

```{r ordered_logistic_latent_factor_plot, include=TRUE, eval=TRUE, echo=FALSE, tidy=FALSE, fig.align='center', out.width='80%'}
knitr::include_graphics( file.path(dir.out, paste0(file.prefix,'m5_probs_barplot.png')) )
```

Here is the model fit (in black) relative to each survey item. We don't achieve a good fit. While we previously we considered this okay, we might like that a factor model, which aims to model the responses for each survey item, does a better job here. 

```{r ordered_logistic_latent_factor_extract_class_probs_by_question, include=FALSE, eval=TRUE, echo=FALSE, tidy=FALSE, cache=TRUE}
po <- ordered_categorical_logit_fit_m5$draws(
  variables = c('ordered_prob_by_obs'),
  inc_warmup = FALSE,
  format = "draws_df"
  )
po <- as.data.table(po)
po <- data.table::melt(po, id.vars = c('.draw','.chain','.iteration'), value.name = 'prob')
po[, value := as.integer(gsub('ordered_prob_by_obs\\[([0-9]),([0-9]+)\\]','\\1',variable)) ]
po[, oid := as.integer(gsub('ordered_prob_by_obs\\[([0-9]),([0-9]+)\\]','\\2',variable)) ]
set(po, NULL, 'variable', NULL)
tmp <- unique(subset(dcat1, select = c(oid, pid, vid, group )))
po <- merge(po, tmp, by = c('oid'))
po <- po[, list(prob = mean(prob)), by = c('.draw','group','vid','value')]

pos <- po[, 
          list( summary_value = quantile(prob, prob = c(0.025, 0.25, 0.5, 0.75, 0.975)),
                summary_name = c('q_lower','iqr_lower','median','iqr_upper','q_upper') 
                ), 
          by = c('group','vid','value')
          ]
pos <- data.table::dcast(pos, group + vid + value ~ summary_name, value.var = 'summary_value')

tmp <- unique( subset(dcat1, select = c(group, arm, arm_label, time, time_label, variable, vid, value, value_label)) )
pos <- merge(pos, tmp, by = c('group','vid','value'))

tmp <- dcat1[, list(n = length(pid)), by = c('arm_label','time_label','variable','value_label','value')]
tmp2 <- tmp[, list(total = sum(n)), by = c('arm_label','time_label','variable')]
tmp <- merge(tmp, tmp2, by = c('arm_label','time_label','variable'))
tmp[, p_emp := n/total]
pos <- merge(pos, subset(tmp, select = -c(value, n, total)), by = c('arm_label','time_label','variable','value_label'))

p <- ggplot(pos, aes(x = value_label, group = interaction(variable, value_label))) +
  geom_col(aes(fill = variable, y = p_emp), 
           position = position_dodge(0.9)
           ) +
  geom_boxplot( aes(ymin = q_lower, lower = iqr_lower, middle = median, upper = iqr_upper, ymax = q_upper), 
                position = position_dodge(0.9),
                stat = 'identity',
                alpha = 0,
                width = 0.3) +
  scale_fill_bmj() +
  scale_y_continuous(labels = scales::percent) +
  facet_grid(arm_label ~ time_label) +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 45,vjust = 1,hjust = 1),
        legend.position = 'bottom'
        ) +
  labs(x = '', x = 'Likert scale', y = 'proportion of outcomes', fill = '')
ggsave(file = file.path(dir.out, paste0(file.prefix,'m5_frequencies_v2.png')), 
       plot = p, 
       h = 10, 
       w = 8
       )
```

```{r ordered_logistic_latent_factor_extract_class_probs_by_question_plot, include=TRUE, eval=TRUE, echo=FALSE, tidy=FALSE, fig.align='center', out.width='80%'}
knitr::include_graphics( file.path(dir.out, paste0(file.prefix,'m5_frequencies_v2.png')) )
```

```{r ordered_logistic_latent_factor_extract_class_probs_by_question3, include=FALSE, eval=TRUE, echo=FALSE, tidy=FALSE, cache=TRUE}
po <- ordered_categorical_logit_fit_m5$draws(
  variables = c('ypred'),
  inc_warmup = FALSE,
  format = "draws_df"
  )
po <- as.data.table(po)
po <- data.table::melt(po, id.vars = c('.draw','.chain','.iteration'), value.name = 'value')
po[, oid := as.integer(gsub('ypred\\[([0-9]+)\\]','\\1',variable)) ]
set(po, NULL, 'variable', NULL)
tmp <- unique(subset(dcat1, select = c(oid, pid, variable, group )))
po <- merge(po, tmp, by = c('oid'))
tmp <- unique(subset(dcat1, select = c(value, value_label )))
po <- merge(po, tmp, by = c('value'))
po <- data.table::dcast( po, .draw + pid + group ~ variable, value.var = 'value_label')
po <- melt(po, id.vars = c('.draw','pid','group','PHQ4_down'), value.name = 'value_label')
po <- po[, list(k = length(pid)), by = c('.draw', 'group', 'PHQ4_down', 'value_label')]
tmp <- po[, list(n = sum(k)), by = c('.draw', 'group', 'PHQ4_down')]
po <- merge(po, tmp, by = c('.draw', 'group', 'PHQ4_down'))
po[, prob := k/n]
tmp <- CJ(.draw = unique(po$.draw),
          group = unique(po$group), 
          PHQ4_down = unique(po$PHQ4_down), 
          value_label = unique(po$PHQ4_down))
po <- merge(tmp, po, by = c('.draw','group','PHQ4_down','value_label'), all.x = TRUE)
set(po, po[, which(is.na(prob))], 'prob', 0.)
pos <- po[, 
          list( summary_value = quantile(prob, prob = c(0.025, 0.25, 0.5, 0.75, 0.975)),
                summary_name = c('q_lower','iqr_lower','median','iqr_upper','q_upper') 
                ), 
          by = c('group','PHQ4_down','value_label')
          ]
pos <- data.table::dcast(pos, group + PHQ4_down + value_label ~ summary_name, value.var = 'summary_value')
tmp <- unique( subset(dcat1, select = c(group, arm, arm_label, time, time_label)) )
pos <- merge(pos, tmp, by = c('group'))

tmp <- data.table:::dcast( dcat1, time_label + arm_label + pid ~ variable, value.var = 'value_label')
tmp <- melt(tmp, id.vars = c('time_label','arm_label','pid','PHQ4_down'))
tmp <- tmp[, list(k = length(pid)), by = c('time_label','arm_label','PHQ4_down','value')]
tmp2 <- tmp[, list(n = sum(k)), by = c('time_label','arm_label','PHQ4_down')]
tmp <- merge(tmp, tmp2, by = c('time_label','arm_label','PHQ4_down'))
tmp[, p_emp := k/n]
set(tmp, NULL, c('k','n'), NULL)
tmp2 <- CJ(time_label = unique(tmp$time_label), 
           arm_label = unique(tmp$arm_label), 
           PHQ4_down = unique(tmp$PHQ4_down), 
           value = unique(tmp$PHQ4_down))
tmp <- merge(tmp2, tmp, by = c('arm_label','time_label','PHQ4_down','value'), all.x = TRUE)
set(tmp, tmp[, which(is.na(p_emp))], 'p_emp', 0.)
setnames(tmp, 'value','value_label')
pos <- merge(pos, tmp, by = c('arm_label','time_label','PHQ4_down','value_label'), all.y = TRUE)

p <- ggplot(pos, aes( x = PHQ4_down, group = interaction(PHQ4_down, value_label))) +
  geom_col(aes(y = p_emp, fill = value_label), 
           stat = 'identity',
           position = position_dodge(0.9)) +
  geom_boxplot( aes(ymin = q_lower, lower = iqr_lower, middle = median, upper = iqr_upper, ymax = q_upper), 
                position = position_dodge(0.9),
                stat = 'identity',
                alpha = 0,
                width = 0.3) +
  scale_fill_bmj() +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 45,vjust = 1,hjust = 1),
        legend.position = 'bottom'
        ) +
  labs(x = 'response to PHQ4_down question', y = 'proportion', fill = 'responses to other PHQ4 questions') +
  facet_grid(arm_label ~ time_label)
ggsave(file = file.path(dir.out, paste0(file.prefix,'m5_polychoric_correlations_across_PHQ_questions_same_arm-time.png')),
       plot = p, 
       h = 8, 
       w = 8
       )
```
Here is a visualisation of the estimated polychoric correlations. We can clearly see how the model is inducing some
correlations between item responses:

```{r ordered_logistic_latent_factor_extract_class_probs_by_question2_plot, include=TRUE, eval=TRUE, echo=FALSE, tidy=FALSE, fig.align='center', out.width='80%'}
knitr::include_graphics( file.path(dir.out, paste0(file.prefix,'m5_polychoric_correlations_across_PHQ_questions_same_arm-time.png')) )
```


## Second multi-group model

Next up, I will investigate a more expressive multi-group model with group-specific item baselines and
group-specific loadings. I intentionally do not introducce group-specific participant effects, because the
same participants are present at Baseline and Endline.

\begin{align*}
& \text{Prob}(Y_{ij} = k) = \text{Ordered-Logistic}(k; \eta_{ij}, c_{G(i)}) \\
& \eta_{ij}= \gamma_{G(i),j} + \lambda_{G(i),j} * (\beta_{G(i)} + \beta^{\text{RE}}_i)\\
& \gamma_g \sim \text{S2Z-Normal}(0,1)\\
& \lambda_{g,1} = 1\\
& \lambda_{g,2:J} \sim \text{Normal}(0,1)\\
& \beta \sim \text{S2Z-Normal}(0, 1)\\
& \beta^{\text{RE}} \sim \text{S2Z-Normal}(0,1)\\
& c_g = [c_{g1}, c_{g1} + \text{cumsum}(\delta_{g,1:{K-2}})] \\
& c_{g1} \sim \text{Normal}(0 , 3.5^2)\\
& \delta_{g,1:{K-2}} \sim \text{Normal}(0 , 2.5^2)\\
\end{align*}

Here is the Stan code:

```{r ordered_logistic_latent_factor-b_stan_code, include=TRUE, eval=TRUE, echo=TRUE, tidy=FALSE, cache=TRUE}
ordered_categorical_logit_txt_m5b <- "
data
{
  int<lower=1> N; // number of observations
  int<lower=3> K; // number of categories
  int<lower=2> Q; // number of questions
  int<lower=2> P; // number of groups
  int<lower=1> U; // number of units
  array [N] int<lower=1, upper=K> y; // observations
  array [N] int<lower=1,upper=P> group_of_obs;
  array [N] int<lower=1,upper=Q> question_of_obs;
  array [N] int<lower=1,upper=U> unit_of_obs;
}
transformed data
{
    real s2z_sd_groups;
    real s2z_sd_unit;
    real s2z_sd_questions;
    s2z_sd_groups = inv(sqrt(1. - inv(P)));
    s2z_sd_unit = inv(sqrt(1. - inv(U)));
    s2z_sd_questions = inv(sqrt(1. - inv(Q)));
}
parameters
{
  sum_to_zero_vector[P] factor_group;
  array [P] sum_to_zero_vector[Q] beta_questions;
  array [P] vector<lower=0>[Q-1] loadings_questions_m1;
  sum_to_zero_vector[U] beta_unit;
  real<lower=0> beta_unit_sd;
  
  // the cutpoints in logit space, using the 'ordered' variable type
  real cut_point_1;
  matrix<lower=0>[K - 2,P] cut_point_gaps_groups;
}
transformed parameters
{
  array [P] ordered[K-1] cut_points;
  array [P] vector<lower=0>[Q] loadings_questions;
  
  for (p in 1:P)
  {
    cut_points[p] = 
      append_row(
        rep_vector(cut_point_1, 1), 
        rep_vector(cut_point_1, K-2) + cumulative_sum(cut_point_gaps_groups[,p])
        );
        
    loadings_questions[p] = append_row(1.0, loadings_questions_m1[p]);
  }
}
model
{
  // likelihood, cannot be vectorized
  for (n in 1:N) {
    y[n] ~ ordered_logistic(
      beta_questions[group_of_obs[n]][question_of_obs[n]] + 
        loadings_questions[group_of_obs[n]][question_of_obs[n]] *
        (
          factor_group[group_of_obs[n]] + 
          beta_unit_sd * beta_unit[unit_of_obs[n]]
        ), 
      cut_points[group_of_obs[n]]);
  }
  
  // priors
  factor_group ~ normal(0, s2z_sd_groups); 
  beta_unit ~ normal(0, s2z_sd_unit); 
  for (p in 1:P)
  {
    beta_questions[p] ~ normal(0, s2z_sd_questions); 
    loadings_questions_m1[p] ~ lognormal(0,1);
  }
  beta_unit_sd ~ exponential(2); 
  cut_point_1 ~ normal(0, 3.5); 
  to_vector(cut_point_gaps_groups) ~ normal(0, 2.5); 
}
generated quantities
{
  array [P,Q] real mean_eta;
  array [K,N] real<lower=0, upper=1> ordered_prob_by_obs;
  array [N] int<lower=0> ypred;
  array [N] real log_lik;
  vector[U] beta_unit_re;
  
  for (q in 1:Q) 
  {
    for (p in 1:P) 
    {
      mean_eta[p,q] = beta_questions[p][q] + loadings_questions[p][q] * factor_group[p];
    }
  }
  
  {
    real tmp_real;
    for (n in 1:N) 
    {
        tmp_real = beta_questions[group_of_obs[n]][question_of_obs[n]] + 
                    loadings_questions[group_of_obs[n]][question_of_obs[n]] *
                    (
                      factor_group[group_of_obs[n]] + 
                      beta_unit_sd * beta_unit[unit_of_obs[n]]
                    );
        ordered_prob_by_obs[1,n]       = 1 - inv_logit( tmp_real - cut_points[group_of_obs[n]][1] );
        ordered_prob_by_obs[2:(K-1),n] = to_array_1d( inv_logit( tmp_real - cut_points[group_of_obs[n]][1:(K-2)] ) 
                                                              - 
                                                              inv_logit( tmp_real - cut_points[group_of_obs[n]][2:(K-1)] )
                                                              );
        ordered_prob_by_obs[K,n]       = inv_logit( tmp_real - cut_points[group_of_obs[n]][K-1] );
    }
  }
  
  for( n in 1:N ) 
  {
    ypred[n] = ordered_logistic_rng( 
                beta_questions[group_of_obs[n]][question_of_obs[n]] + 
                  loadings_questions[group_of_obs[n]][question_of_obs[n]] *
                  (
                    factor_group[group_of_obs[n]] + 
                    beta_unit_sd * beta_unit[unit_of_obs[n]]
                  ), 
                cut_points[group_of_obs[n]]
                ); 
    log_lik[n] =  ordered_logistic_lpmf( y[n] |
                    beta_questions[group_of_obs[n]][question_of_obs[n]] + 
                      loadings_questions[group_of_obs[n]][question_of_obs[n]] *
                      (
                        factor_group[group_of_obs[n]] + 
                        beta_unit_sd * beta_unit[unit_of_obs[n]]
                      ),
                    cut_points[group_of_obs[n]]);
  }
  
  beta_unit_re = beta_unit_sd * beta_unit;
}
"

# compile the model
ordered_categorical_logit_m5b_filename <- cmdstanr::write_stan_file(
  gsub('\t',' ', ordered_categorical_logit_txt_m5b),
  dir = dir.out,
  basename = NULL,
  force_overwrite = FALSE,
  hash_salt = ""
)

# compile Stan model
ordered_categorical_logit_compiled_m5b <- cmdstanr::cmdstan_model(ordered_categorical_logit_m5b_filename)
```
Let's fit the model!

```{r ordered_logistic_latent_factor-b_run_model, include=TRUE, eval=TRUE, echo=TRUE, message=FALSE, tidy=FALSE, cache=TRUE}
file.prefix <- 'parentalmh_ordered_categorical_'

# define data in format needed for model specification
stan_data <- list()
stan_data$N <- nrow(dcat1)
stan_data$P <- max(dcat1$group)
stan_data$Q <- max(dcat1$vid)
stan_data$U <- max(dcat1$pid)
stan_data$K <- length(unique(dcat1$value))
stan_data$y <- dcat1$value
stan_data$group_of_obs <- dcat1$group
stan_data$question_of_obs <- dcat1$vid
stan_data$unit_of_obs <- dcat1$pid

# sample
ordered_categorical_logit_fit_m5b <- ordered_categorical_logit_compiled_m5b$sample(
  data = stan_data,
  seed = 123,
  chains = 2,
  parallel_chains = 2,
  iter_warmup = 5e2,
  iter_sampling = 15e2,
  refresh = 500,
  save_warmup = TRUE
)

# save output to RDS
ordered_categorical_logit_fit_m5b$save_object(file = file.path(dir.out, paste0(file.prefix,"m5b_stan.rds")))
```

```{r ordered_logistic_latent_factor-b_load_model, include=FALSE, eval=TRUE, echo=FALSE, tidy=FALSE, cache=TRUE}
ordered_categorical_logit_fit_m5b <- readRDS(file = file.path(dir.out, paste0(file.prefix,'m5b_stan.rds')))
```

```{r ordered_logistic_latent_factor-b_check_mixing_convergence, include=FALSE, eval=TRUE, echo=FALSE, tidy=FALSE, cache=TRUE}
#	check convergence and check mixing
#	get 95% credible intervals
tmp <- ordered_categorical_logit_fit_m5b$summary(
  variables = c('factor_group','beta_unit','beta_unit_sd','beta_questions',
                'loadings_questions_m1','cut_point_1','cut_point_gaps_groups'),
  posterior::default_summary_measures(),
  posterior::default_convergence_measures(),
  extra_quantiles = ~posterior::quantile2(., probs = c(.0275, .975))
)
tmp <- as.data.table(tmp)
tmp
  
# parameter with lowest ess_bulk
worst_var <- tmp$variable[ which.min(tmp$ess_bulk) ]

# extract samples
po <- ordered_categorical_logit_fit_m5b$draws(
  variables = c("lp__", worst_var),
  inc_warmup = TRUE,
  format = "draws_array"
  )

# make worst trace plot
p <- bayesplot:::mcmc_trace(po,  
                            pars = c("lp__",worst_var), 
                            n_warmup = 500,
                            facet_args = list(nrow = 2)
                            )
p <- p + theme_bw()
ggsave(file = file.path(dir.out, paste0(file.prefix,'m5b_worsttrace.png')), 
       plot = p, 
       h = 10, 
       w = 8
       )

# make intervals plot
po <- ordered_categorical_logit_fit_m5b$draws(
  variables = c('factor_group','beta_questions','loadings_questions',
                'cut_point_1','cut_point_gaps_groups',
                'beta_unit_sd','beta_unit'),
  inc_warmup = FALSE,
  format = "draws_array"
  )

color_scheme_set("teal")
p <- bayesplot::mcmc_intervals(po, prob = 0.5, prob_outer = 0.95, outer_size = 1, point_size = 2) +
  theme_bw()
ggsave(file = file.path(dir.out, paste0(file.prefix,'m5b_intervals.png')), 
       plot = p, 
       h = 50, 
       w = 8, 
       limitsize = FALSE
       )

# make pairs plot
po <- ordered_categorical_logit_fit_m5b$draws(
  variables = c('factor_group','cut_point_1','cut_point_gaps_groups'),
  inc_warmup = FALSE,
  format = "draws_array"
  )

bayesplot::color_scheme_set('viridisC')
p <- bayesplot::mcmc_pairs(po, 
                           diag_fun = "dens", 
                           off_diag_fun = "hex"
                           ) 
ggsave(file = file.path(dir.out, paste0(file.prefix,'m5b_pairsplot.png')), 
       plot = p, 
       h = 30, 
       w = 30, 
       limitsize = FALSE
       )
bayesplot::color_scheme_set('brewer-RdYlBu')

# make posterior predictive check

# create median and 95\% credible intervals
po <- ordered_categorical_logit_fit_m5b$draws(
  variables = c('ypred'),
  inc_warmup = FALSE,
  format = "draws_df"
  )
po <- as.data.table(po)
po <- data.table::melt(po, id.vars = c('.draw','.chain','.iteration'), value.name = 'ypred')
po[, oid := as.integer(gsub('ypred\\[([0-9]+)\\]','\\1',variable)) ]
pos <- 
  po[,
     list( summary_value = quantile(ypred, prob = c(0.025, 0.25, 0.5, 0.75, 0.975)),
           summary_name = c('q_lower','iqr_lower','median','iqr_upper','q_upper') 
           ),
     by = 'oid'
     ]
pos <- 
  data.table::dcast(pos,
                    oid ~ summary_name, 
                    value.var = 'summary_value'
                    )
pos <- merge(pos, dcat1, by = 'oid')
pos[, IN_PPI := value >= q_lower & value <= q_upper]

# plot posterior predictive check
p <- ggplot(pos, aes(x = oid, group = oid)) + 
  geom_boxplot( aes( ymin = q_lower,
                     lower = iqr_lower,
                     middle = median,
                     upper = iqr_upper,
                     ymax = q_upper),
                stat = 'identity') +
  geom_point( aes(y = value, colour = IN_PPI ) ) +
  facet_grid(variable ~ arm_label+time_label, scales = 'free') +
  scale_x_discrete() +
  scale_y_continuous() +
  ggsci::scale_color_npg() +
  labs(x = '', 
       y = 'outcome', 
       colour = 'within\n95% posterior\nprediction\ninterval') +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 45,vjust = 1,hjust = 1))
ggsave(file = file.path(dir.out, paste0(file.prefix,'m5b_ppcheck.png')), 
       p, 
       w = 12, 
       h = 12)
```

```{r ordered_logistic_latent_factor-b_extract_participant_effects, include=FALSE, eval=TRUE, echo=FALSE, tidy=FALSE, cache=TRUE}
# extract participant effects
po <- ordered_categorical_logit_fit_m5b$draws(
  variables = c('beta_unit_sd','beta_unit'),
  inc_warmup = FALSE,
  format = "draws_df"
  )
po <- as.data.table(po)
po <- data.table::melt(po, id.vars = c('.draw','.chain','.iteration'))
tmp <- subset(po, variable == 'beta_unit_sd')
tmp <- data.table::dcast(tmp, .draw ~ variable, value.var = 'value')
po <- subset(po, variable != 'beta_unit_sd')
po <- merge(po, tmp, by = '.draw')
po[, pid := as.integer(gsub('beta_unit\\[([0-9]+)\\]','\\1',variable)) ]
po[, value := value * beta_unit_sd]

pos <- po[, 
          list( summary_value = quantile(value, prob = c(0.025, 0.25, 0.5, 0.75, 0.975)),
                summary_name = c('q_lower','iqr_lower','median','iqr_upper','q_upper') 
                ), 
          by = c('pid')
          ]
pos <- data.table::dcast(pos, pid ~ summary_name, value.var = 'summary_value')
tmp <- unique(subset(dcat1, select = c(pid, arm_label, time_label)))
pos <- merge(pos, tmp, by = c('pid'))
tmp <- dcat1[, list(avg_value = mean(value)), by = 'pid']
tmp[, avg_value_jitter := avg_value + runif(nrow(tmp), min = -1/8, max = 1/8)]
pos <- merge(pos, tmp, by = 'pid')

tmp <- pos[, list(avg_effect = mean(median)), by = c('arm_label','time_label')]

p <- ggplot(data = pos, aes( x = avg_value_jitter)) +
  geom_hline(data = tmp, aes( yintercept = avg_effect), colour = 'grey50') +
  geom_linerange(aes( ymin = q_lower, ymax = q_upper), 
           stat = 'identity') +
  geom_point(aes( y = median)) +
  labs(x = '\naverage response of participant', y = 'participant effect') +
  theme_bw() +
  facet_grid(arm_label ~ time_label, scales = 'free') +
  theme(axis.text.x = element_text(angle = 45,vjust = 1,hjust = 1),
        legend.position = 'bottom'
        )
ggsave(file = file.path(dir.out, paste0(file.prefix,'m5b_participant_effects.png')), 
       plot = p, 
       h = 8, 
       w = 12
       )
```

```{r ordered_logistic_latent_factor-b_participant_effect_plot, include=FALSE, eval=FALSE, echo=FALSE, tidy=FALSE, fig.align='center', out.width='80%'}
knitr::include_graphics( file.path(dir.out, paste0(file.prefix,'m5b_participant_effects.png')) )
```

```{r ordered_logistic_latent_factor-b_latent_eta, include=FALSE, eval=TRUE, echo=FALSE, tidy=FALSE, cache=TRUE}
# extract model probabilities
po <- ordered_categorical_logit_fit_m5b$draws(
  variables = c('mean_eta'),
  inc_warmup = FALSE,
  format = "draws_df"
  )
po <- as.data.table(po)
po <- data.table::melt(po, id.vars = c('.draw','.chain','.iteration'), value.name = 'mean_eta')
po[, group := as.integer(gsub('(.*)\\[([0-9]),([0-9])\\]$','\\2',variable)) ]
po[, vid := as.integer(gsub('(.*)\\[([0-9]),([0-9])\\]$','\\3',variable)) ]
set(po, NULL, 'variable', NULL)
tmp <- unique( subset(dcat1, select = c(group, arm, arm_label, time, time_label, variable, vid)) )
po <- merge(po, tmp, by = c('group','vid'))
po <- data.table::dcast(po, .draw + arm + arm_label + time + time_label + group ~ variable, value.var = 'mean_eta')
p <- GGally::ggpairs(po, 
                columns = c('PHQ4_down','PHQ4_interest','PHQ4_nervous','PHQ4_worry'),
                ggplot2::aes(colour = interaction(arm_label, time_label)),
                lower = list(continuous = ggally_density),
                diag = list(continuous = 'blankDiag')
                )
p <- p + scale_fill_npg() + 
  theme_bw()
ggsave(file = file.path(dir.out, paste0(file.prefix,'m5b_eta_pairsplot.png')), 
       plot = p, 
       h = 10, 
       w = 10
       )
```

```{r ordered_logistic_latent_factor-b_latent_eta_plot, include=FALSE, eval=FALSE, echo=FALSE, tidy=FALSE, fig.align='center', out.width='80%'}
knitr::include_graphics( file.path(dir.out, paste0(file.prefix,'m5b_eta_pairsplot.png')) )
```

```{r ordered_logistic_latent_factor-b_extract_class_probs_by_n, include=FALSE, eval=TRUE, echo=FALSE, tidy=FALSE, cache=TRUE}
po <- ordered_categorical_logit_fit_m5b$draws(
  variables = c('ordered_prob_by_obs'),
  inc_warmup = FALSE,
  format = "draws_df"
  )
po <- as.data.table(po)
po <- data.table::melt(po, id.vars = c('.draw','.chain','.iteration'), value.name = 'prob')
po[, value := as.integer(gsub('ordered_prob_by_obs\\[([0-9]),([0-9]+)\\]','\\1',variable)) ]
po[, oid := as.integer(gsub('ordered_prob_by_obs\\[([0-9]),([0-9]+)\\]','\\2',variable)) ]
set(po, NULL, 'variable', NULL)
tmp <- unique(subset(dcat1, select = c(oid, pid, vid, group )))
po <- merge(po, tmp, by = c('oid'))
po <- po[, list(prob = mean(prob)), by = c('.draw','group','value')]

pos <- po[, 
          list( summary_value = quantile(prob, prob = c(0.025, 0.25, 0.5, 0.75, 0.975)),
                summary_name = c('q_lower','iqr_lower','median','iqr_upper','q_upper') 
                ), 
          by = c('group','value')
          ]
pos <- data.table::dcast(pos, group + value ~ summary_name, value.var = 'summary_value')

tmp <- unique( subset(dcat1, select = c(group, arm, arm_label, time, time_label, value, value_label)) )
pos <- merge(pos, tmp, by = c('group','value'))

tmp <- dcat1[, list(n = length(pid)), by = c('arm_label','time_label','value_label','value')]
tmp2 <- tmp[, list(total = sum(n)), by = c('arm_label','time_label')]
tmp <- merge(tmp, tmp2, by = c('arm_label','time_label'))
tmp[, p_emp := n/total]
pos <- merge(pos, subset(tmp, select = -c(value, n, total)), by = c('arm_label','time_label','value_label'))

p <- ggplot(data = pos, aes( x = value_label, fill = interaction(arm_label,time_label))) +
  geom_bar(aes( y = p_emp), 
           stat = 'identity', 
           position = position_dodge()) +
  geom_point(aes( y = median),
             position = position_dodge(0.9), colour = 'black') +
  geom_boxplot( aes(ymin = q_lower, lower = iqr_lower, middle = median, upper = iqr_upper, ymax = q_upper), 
                position = position_dodge(0.9),
                stat = 'identity',
                alpha = 0,
                width = 0.3) +
  scale_fill_npg() +
  scale_y_continuous(labels = scales::percent) +
  labs(x = '\nParental mental health\nPHQ4 outcome', y = 'group-level\nposterior probability', fill = '') +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 45,vjust = 1,hjust = 1),
        legend.position = 'bottom'
        )
ggsave(file = file.path(dir.out, paste0(file.prefix,'m5b_probs_barplot.png')), 
       plot = p, 
       h = 8, 
       w = 8
       )
```

```{r ordered_logistic_latent_factor-b_plot, include=FALSE, eval=FALSE, echo=FALSE, tidy=FALSE, fig.align='center', out.width='80%'}
knitr::include_graphics( file.path(dir.out, paste0(file.prefix,'m5b_probs_barplot.png')) )
```

Here is the model fit (in black) relative to the each survey item. We still don't achieve a good fit, I believe this must be coming from the temporal constraints imposed by participant effects across Baseline and Endline. It s puzzling though - there is no improvement relative to the first multi-group model.

```{r ordered_logistic_latent_factor-b_extract_class_probs_by_question, include=FALSE, eval=TRUE, echo=FALSE, tidy=FALSE, cache=TRUE}
po <- ordered_categorical_logit_fit_m5b$draws(
  variables = c('ordered_prob_by_obs'),
  inc_warmup = FALSE,
  format = "draws_df"
  )
po <- as.data.table(po)
po <- data.table::melt(po, id.vars = c('.draw','.chain','.iteration'), value.name = 'prob')
po[, value := as.integer(gsub('ordered_prob_by_obs\\[([0-9]),([0-9]+)\\]','\\1',variable)) ]
po[, oid := as.integer(gsub('ordered_prob_by_obs\\[([0-9]),([0-9]+)\\]','\\2',variable)) ]
set(po, NULL, 'variable', NULL)
tmp <- unique(subset(dcat1, select = c(oid, pid, vid, group )))
po <- merge(po, tmp, by = c('oid'))
po <- po[, list(prob = mean(prob)), by = c('.draw','group','vid','value')]

pos <- po[, 
          list( summary_value = quantile(prob, prob = c(0.025, 0.25, 0.5, 0.75, 0.975)),
                summary_name = c('q_lower','iqr_lower','median','iqr_upper','q_upper') 
                ), 
          by = c('group','vid','value')
          ]
pos <- data.table::dcast(pos, group + vid + value ~ summary_name, value.var = 'summary_value')

tmp <- unique( subset(dcat1, select = c(group, arm, arm_label, time, time_label, variable, vid, value, value_label)) )
pos <- merge(pos, tmp, by = c('group','vid','value'))

tmp <- dcat1[, list(n = length(pid)), by = c('arm_label','time_label','variable','value_label','value')]
tmp2 <- tmp[, list(total = sum(n)), by = c('arm_label','time_label','variable')]
tmp <- merge(tmp, tmp2, by = c('arm_label','time_label','variable'))
tmp[, p_emp := n/total]
pos <- merge(pos, subset(tmp, select = -c(value, n, total)), by = c('arm_label','time_label','variable','value_label'))

p <- ggplot(pos, aes(x = value_label, group = interaction(variable, value_label))) +
  geom_col(aes(fill = variable, y = p_emp), 
           position = position_dodge(0.9)
           ) +
  geom_boxplot( aes(ymin = q_lower, lower = iqr_lower, middle = median, upper = iqr_upper, ymax = q_upper), 
                position = position_dodge(0.9),
                stat = 'identity',
                alpha = 0,
                width = 0.3) +
  scale_fill_bmj() +
  scale_y_continuous(labels = scales::percent) +
  facet_grid(arm_label ~ time_label) +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 45,vjust = 1,hjust = 1),
        legend.position = 'bottom'
        ) +
  labs(x = '', x = 'Likert scale', y = 'proportion of outcomes', fill = '')
ggsave(file = file.path(dir.out, paste0(file.prefix,'m5b_frequencies_v2.png')), 
       plot = p, 
       h = 10, 
       w = 8
       )
```

```{r ordered_logistic_latent_factor-b_extract_class_probs_by_question_plot, include=TRUE, eval=TRUE, echo=FALSE, tidy=FALSE, fig.align='center', out.width='80%'}
knitr::include_graphics( file.path(dir.out, paste0(file.prefix,'m5b_frequencies_v2.png')) )
```

At this point, let us inspect Bayes-LOO-ELPD.


```{r PHQ4_latent_factor-b_compare_loo, include=FALSE, eval=TRUE, echo=FALSE, message=FALSE, tidy=FALSE, cache=TRUE}
poisson_log_likert_fit_m0 <- readRDS(file = file.path(dir.out, 'parentalmh_poisson_likert_m0_stan.rds'))
poisson_log_likert_fit_m1 <- readRDS(file = file.path(dir.out, 'parentalmh_poisson_likert_m1_stan.rds'))
ordered_categorical_logit_fit_m0 <- readRDS(file = file.path(dir.out, 'parentalmh_ordered_categorical_m0_stan.rds'))
ordered_categorical_logit_fit_m0a <- readRDS(file = file.path(dir.out, 'parentalmh_ordered_categorical_m0a_stan.rds'))
ordered_categorical_logit_fit <- readRDS(file = file.path(dir.out, 'parentalmh_ordered_categorical_m1_stan.rds'))
ordered_categorical_logit_fit_m4a <- readRDS(file = file.path(dir.out, 'parentalmh_ordered_categorical_m4a_stan.rds'))
ordered_categorical_logit_fit_m4 <- readRDS(file = file.path(dir.out, 'parentalmh_ordered_categorical_m4_stan.rds'))
ordered_categorical_logit_fit_m5a <- readRDS(file = file.path(dir.out, 'parentalmh_ordered_categorical_m5a_stan.rds'))
ordered_categorical_logit_fit_m5 <- readRDS(file = file.path(dir.out, 'parentalmh_ordered_categorical_m5_stan.rds'))
ordered_categorical_logit_fit_m5b <- readRDS(file = file.path(dir.out, 'parentalmh_ordered_categorical_m5b_stan.rds'))

poisson_log_likert_fit_m0_loo <- poisson_log_likert_fit_m0$loo()
poisson_log_likert_fit_m1_loo <- poisson_log_likert_fit_m1$loo()
ordered_categorical_logit_fit_m0_loo <- ordered_categorical_logit_fit_m0$loo()
ordered_categorical_logit_fit_m0a_loo <- ordered_categorical_logit_fit_m0a$loo()
ordered_categorical_logit_fit_loo <- ordered_categorical_logit_fit$loo()
ordered_categorical_logit_fit_m4a_loo <- ordered_categorical_logit_fit_m4a$loo()
ordered_categorical_logit_fit_m4_loo <- ordered_categorical_logit_fit_m4$loo()
ordered_categorical_logit_fit_m5a_loo <- ordered_categorical_logit_fit_m5a$loo()
ordered_categorical_logit_fit_m5_loo <- ordered_categorical_logit_fit_m5$loo()
ordered_categorical_logit_fit_m5b_loo <- ordered_categorical_logit_fit_m5b$loo()
```

We clearly see that the Ordered Categorical 1D latent factor models explain the data better 
than all other models. Considering the standard error around ELPD differences between models,
we also see that none of the 1D latent factor models is clearly better than any of the other
two in terms of ELPD.

```{r PHQ4_latent_factor-b_compare_loo_table, include=TRUE, eval=TRUE, echo=TRUE, tidy=FALSE, cache=TRUE}
# Model 1 - Poisson no participant effects
# Model 2 - Poisson with participant random effects
# Model 3 - Textbook Ordered Cat, no participant effects
# Model 4 - Multi-group Ordered Cat, no participant effects
# Model 5 - Multi-group Ordered Cat, with participant random effects
# Model 6 - Textbook Ordered Cat with response correlations, no participant effects
# Model 7 - Multi-group Ordered Cat with response correlations, no participant effects
# Model 8 - Textbook 1D Latent factor model
# Model 9 - First Multi-group 1D Latent factor model
# Model 10 - Second Multi-group 1D Latent factor model
comp <- loo::loo_compare(poisson_log_likert_fit_m0_loo,
                         poisson_log_likert_fit_m1_loo,
                         ordered_categorical_logit_fit_m0a_loo,
                         ordered_categorical_logit_fit_m0_loo, 
                         ordered_categorical_logit_fit_loo,
                         ordered_categorical_logit_fit_m4a_loo,
                         ordered_categorical_logit_fit_m4_loo,
                         ordered_categorical_logit_fit_m5a_loo,
                         ordered_categorical_logit_fit_m5_loo,
                         ordered_categorical_logit_fit_m5b_loo
                         )
print(comp, simplify = FALSE)
```

# VAC - Ordered Cat model

Let us move to the VAC data, and repeat the main steps above. Here are the survey responses to each of the 10 items targeting latent behavior 'violence against children (VAC)' across participants:

```{r vac_data, include=TRUE, eval=TRUE, message=FALSE, echo=FALSE, warning=FALSE, tidy=TRUE}
# Select ordered categorical outcome
dcat3 <- subset(dp, vid > 5 & variable_type == 'likert')
file.prefix <- 'vac_ordered_categorical_'

# Preprocessing
dcat3 <- dcat3[order(pid, arm, time, variable)]
dcat3[, oid := seq_len(nrow(dcat3))]
set(dcat3, NULL, 'value', dcat3[, value + 1L])
set(dcat3, NULL, 'arm', dcat3[, arm + 1L])
set(dcat3, NULL, 'time', dcat3[, time + 1L])
set(dcat3, NULL, 'group', dcat3[, max(arm) * (time - 1L) + arm])
set(dcat3, NULL, 'vid', NULL)
tmp <- unique(subset(dcat3, select = c('variable')))
tmp[, vid := 1:nrow(tmp)]
dcat3 <- merge(dcat3, tmp, by = 'variable')

tmp <- data.table(value = 1:4,
                  value_label = factor(1:4, 
                                       levels = 1:4, 
                                       labels = c('Not at all','Several days','More than half the days','Nearly every day')
                                       )
                  )
dcat3 <- merge(dcat3, tmp, by = 'value')
dcat3 <- dcat3[order(pid, arm, time, variable)]

# Frequency plot
tmp <- dcat3[, list(n = length(pid)), by = c('arm_label','time_label','variable','value_label', 'value')]
tmp2 <- tmp[, list(total = sum(n)), by = c('arm_label','time_label','variable')]
tmp <- merge(tmp, tmp2, by = c('arm_label','time_label','variable'))
tmp[, p := n/total]

p <- ggplot(tmp, aes(x = value_label, y = n/total)) +
  geom_col(aes(fill = variable), position = position_dodge()) +
  scale_fill_simpsons() +
  facet_grid(arm_label ~ time_label) +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 45,vjust = 1,hjust = 1),
        legend.position = 'bottom'
        ) +
  labs(x = '', fill = 'Likert scale')
ggsave(file = file.path(dir.out, paste0(file.prefix,'frequencies.png')), 
       plot = p, 
       h = 10, 
       w = 12
       )
```

```{r vac_data_plot, include=TRUE, eval=TRUE, echo=FALSE, tidy=FALSE, fig.align='center', out.width='80%'}
knitr::include_graphics( file.path(dir.out, paste0(file.prefix,'frequencies.png')) )
```

Polychoric correclations among item responses in the same group are again very high:

```{r vac_data_explore_clustering1, include=TRUE, eval=TRUE, echo=TRUE, message=FALSE, tidy=FALSE, cache=TRUE}
tmp <- data.table:::dcast( dcat3, time_label + arm_label + pid ~ variable, value.var = 'value')
tmp <- melt(tmp, id.vars = c('time_label','arm_label','pid','VACE_threat'))
tmp[,
    {
      z <- polycor::polychor(VACE_threat, value, std.err = TRUE, ML = FALSE)
      list(polchor = z$rho, polychor_sd = sqrt(as.numeric(z$var))) 
    }, 
    by = c('arm_label','time_label')
    ]
```

## Flexible Multi-Group Ordered Categorial Model without participant effects

Let's fit the flexible Multi-Group Model without participant effects first.

```{r vac_ordered_logistic_no_unit_effects_run_model, include=TRUE, eval=TRUE, echo=TRUE, message=FALSE, tidy=FALSE, cache=TRUE}
# define data in format needed for model specification
stan_data <- list()
stan_data$N <- nrow(dcat3)
stan_data$P <- max(dcat3$group)
stan_data$K <- length(unique(dcat3$value))
stan_data$y <- dcat3$value
stan_data$group_of_obs <- dcat3$group

# sample
vac_ordered_categorical_logit_fit_m0 <- ordered_categorical_logit_compiled_m0$sample(
  data = stan_data,
  seed = 123,
  chains = 2,
  parallel_chains = 2,
  iter_warmup = 5e2,
  iter_sampling = 15e2,
  refresh = 500,
  save_warmup = TRUE
)

# save output to RDS
vac_ordered_categorical_logit_fit_m0$save_object(file = file.path(dir.out, paste0(file.prefix,"m0_stan.rds")))
```

```{r vac_ordered_logistic_no_unit_effects_load_model, include=FALSE, eval=TRUE, echo=FALSE, tidy=FALSE, cache=TRUE}
vac_ordered_categorical_logit_fit_m0 <- readRDS(file = file.path(dir.out, paste0(file.prefix,'m0_stan.rds')))
```

```{r vac_ordered_logistic_no_unit_effects_check mixing convergence, include=FALSE, eval=TRUE, echo=FALSE, tidy=FALSE, cache=TRUE}
#	check convergence and check mixing
#	get 95% credible intervals
tmp <- vac_ordered_categorical_logit_fit_m0$summary(
  variables = c('beta_group','cut_point_1','cut_point_gaps_groups'),
  posterior::default_summary_measures(),
  posterior::default_convergence_measures(),
  extra_quantiles = ~posterior::quantile2(., probs = c(.0275, .975))
)
tmp <- as.data.table(tmp)
tmp
  
# parameter with lowest ess_bulk
worst_var <- tmp$variable[ which.min(tmp$ess_bulk) ]

# extract samples
po <- vac_ordered_categorical_logit_fit_m0$draws(
  variables = c("lp__", worst_var),
  inc_warmup = TRUE,
  format = "draws_array"
  )

# make worst trace plot
p <- bayesplot:::mcmc_trace(po,  
                            pars = c("lp__",worst_var), 
                            n_warmup = 500,
                            facet_args = list(nrow = 2)
                            )
p <- p + theme_bw()
ggsave(file = file.path(dir.out, paste0(file.prefix,'m0_worsttrace.png')), 
       plot = p, 
       h = 10, 
       w = 8
       )

# make pairs plot
po <- vac_ordered_categorical_logit_fit_m0$draws(
  variables = c('beta_group','cut_point_1','cut_point_gaps_groups'),
  inc_warmup = FALSE,
  format = "draws_array"
  )

color_scheme_set("teal")
p <- bayesplot::mcmc_intervals(po, prob = 0.5, prob_outer = 0.95, outer_size = 1, point_size = 2) +
  theme_bw()
ggsave(file = file.path(dir.out, paste0(file.prefix,'m0_intervals.png')), 
       plot = p, 
       h = 7, 
       w = 8, 
       limitsize = FALSE
       )

po <- vac_ordered_categorical_logit_fit_m0$draws(
  variables = c('beta_group','cut_point_1','cut_point_gaps_groups'),
  inc_warmup = FALSE,
  format = "draws_array"
  )

bayesplot::color_scheme_set('viridisC')
p <- bayesplot::mcmc_pairs(po, 
                           diag_fun = "dens", 
                           off_diag_fun = "hex"
                           ) 
ggsave(file = file.path(dir.out, paste0(file.prefix,'m0_pairsplot.png')), 
       plot = p, 
       h = 30, 
       w = 30, 
       limitsize = FALSE
       )
bayesplot::color_scheme_set('brewer-RdYlBu')

# make posterior predictive check

# create median and 95\% credible intervals
po <- vac_ordered_categorical_logit_fit_m0$draws(
  variables = c('ypred'),
  inc_warmup = FALSE,
  format = "draws_df"
  )
po <- as.data.table(po)
po <- data.table::melt(po, id.vars = c('.draw','.chain','.iteration'), value.name = 'ypred')
po[, oid := as.integer(gsub('ypred\\[([0-9]+)\\]','\\1',variable)) ]
pos <- 
  po[,
     list( summary_value = quantile(ypred, prob = c(0.025, 0.25, 0.5, 0.75, 0.975)),
           summary_name = c('q_lower','iqr_lower','median','iqr_upper','q_upper') 
           ),
     by = 'oid'
     ]
pos <- 
  data.table::dcast(pos,
                    oid ~ summary_name, 
                    value.var = 'summary_value'
                    )
pos <- merge(pos, dcat3, by = 'oid')
pos[, IN_PPI := value >= q_lower & value <= q_upper]
pos[, mean(IN_PPI)]

# plot posterior predictive check
p <- ggplot(pos, aes(x = oid, group = oid)) + 
  geom_boxplot( aes( ymin = q_lower,
                     lower = iqr_lower,
                     middle = median,
                     upper = iqr_upper,
                     ymax = q_upper),
                stat = 'identity') +
  geom_point( aes(y = value, colour = IN_PPI ) ) +
  facet_grid(variable ~ arm_label+time_label, scales = 'free') +
  scale_x_discrete() +
  scale_y_continuous() +
  ggsci::scale_color_npg() +
  labs(x = '', 
       y = 'outcome', 
       colour = 'within\n95% posterior\nprediction\ninterval') +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 45,vjust = 1,hjust = 1))
ggsave(file = file.path(dir.out, paste0(file.prefix,'m0_ppcheck.png')), 
       p, 
       w = 12, 
       h = 12)
```

```{r vac_ordered_logistic_no_unit_effects_extract_class_probs, include=FALSE, eval=TRUE, echo=FALSE, tidy=FALSE, cache=TRUE}
# extract model probabilities
po <- vac_ordered_categorical_logit_fit_m0$draws(
  variables = c('ordered_prob'),
  inc_warmup = FALSE,
  format = "draws_df"
  )
po <- as.data.table(po)
setnames(po, colnames(po), gsub('\\]','',gsub('\\[','_',colnames(po))))
po <- data.table::melt(po, id.vars = c('.draw','.chain','.iteration'), value.name = 'prob')
po[, value := as.integer(gsub('ordered_prob_([0-9]),([0-9])','\\1',variable)) ]
po[, group := as.integer(gsub('ordered_prob_([0-9]),([0-9])','\\2',variable)) ]

tmp <- unique( subset(dcat3, select = c(group, arm, arm_label, time, time_label, value, value_label)) )
po <- merge(po, tmp, by = c('group','value'))
```

```{r vac_ordered_logistic_no_unit_effects_make_plots, include=FALSE, eval=TRUE, echo=FALSE, tidy=FALSE, cache=TRUE}
pos <- po[, 
          list( summary_value = quantile(prob, prob = c(0.025, 0.25, 0.5, 0.75, 0.975)),
                summary_name = c('q_lower','iqr_lower','median','iqr_upper','q_upper') 
                ), 
          by = c('group','arm','arm_label','time','time_label','value','value_label')
          ]
pos <- data.table::dcast(pos, group + arm + arm_label + time + time_label + value + value_label ~ summary_name, value.var = 'summary_value')

tmp <- dcat3[, list(n = length(pid)), by = c('arm_label','time_label','value_label','value')]
tmp2 <- tmp[, list(total = sum(n)), by = c('arm_label','time_label')]
tmp <- merge(tmp, tmp2, by = c('arm_label','time_label'))
tmp[, p_emp := n/total]
pos <- merge(pos, subset(tmp, select = -c(value, n, total)), by = c('arm_label','time_label','value_label'))

p <- ggplot(data = pos, aes( x = value_label, fill = interaction(arm_label,time_label))) +
  geom_bar(aes( y = p_emp), 
           stat = 'identity', 
           position = position_dodge()) +
  geom_boxplot( aes(ymin = q_lower, lower = iqr_lower, middle = median, upper = iqr_upper, ymax = q_upper), 
                 position = position_dodge(0.9),
                stat = 'identity',
                alpha = 0,
                width = 0.3,
                outliers = FALSE) +
  scale_fill_futurama() +
  scale_y_continuous(labels = scales::percent) +
  labs(x = '\nViolence against children\nVAC outcome', y = 'group-level\nposterior probability', fill = '') +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 45,vjust = 1,hjust = 1),
        legend.position = 'bottom'
        )
ggsave(file = file.path(dir.out, paste0(file.prefix,'m0_probs_barplot.png')), 
       plot = p, 
       h = 8, 
       w = 8
       )
```

This is the model fit (in black) relative to the aggregated survey items. The model is flexible enough to capture the proportions for each Likert outcome and each group exactly.

```{r vac_ordered_logistic_no_unit_effects_plot, include=TRUE, eval=TRUE, echo=FALSE, tidy=FALSE, fig.align='center', out.width='80%'}
knitr::include_graphics( file.path(dir.out, paste0(file.prefix,'m0_probs_barplot.png')) )
```


```{r vac_ordered_logistic_no_unit_effects_make_plots_2, include=FALSE, eval=TRUE, echo=FALSE, tidy=FALSE, cache=TRUE}
# Frequency plot
tmp <- dcat3[, list(n = length(pid)), by = c('arm_label','time_label','variable','value_label', 'value')]
tmp2 <- tmp[, list(total = sum(n)), by = c('arm_label','time_label','variable')]
tmp <- merge(tmp, tmp2, by = c('arm_label','time_label','variable'))
tmp[, p_emp_by_variable := n/total]


p <- ggplot(tmp, aes(x = value_label)) +
  geom_col(aes(fill = variable, y = p_emp_by_variable), position = position_dodge()) +
  geom_point(data = pos, aes(y = median), pch = 18, size = 2, colour = 'black') +
  geom_linerange(data = pos, aes(ymin = q_lower, ymax = q_upper), colour = 'black') +
  scale_fill_simpsons() +
  scale_y_continuous(labels = scales::percent) +
  facet_grid(arm_label ~ time_label) +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 45,vjust = 1,hjust = 1),
        legend.position = 'bottom'
        ) +
  labs(x = '', x = 'Likert scale', y = 'proportion of outcomes', fill = '')
ggsave(file = file.path(dir.out, paste0(file.prefix,'m0_frequencies_v2.png')), 
       plot = p, 
       h = 10, 
       w = 8
       )
```

Below is the model fit (in black) relative to each survey item. Again, there remains unexplained heterogeneity across each 
survey item, and we consider this okay because each question elicits the underlying target behaviour in a slightly 
different way. We note that the outcome frequencies are bimodal and increasing for the 'Nearly every day' category, particularly
in the Control group. The Categorical model can match these trends, and it is already clear that a Poisson model will fail in matching
these trends.

```{r vac_ordered_logistic_no_unit_effects_plot_2, include=TRUE, eval=TRUE, echo=FALSE, tidy=FALSE, fig.align='center', out.width='80%'}
knitr::include_graphics( file.path(dir.out, paste0(file.prefix,'m0_frequencies_v2.png')) )
```

## Multi-Group Ordered Categorical Model with participant effects

Let's now fit theMulti-Group Model with participant effects.

```{r vac_ordered_logistic_with_unit_effects_run_model, include=TRUE, eval=TRUE, echo=TRUE, message=FALSE, tidy=FALSE, cache=TRUE}
# define data in format needed for model specification
stan_data <- list()
stan_data$N <- nrow(dcat3)
stan_data$P <- max(dcat3$group)
stan_data$U <- max(dcat3$pid)
stan_data$K <- length(unique(dcat3$value))
stan_data$y <- dcat3$value
stan_data$group_of_obs <- dcat3$group
stan_data$unit_of_obs <- dcat3$pid

# sample
vac_ordered_categorical_logit_fit <- ordered_categorical_logit_compiled_m1$sample(
  data = stan_data,
  seed = 123,
  chains = 2,
  parallel_chains = 2,
  iter_warmup = 5e2,
  iter_sampling = 15e2,
  refresh = 500,
  save_warmup = TRUE
)

# save output to RDS
vac_ordered_categorical_logit_fit$save_object(file = file.path(dir.out, paste0(file.prefix,"m1_stan.rds")))
```


```{r vac_ordered_logistic_with_unit_effects_load_model, include=FALSE, eval=TRUE, echo=FALSE, tidy=FALSE, cache=TRUE}
vac_ordered_categorical_logit_fit <- readRDS(file = file.path(dir.out, paste0(file.prefix,'m1_stan.rds')))
```


```{r vac_ordered_logistic_with_unit_effects_check_mixing_convergence, include=FALSE, eval=TRUE, echo=FALSE, tidy=FALSE, cache=TRUE}
#	check convergence and check mixing
#	get 95% credible intervals
tmp <- vac_ordered_categorical_logit_fit$summary(
  variables = c('beta_group','beta_unit','beta_unit_sd','cut_point_1','cut_point_gaps_groups'),
  posterior::default_summary_measures(),
  posterior::default_convergence_measures(),
  extra_quantiles = ~posterior::quantile2(., probs = c(.0275, .975))
)
tmp <- as.data.table(tmp)
tmp
  
# parameter with lowest ess_bulk
worst_var <- tmp$variable[ which.min(tmp$ess_bulk) ]

# extract samples
po <- vac_ordered_categorical_logit_fit$draws(
  variables = c("lp__", worst_var),
  inc_warmup = TRUE,
  format = "draws_array"
  )

# make worst trace plot
p <- bayesplot:::mcmc_trace(po,  
                            pars = c("lp__",worst_var), 
                            n_warmup = 500,
                            facet_args = list(nrow = 2)
                            )
p <- p + theme_bw()
ggsave(file = file.path(dir.out, paste0(file.prefix,'m1_worsttrace.png')), 
       plot = p, 
       h = 10, 
       w = 8
       )

# make intervals plot
po <- vac_ordered_categorical_logit_fit$draws(
  variables = c('beta_group','beta_group','cut_point_1','cut_point_gaps_groups','beta_unit_sd','beta_unit'),
  inc_warmup = FALSE,
  format = "draws_array"
  )

color_scheme_set("teal")
p <- bayesplot::mcmc_intervals(po, prob = 0.5, prob_outer = 0.95, outer_size = 1, point_size = 2) +
  theme_bw()
ggsave(file = file.path(dir.out, paste0(file.prefix,'m1_intervals.png')), 
       plot = p, 
       h = 35, 
       w = 8, 
       limitsize = FALSE
       )

# make pairs plot
po <- vac_ordered_categorical_logit_fit$draws(
  variables = c('beta_group','cut_point_1','cut_point_gaps_groups'),
  inc_warmup = FALSE,
  format = "draws_array"
  )

bayesplot::color_scheme_set('viridisC')
p <- bayesplot::mcmc_pairs(po, 
                           diag_fun = "dens", 
                           off_diag_fun = "hex"
                           ) 
ggsave(file = file.path(dir.out, paste0(file.prefix,'m1_pairsplot.png')), 
       plot = p, 
       h = 30, 
       w = 30, 
       limitsize = FALSE
       )
bayesplot::color_scheme_set('brewer-RdYlBu')

# make posterior predictive check

# create median and 95\% credible intervals
po <- vac_ordered_categorical_logit_fit$draws(
  variables = c('ypred'),
  inc_warmup = FALSE,
  format = "draws_df"
  )
po <- as.data.table(po)
po <- data.table::melt(po, id.vars = c('.draw','.chain','.iteration'), value.name = 'ypred')
po[, oid := as.integer(gsub('ypred\\[([0-9]+)\\]','\\1',variable)) ]
pos <- 
  po[,
     list( summary_value = quantile(ypred, prob = c(0.025, 0.25, 0.5, 0.75, 0.975)),
           summary_name = c('q_lower','iqr_lower','median','iqr_upper','q_upper') 
           ),
     by = 'oid'
     ]
pos <- 
  data.table::dcast(pos,
                    oid ~ summary_name, 
                    value.var = 'summary_value'
                    )
pos <- merge(pos, dcat3, by = 'oid')
pos[, IN_PPI := value >= q_lower & value <= q_upper]

# plot posterior predictive check
p <- ggplot(pos, aes(x = oid, group = oid)) + 
  geom_boxplot( aes( ymin = q_lower,
                     lower = iqr_lower,
                     middle = median,
                     upper = iqr_upper,
                     ymax = q_upper),
                stat = 'identity') +
  geom_point( aes(y = value, colour = IN_PPI ) ) +
  facet_grid(variable ~ arm_label+time_label, scales = 'free') +
  scale_x_discrete() +
  scale_y_continuous() +
  ggsci::scale_color_npg() +
  labs(x = '', 
       y = 'outcome', 
       colour = 'within\n95% posterior\nprediction\ninterval') +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 45,vjust = 1,hjust = 1))
ggsave(file = file.path(dir.out, paste0(file.prefix,'m1_ppcheck.png')), 
       p, 
       w = 12, 
       h = 12)
```

```{r vac_ordered_logistic_with_unit_effects_extract_participant_effects, include=FALSE, eval=TRUE, echo=FALSE, tidy=FALSE, cache=TRUE}
# extract participant effects
po <- vac_ordered_categorical_logit_fit$draws(
  variables = c('beta_unit_sd','beta_unit'),
  inc_warmup = FALSE,
  format = "draws_df"
  )
po <- as.data.table(po)
po <- data.table::melt(po, id.vars = c('.draw','.chain','.iteration'))
tmp <- subset(po, variable == 'beta_unit_sd')
tmp <- data.table::dcast(tmp, .draw ~ variable, value.var = 'value')
po <- subset(po, variable != 'beta_unit_sd')
po <- merge(po, tmp, by = '.draw')
po[, pid := as.integer(gsub('beta_unit\\[([0-9]+)\\]','\\1',variable)) ]
po[, value := value * beta_unit_sd]

pos <- po[, 
          list( summary_value = quantile(value, prob = c(0.025, 0.25, 0.5, 0.75, 0.975)),
                summary_name = c('q_lower','iqr_lower','median','iqr_upper','q_upper') 
                ), 
          by = c('pid')
          ]
pos <- data.table::dcast(pos, pid ~ summary_name, value.var = 'summary_value')
tmp <- unique(subset(dcat3, select = c(pid, arm_label, time_label)))
pos <- merge(pos, tmp, by = c('pid'))
tmp <- dcat3[, list(avg_value = mean(value)), by = 'pid']
tmp[, avg_value_jitter := avg_value + runif(nrow(tmp), min = -1/8, max = 1/8)]
pos <- merge(pos, tmp, by = 'pid')

tmp <- pos[, list(avg_effect = mean(median)), by = c('arm_label','time_label')]

p <- ggplot(data = pos, aes( x = avg_value_jitter)) +
  geom_hline(data = tmp, aes( yintercept = avg_effect), colour = 'grey50') +
  geom_linerange(aes( ymin = q_lower, ymax = q_upper), 
           stat = 'identity') +
  geom_point(aes( y = median)) +
  labs(x = '\naverage response of participant', y = 'participant effect') +
  theme_bw() +
  facet_grid(arm_label ~ time_label, scales = 'free') +
  theme(axis.text.x = element_text(angle = 45,vjust = 1,hjust = 1),
        legend.position = 'bottom'
        )
ggsave(file = file.path(dir.out, paste0(file.prefix,'m1_participant_effects.png')), 
       plot = p, 
       h = 8, 
       w = 12
       )
```

We see once more distinct participant-level effects that define participant-level latent factors:

```{r vac_ordered_logistic_with_unit_effects_extract_participant_effects_plot, include=TRUE, eval=TRUE, echo=FALSE, tidy=FALSE, fig.align='center', out.width='80%'}
knitr::include_graphics( file.path(dir.out, paste0(file.prefix,'m1_participant_effects.png')) )
```


```{r vac_ordered_logistic_with_unit_effects_make_class_probs_pair_plot, include=FALSE, eval=TRUE, echo=FALSE, tidy=FALSE, cache=TRUE}
po <- vac_ordered_categorical_logit_fit$draws(
  variables = c('ordered_prob_by_obs'),
  inc_warmup = FALSE,
  format = "draws_df"
  )
po <- as.data.table(po)
po <- data.table::melt(po, id.vars = c('.draw','.chain','.iteration'), value.name = 'prob')
po[, value := as.integer(gsub('ordered_prob_by_obs\\[([0-9]),([0-9]+)\\]','\\1',variable)) ]
po[, oid := as.integer(gsub('ordered_prob_by_obs\\[([0-9]),([0-9]+)\\]','\\2',variable)) ]
set(po, NULL, 'variable', NULL)
tmp <- unique(subset(dcat3, select = c(oid, pid, vid, group )))
po <- merge(po, tmp, by = c('oid'))
po <- po[, list(prob = mean(prob)), by = c('.draw','group','value')]

pos <- po[, 
          list( summary_value = quantile(prob, prob = c(0.025, 0.25, 0.5, 0.75, 0.975)),
                summary_name = c('q_lower','iqr_lower','median','iqr_upper','q_upper') 
                ), 
          by = c('group','value')
          ]
pos <- data.table::dcast(pos, group + value ~ summary_name, value.var = 'summary_value')

tmp <- unique( subset(dcat3, select = c(group, arm, arm_label, time, time_label, value, value_label)) )
pos <- merge(pos, tmp, by = c('group','value'))

tmp <- dcat3[, list(n = length(pid)), by = c('arm_label','time_label','value_label','value')]
tmp2 <- tmp[, list(total = sum(n)), by = c('arm_label','time_label')]
tmp <- merge(tmp, tmp2, by = c('arm_label','time_label'))
tmp[, p_emp := n/total]
pos <- merge(pos, subset(tmp, select = -c(value, n, total)), by = c('arm_label','time_label','value_label'))


p <- ggplot(data = pos, aes( x = value_label, fill = interaction(arm_label,time_label))) +
  geom_bar(aes( y = p_emp), 
           stat = 'identity', 
           position = position_dodge()) +
  geom_boxplot( aes(ymin = q_lower, lower = iqr_lower, middle = median, upper = iqr_upper, ymax = q_upper), 
                 position = position_dodge(0.9),
                stat = 'identity',
                alpha = 0,
                width = 0.3,
                outliers = FALSE) +
  scale_fill_futurama() +
  scale_y_continuous(labels = scales::percent) +
  labs(x = '\nViolence against children\nVAC outcome', y = 'group-level\nposterior probability', fill = '') +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 45,vjust = 1,hjust = 1),
        legend.position = 'bottom'
        )
ggsave(file = file.path(dir.out, paste0(file.prefix,'m1_probs_barplot.png')), 
       plot = p, 
       h = 8, 
       w = 8
       )
```

This is the model fit (in black) relative to the aggregated survey items. In this model, the participant level effects systematically influence
the group-level outcome probabilities.

```{r vac_ordered_logistic_with_unit_effects_plot, include=TRUE, eval=TRUE, echo=FALSE, tidy=FALSE, fig.align='center', out.width='80%'}
knitr::include_graphics( file.path(dir.out, paste0(file.prefix,'m1_probs_barplot.png')) )
```


```{r vac_ordered_logistic_with_unit_effects_polychoric_corrs, include=FALSE, eval=TRUE, echo=FALSE, tidy=FALSE, cache=TRUE}
po <- vac_ordered_categorical_logit_fit$draws(
  variables = c('ypred'),
  inc_warmup = FALSE,
  format = "draws_df"
  )
po <- as.data.table(po)
po <- data.table::melt(po, id.vars = c('.draw','.chain','.iteration'), value.name = 'value')
po[, oid := as.integer(gsub('ypred\\[([0-9]+)\\]','\\1',variable)) ]
set(po, NULL, 'variable', NULL)
tmp <- unique(subset(dcat3, select = c(oid, pid, variable, group )))
po <- merge(po, tmp, by = c('oid'))
tmp <- unique(subset(dcat3, select = c(value, value_label )))
po <- merge(po, tmp, by = c('value'))
po <- data.table::dcast( po, .draw + pid + group ~ variable, value.var = 'value_label')
po <- melt(po, id.vars = c('.draw','pid','group','VACE_threat'), value.name = 'value_label')
po <- po[, list(k = length(pid)), by = c('.draw', 'group', 'VACE_threat', 'value_label')]
tmp <- po[, list(n = sum(k)), by = c('.draw', 'group', 'VACE_threat')]
po <- merge(po, tmp, by = c('.draw', 'group', 'VACE_threat'))
po[, prob := k/n]
tmp <- CJ(.draw = unique(po$.draw),
          group = unique(po$group), 
          VACE_threat = unique(po$VACE_threat), 
          value_label = unique(po$VACE_threat))
po <- merge(tmp, po, by = c('.draw','group','VACE_threat','value_label'), all.x = TRUE)
set(po, po[, which(is.na(prob))], 'prob', 0.)
pos <- po[, 
          list( summary_value = quantile(prob, prob = c(0.025, 0.25, 0.5, 0.75, 0.975)),
                summary_name = c('q_lower','iqr_lower','median','iqr_upper','q_upper') 
                ), 
          by = c('group','VACE_threat','value_label')
          ]
pos <- data.table::dcast(pos, group + VACE_threat + value_label ~ summary_name, value.var = 'summary_value')
tmp <- unique( subset(dcat3, select = c(group, arm, arm_label, time, time_label)) )
pos <- merge(pos, tmp, by = c('group'))

tmp <- data.table:::dcast( dcat3, time_label + arm_label + pid ~ variable, value.var = 'value_label')
tmp <- melt(tmp, id.vars = c('time_label','arm_label','pid','VACE_threat'))
tmp <- tmp[, list(k = length(pid)), by = c('time_label','arm_label','VACE_threat','value')]
tmp2 <- tmp[, list(n = sum(k)), by = c('time_label','arm_label','VACE_threat')]
tmp <- merge(tmp, tmp2, by = c('time_label','arm_label','VACE_threat'))
tmp[, p_emp := k/n]
set(tmp, NULL, c('k','n'), NULL)
tmp2 <- CJ(time_label = unique(tmp$time_label), 
           arm_label = unique(tmp$arm_label), 
           VACE_threat = unique(tmp$VACE_threat), 
           value = unique(tmp$VACE_threat))
tmp <- merge(tmp2, tmp, by = c('arm_label','time_label','VACE_threat','value'), all.x = TRUE)
set(tmp, tmp[, which(is.na(p_emp))], 'p_emp', 0.)
setnames(tmp, 'value','value_label')
pos <- merge(pos, tmp, by = c('arm_label','time_label','VACE_threat','value_label'), all.y = TRUE)
pos[, value_label := factor(value_label)]

p <- ggplot(pos, aes( x = VACE_threat, group = interaction(VACE_threat, value_label))) +
  geom_bar(aes(y = p_emp, fill = value_label), 
           stat = 'identity',
           position = position_dodge(0.9)) +
  geom_boxplot( aes(ymin = q_lower, lower = iqr_lower, middle = median, upper = iqr_upper, ymax = q_upper), 
                position = position_dodge(0.9),
                stat = 'identity',
                alpha = 0,
                width = 0.3) +
  scale_fill_bmj() +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 45,vjust = 1,hjust = 1),
        legend.position = 'bottom'
        ) +
  labs(x = 'response to VACE_threat question', y = 'proportion', fill = 'responses to other VAC questions') +
  facet_grid(arm_label ~ time_label)
ggsave(file = file.path(dir.out, paste0(file.prefix,'m1_polychoric_correlations_across_vac_questions_same_arm-time.png')),
       plot = p, 
       h = 8, 
       w = 8
       )
```
Here is a visualisation of the estimated polychoric correlations. We can clearly see that with participant effects, the model is already able to capture correlations between item responses.

```{r vac_ordered_logistic_no_unit_effects_polychoric_corrs_plot, include=TRUE, eval=TRUE, echo=FALSE, tidy=FALSE, fig.align='center', out.width='80%'}
knitr::include_graphics( file.path(dir.out, paste0(file.prefix,'m1_polychoric_correlations_across_vac_questions_same_arm-time.png')) )
```

# VAC - Poisson model

I will next consider the VAC Likert outcomes as if they were counts and aggregate these across all itemized questions that target the same behaviour.

```{r Subset data for vac poisson models, include=FALSE, eval=TRUE, message=FALSE, echo=FALSE, warning=FALSE, tidy=TRUE}
# Select ordered categorical outcome
dcat4 <- subset(dp, vid == 8)
file.prefix <- 'vac_poisson_aggregated_'

# Preprocessing
dcat4 <- dcat4[order(pid, arm, time, variable)]
dcat4[, oid := seq_len(nrow(dcat4))]
set(dcat4, NULL, 'arm', dcat4[, arm + 1L])
set(dcat4, NULL, 'time', dcat4[, time + 1L])
set(dcat4, NULL, 'group', dcat4[, max(arm) * (time - 1L) + arm])

# Frequency plot
tmp <- dcat4[, list(n = length(pid)), by = c('arm_label','time_label','value')]

p <- ggplot(tmp, aes(x = value, y = n)) +
  geom_col(aes(fill = interaction(arm_label,time_label)), position = position_dodge()) +
  scale_fill_futurama() +
  facet_grid(arm_label ~ time_label) +
  theme_bw() +
  theme(legend.position = 'bottom') +
  labs(x = '', fill = 'counts of Likert outcomes')
ggsave(file = file.path(dir.out, paste0(file.prefix,'frequencies.png')), 
       plot = p, 
       h = 10, 
       w = 8
       )
```

```{r Subset data for vac poisson models_plot, include=TRUE, eval=TRUE, echo=FALSE, tidy=FALSE, fig.align='center', out.width='80%'}
knitr::include_graphics( file.path(dir.out, paste0(file.prefix,'frequencies.png')) )
```



## Model without participant effects


Let us fit the Poisson model.

```{r vac_agg_poisson_without_unit_effects_run_model, include=FALSE, eval=TRUE, echo=FALSE, message=FALSE, tidy=FALSE, cache=TRUE}
# define data in format needed for model specification
stan_data <- list()
stan_data$N <- nrow(dcat4)
stan_data$K <- max(dcat4$value)
stan_data$P <- max(dcat4$group)
stan_data$y <- dcat4$value
stan_data$group_of_obs <- dcat4$group

# sample
vac_poisson_log_fit_m0 <- poisson_log_compiled_m0$sample(
  data = stan_data,
  seed = 123,
  chains = 2,
  parallel_chains = 2,
  iter_warmup = 5e2,
  iter_sampling = 15e2,
  refresh = 500,
  init = list(list(beta_0 = 6, beta_groups_m1 = rep(0,3)), list(beta_0 = 6, beta_groups_m1 = rep(0,3))),
  save_warmup = TRUE
)

# save output to RDS
vac_poisson_log_fit_m0$save_object(file = file.path(dir.out, paste0(file.prefix,"m0_stan.rds")))
```

```{r vac_agg_poisson_without_unit_effects_check mixing convergence, include=FALSE, eval=TRUE, echo=FALSE, tidy=FALSE, cache=TRUE}
#	check convergence and check mixing
#	get 95% credible intervals
tmp <- vac_poisson_log_fit_m0$summary(
  variables = c('beta_groups_m1','beta_0'),
  posterior::default_summary_measures(),
  posterior::default_convergence_measures(),
  extra_quantiles = ~posterior::quantile2(., probs = c(.0275, .975))
)
tmp <- as.data.table(tmp)
tmp
  
# parameter with lowest ess_bulk
worst_var <- tmp$variable[ which.min(tmp$ess_bulk) ]

# extract samples
po <- vac_poisson_log_fit_m0$draws(
  variables = c("lp__", worst_var),
  inc_warmup = TRUE,
  format = "draws_array"
  )

# make worst trace plot
p <- bayesplot:::mcmc_trace(po,  
                            pars = c("lp__",worst_var), 
                            n_warmup = 500,
                            facet_args = list(nrow = 2)
                            )
p <- p + theme_bw()
ggsave(file = file.path(dir.out, paste0(file.prefix,'m0_worsttrace.png')), 
       plot = p, 
       h = 10, 
       w = 8
       )

# make intervals plot
po <- vac_poisson_log_fit_m0$draws(
  variables = c('beta_groups_m1','beta_group','beta_0'),
  inc_warmup = FALSE,
  format = "draws_array"
  )

color_scheme_set("teal")
p <- bayesplot::mcmc_intervals(po, prob = 0.5, prob_outer = 0.95, outer_size = 1, point_size = 2) +
  theme_bw()
ggsave(file = file.path(dir.out,paste0(file.prefix,'m0_intervals.png')), 
       plot = p, 
       h = 7, 
       w = 8, 
       limitsize = FALSE
       )

po <- vac_poisson_log_fit_m0$draws(
  variables = c('beta_groups_m1','beta_0'),
  inc_warmup = FALSE,
  format = "draws_array"
  )

bayesplot::color_scheme_set('viridisC')
p <- bayesplot::mcmc_pairs(po, 
                           diag_fun = "dens", 
                           off_diag_fun = "hex"
                           ) 
ggsave(file = file.path(dir.out,paste0(file.prefix,'m0_pairsplot.png')), 
       plot = p, 
       h = 30, 
       w = 30, 
       limitsize = FALSE
       )
bayesplot::color_scheme_set('brewer-RdYlBu')

# make posterior predictive check

# create median and 95\% credible intervals
po <- vac_poisson_log_fit_m0$draws(
  variables = c('ypred'),
  inc_warmup = FALSE,
  format = "draws_df"
  )
po <- as.data.table(po)
po <- data.table::melt(po, id.vars = c('.draw','.chain','.iteration'), value.name = 'ypred')
po[, oid := as.integer(gsub('ypred\\[([0-9]+)\\]','\\1',variable)) ]
pos <- 
  po[,
     list( summary_value = quantile(ypred, prob = c(0.025, 0.25, 0.5, 0.75, 0.975)),
           summary_name = c('q_lower','iqr_lower','median','iqr_upper','q_upper') 
           ),
     by = 'oid'
     ]
pos <- 
  data.table::dcast(pos,
                    oid ~ summary_name, 
                    value.var = 'summary_value'
                    )
pos <- merge(pos, dcat4, by = 'oid')
pos[, IN_PPI := value >= q_lower & value <= q_upper]
pos[, mean(IN_PPI)]

# plot posterior predictive check
p <- ggplot(pos, aes(x = oid, group = oid)) + 
  geom_boxplot( aes( ymin = q_lower,
                     lower = iqr_lower,
                     middle = median,
                     upper = iqr_upper,
                     ymax = q_upper),
                stat = 'identity') +
  geom_point( aes(y = value, colour = IN_PPI ) ) +
  facet_grid(variable ~ arm_label+time_label, scales = 'free') +
  scale_x_discrete() +
  scale_y_continuous() +
  ggsci::scale_color_npg() +
  labs(x = '', 
       y = 'outcome', 
       colour = 'within\n95% posterior\nprediction\ninterval') +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 45,vjust = 1,hjust = 1))
ggsave(file = file.path(dir.out, paste0(file.prefix,'m0_ppcheck.png')), 
       p, 
       w = 12, 
       h = 12)
```

```{r vac_agg_poisson_without_unit_effects_extract_class_probs, include=FALSE, eval=TRUE, echo=FALSE, tidy=FALSE, cache=TRUE}
# extract model probabilities
po <- vac_poisson_log_fit_m0$draws(
  variables = c('beta_0','beta_group'),
  inc_warmup = FALSE,
  format = "draws_df"
  )
po <- as.data.table(po)
po <- data.table::melt(po, id.vars = c('.draw','.chain','.iteration','beta_0'))
po[, group := as.integer(gsub('beta_group\\[([0-9])\\]','\\1',variable)) ]
po[, value := exp(beta_0 + value)]
po <- po[, list(value = 0:max(dcat4$value), pdf = dpois(0:max(dcat4$value), value)), by = c('.draw','group')]
tmp <- po[, list(total = sum(pdf)), by = c('.draw','group')]
po <- merge(po, tmp, by = c('.draw','group'))
po[, pdf := pdf/total]

pos <- 
  po[,
     list( summary_value = quantile(pdf, prob = c(0.025, 0.25, 0.5, 0.75, 0.975)),
           summary_name = c('q_lower','iqr_lower','median','iqr_upper','q_upper') 
           ),
     by = c('group','value')
     ]
pos <- 
  data.table::dcast(pos,
                    group + value ~ summary_name, 
                    value.var = 'summary_value'
                    )

tmp <- unique( subset(dcat4, select = c(group, arm, arm_label, time, time_label)) )
pos <- merge(pos, tmp, by = c('group'))

# Add empirical frequencies
tmp <- dcat4[, list(n = length(pid)), by = c('arm_label','time_label','value')]
tmp2 <- tmp[, list(total = sum(n)), by = c('arm_label','time_label')]
tmp <- merge(tmp, tmp2, by = c('arm_label','time_label'))
tmp[, p_emp := n/total]
pos <- merge(pos, subset(tmp, select = c(arm_label, time_label, value, p_emp)), by = c('arm_label','time_label','value'), all.x = TRUE)
set(pos, pos[, which(is.na(p_emp))], 'p_emp', 0.)

p <- ggplot(pos, aes(x = value)) +
  geom_col(aes(fill = interaction(arm_label,time_label), y = p_emp), alpha = 1) +
  geom_linerange(aes(ymin = q_lower, ymax = q_upper), colour = 'black') +
  geom_point(aes(y = median), colour = 'black', pch = 18, size = 2) +
  scale_fill_futurama() +
  facet_grid(arm_label ~ time_label) +
  theme_bw() +
  theme(legend.position = 'bottom') +
  labs(x = 'aggregated Likert outcomes', fill = '', colour = '', y = 'prob')
ggsave(file = file.path(dir.out, paste0(file.prefix,'m0_frequencies.png')), 
       plot = p, 
       h = 10, 
       w = 8
       )
```

This is the model fit to the aggregated Likert outcomes when considered as counts. With 10 questions targeting the same behaviour, it is immediately clear that the Poisson model provides an unacceptably poor fit to the aggregated data.

```{r vac_agg_poisson_without_unit_effects_extract_class_probs_plot, include=TRUE, eval=TRUE, echo=FALSE, tidy=FALSE, fig.align='center', out.width='80%'}
knitr::include_graphics( file.path(dir.out, paste0(file.prefix,'m0_frequencies.png')) )
```

## On Likert data for model comparison

I will now again consider the raw data for each survey question, but instead of interpreting these data as Likert scale outcomes,
I will interpret these data as counts, and model these with a Poisson distribution. 

I will start again
with the model without participant effects.

```{r vac_likert_poisson_without_unit_effects_run_model, include=FALSE, eval=TRUE, message=FALSE, echo=FALSE, tidy=FALSE, cache=TRUE}
file.prefix <- 'vac_poisson_likert_'

# define data in format needed for model specification
stan_data <- list()
stan_data$N <- nrow(dcat3)
stan_data$K <- max(dcat3$value - 1L)
stan_data$P <- max(dcat3$group)
stan_data$y <- dcat3$value - 1L
stan_data$group_of_obs <- dcat3$group

# sample
vac_poisson_log_likert_fit_m0 <- poisson_log_compiled_m0$sample(
  data = stan_data,
  seed = 123,
  chains = 2,
  parallel_chains = 2,
  iter_warmup = 5e2,
  iter_sampling = 15e2,
  refresh = 500,
  init = list(list(beta_0 = 6, beta_groups_m1 = rep(0,3)), list(beta_0 = 6, beta_groups_m1 = rep(0,3))),
  save_warmup = TRUE
)

# save output to RDS
vac_poisson_log_likert_fit_m0$save_object(file = file.path(dir.out, paste0(file.prefix,"m0_stan.rds")))
```

```{r vac_likert_poisson_without_unit_effects_check mixing convergence, include=FALSE, eval=TRUE, echo=FALSE, tidy=FALSE, cache=TRUE}
#	check convergence and check mixing
#	get 95% credible intervals
tmp <- vac_poisson_log_likert_fit_m0$summary(
  variables = c('beta_groups_m1','beta_0'),
  posterior::default_summary_measures(),
  posterior::default_convergence_measures(),
  extra_quantiles = ~posterior::quantile2(., probs = c(.0275, .975))
)
tmp <- as.data.table(tmp)
tmp
  
# parameter with lowest ess_bulk
worst_var <- tmp$variable[ which.min(tmp$ess_bulk) ]

# extract samples
po <- vac_poisson_log_likert_fit_m0$draws(
  variables = c("lp__", worst_var),
  inc_warmup = TRUE,
  format = "draws_array"
  )

# make worst trace plot
p <- bayesplot:::mcmc_trace(po,  
                            pars = c("lp__",worst_var), 
                            n_warmup = 500,
                            facet_args = list(nrow = 2)
                            )
p <- p + theme_bw()
ggsave(file = file.path(dir.out, paste0(file.prefix,'m0_worsttrace.png')), 
       plot = p, 
       h = 10, 
       w = 8
       )

# make intervals plot
po <- vac_poisson_log_likert_fit_m0$draws(
  variables = c('beta_groups_m1','beta_group','beta_0'),
  inc_warmup = FALSE,
  format = "draws_array"
  )

color_scheme_set("teal")
p <- bayesplot::mcmc_intervals(po, prob = 0.5, prob_outer = 0.95, outer_size = 1, point_size = 2) +
  theme_bw()
ggsave(file = file.path(dir.out, paste0(file.prefix,'m0_intervals.png')), 
       plot = p, 
       h = 7, 
       w = 8, 
       limitsize = FALSE
       )

po <- vac_poisson_log_likert_fit_m0$draws(
  variables = c('beta_groups_m1','beta_0'),
  inc_warmup = FALSE,
  format = "draws_array"
  )

bayesplot::color_scheme_set('viridisC')
p <- bayesplot::mcmc_pairs(po, 
                           diag_fun = "dens", 
                           off_diag_fun = "hex"
                           ) 
ggsave(file = file.path(dir.out, paste0(file.prefix,'m0_pairsplot.png')), 
       plot = p, 
       h = 30, 
       w = 30, 
       limitsize = FALSE
       )
bayesplot::color_scheme_set('brewer-RdYlBu')

# make posterior predictive check

# create median and 95\% credible intervals
po <- vac_poisson_log_likert_fit_m0$draws(
  variables = c('ypred'),
  inc_warmup = FALSE,
  format = "draws_df"
  )
po <- as.data.table(po)
po <- data.table::melt(po, id.vars = c('.draw','.chain','.iteration'), value.name = 'ypred')
po[, oid := as.integer(gsub('ypred\\[([0-9]+)\\]','\\1',variable)) ]
pos <- 
  po[,
     list( summary_value = quantile(ypred, prob = c(0.025, 0.25, 0.5, 0.75, 0.975)),
           summary_name = c('q_lower','iqr_lower','median','iqr_upper','q_upper') 
           ),
     by = 'oid'
     ]
pos <- 
  data.table::dcast(pos,
                    oid ~ summary_name, 
                    value.var = 'summary_value'
                    )
pos <- merge(pos, dcat3, by = 'oid')
pos[, IN_PPI := value >= q_lower & value <= q_upper]
pos[, mean(IN_PPI)]

# plot posterior predictive check
p <- ggplot(pos, aes(x = oid, group = oid)) + 
  geom_boxplot( aes( ymin = q_lower,
                     lower = iqr_lower,
                     middle = median,
                     upper = iqr_upper,
                     ymax = q_upper),
                stat = 'identity') +
  geom_point( aes(y = value, colour = IN_PPI ) ) +
  facet_grid(variable ~ arm_label+time_label, scales = 'free') +
  scale_x_discrete() +
  scale_y_continuous() +
  ggsci::scale_color_npg() +
  labs(x = '', 
       y = 'outcome', 
       colour = 'within\n95% posterior\nprediction\ninterval') +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 45,vjust = 1,hjust = 1))
ggsave(file = file.path(dir.out, paste0(file.prefix,'m0_ppcheck.png')), 
       p, 
       w = 12, 
       h = 12)
```

```{r vac_likert_poisson_without_unit_effects_extract class probs, include=FALSE, eval=TRUE, echo=FALSE, tidy=FALSE, cache=TRUE}
# extract model probabilities
po <- vac_poisson_log_likert_fit_m0$draws(
  variables = c('beta_0','beta_group'),
  inc_warmup = FALSE,
  format = "draws_df"
  )
po <- as.data.table(po)
po <- data.table::melt(po, id.vars = c('.draw','.chain','.iteration','beta_0'))
po[, group := as.integer(gsub('beta_group\\[([0-9])\\]','\\1',variable)) ]
po[, value := exp(beta_0 + value)]
po <- po[, list(value = 0:3, pdf = dpois(0:3, value)), by = c('.draw','group')]
tmp <- po[, list(total = sum(pdf)), by = c('.draw','group')]
po <- merge(po, tmp, by = c('.draw','group'))
po[, pdf := pdf/total]

pos <- 
  po[,
     list( summary_value = quantile(pdf, prob = c(0.025, 0.25, 0.5, 0.75, 0.975)),
           summary_name = c('q_lower','iqr_lower','median','iqr_upper','q_upper') 
           ),
     by = c('group','value')
     ]
pos <- 
  data.table::dcast(pos,
                    group + value ~ summary_name, 
                    value.var = 'summary_value'
                    )

tmp <- unique( subset(dcat3, select = c(group, arm, arm_label, time, time_label, value, value_label)) )
tmp[, value := value - 1L]
pos <- merge(pos, tmp, by = c('group','value'))

# Add empirical frequencies
tmp <- dcat3[, list(n = length(pid)), by = c('arm_label','time_label','variable','value','value_label')]
tmp2 <- tmp[, list(total = sum(n)), by = c('arm_label','time_label','variable')]
tmp <- merge(tmp, tmp2, by = c('arm_label','time_label','variable'))
tmp[, p_emp := n/total]
tmp[, value := value - 1L]
pos <- merge(pos, subset(tmp, select = -c(n, total)), by = c('arm_label','time_label','value','value_label'), all.x = TRUE)
set(pos, pos[, which(is.na(p_emp))], 'p_emp', 0.)

p <- ggplot(pos, aes(x = value)) +
  geom_col(aes(fill = variable, y = p_emp), alpha = 1, position = position_dodge(0.9)) +
  geom_linerange(aes(ymin = q_lower, ymax = q_upper), colour = 'black') +
  geom_point(aes(y = median), colour = 'black', pch = 18, size = 2) +
  scale_fill_simpsons() +
  scale_y_continuous(labels = scales::percent) +
  facet_grid(arm_label ~ time_label) +
  theme_bw() +
  theme(legend.position = 'bottom') +
  labs(x = 'Likert outcomes as counts', fill = '', colour = '', y = 'proportion of coutcomes')
ggsave(file = file.path(dir.out, paste0(file.prefix,'m0_frequencies.png')), 
       plot = p, 
       h = 10, 
       w = 8
       )
```

We can see that the Poisson fit always follows a unimodal shape that does not capture the increasing trends in frequencies of the 2 and 3 category, particularly in the Control group.

```{r vac_likert_poisson_without_unit_effects_extract_probs_plot, include=TRUE, eval=TRUE, echo=FALSE, tidy=FALSE, fig.align='center', out.width='80%'}
knitr::include_graphics( file.path(dir.out, paste0(file.prefix,'m0_frequencies.png')) )
```

## Model with participant effects

```{r vac_likert_poisson_with_unit_effects_run_model, include=FALSE, eval=TRUE, echo=FALSE, message=FALSE, tidy=FALSE, cache=TRUE}
# define data in format needed for model specification
stan_data <- list()
stan_data$N <- nrow(dcat3)
stan_data$K <- max(dcat3$value - 1L)
stan_data$P <- max(dcat3$group)
stan_data$y <- dcat3$value - 1L
stan_data$group_of_obs <- dcat3$group
stan_data$U <- max(dcat3$pid)
stan_data$unit_of_obs <- dcat3$pid

# sample
vac_poisson_log_likert_fit_m1 <- poisson_log_compiled_m1$sample(
  data = stan_data,
  seed = 123,
  chains = 2,
  parallel_chains = 2,
  iter_warmup = 5e2,
  iter_sampling = 15e2,
  refresh = 500,
  init = list(list(beta_0 = 6, beta_groups_m1 = rep(0,3)), list(beta_0 = 6, beta_groups_m1 = rep(0,3))),
  save_warmup = TRUE
)

# save output to RDS
vac_poisson_log_likert_fit_m1$save_object(file = file.path(dir.out, paste0(file.prefix,"m1_stan.rds")))
```

```{r vac_likert_poisson_with_unit_effects_check mixing convergence, include=FALSE, eval=TRUE, echo=FALSE, tidy=FALSE, cache=TRUE}
#	check convergence and check mixing
#	get 95% credible intervals
tmp <- vac_poisson_log_likert_fit_m1$summary(
  variables = c('beta_groups_m1','beta_0','beta_unit_m1','beta_unit_sd'),
  posterior::default_summary_measures(),
  posterior::default_convergence_measures(),
  extra_quantiles = ~posterior::quantile2(., probs = c(.0275, .975))
)
tmp <- as.data.table(tmp)
tmp
  
# parameter with lowest ess_bulk
worst_var <- tmp$variable[ which.min(tmp$ess_bulk) ]

# extract samples
po <- vac_poisson_log_likert_fit_m1$draws(
  variables = c("lp__", worst_var),
  inc_warmup = TRUE,
  format = "draws_array"
  )

# make worst trace plot
p <- bayesplot:::mcmc_trace(po,  
                            pars = c("lp__",worst_var), 
                            n_warmup = 500,
                            facet_args = list(nrow = 2)
                            )
p <- p + theme_bw()
ggsave(file = file.path(dir.out, paste0(file.prefix,'m1_worsttrace.png')), 
       plot = p, 
       h = 10, 
       w = 8
       )

# make intervals plot
po <- vac_poisson_log_likert_fit_m1$draws(
  variables = c('beta_groups_m1','beta_group','beta_0','beta_unit_m1','beta_unit_sd'),
  inc_warmup = FALSE,
  format = "draws_array"
  )

color_scheme_set("teal")
p <- bayesplot::mcmc_intervals(po, prob = 0.5, prob_outer = 0.95, outer_size = 1, point_size = 2) +
  theme_bw()
ggsave(file = file.path(dir.out, paste0(file.prefix,'m1_intervals.png')), 
       plot = p, 
       h = 25, 
       w = 8, 
       limitsize = FALSE
       )

po <- vac_poisson_log_likert_fit_m1$draws(
  variables = c('beta_groups_m1','beta_0'),
  inc_warmup = FALSE,
  format = "draws_array"
  )

bayesplot::color_scheme_set('viridisC')
p <- bayesplot::mcmc_pairs(po, 
                           diag_fun = "dens", 
                           off_diag_fun = "hex"
                           ) 
ggsave(file = file.path(dir.out, paste0(file.prefix,'m1_pairsplot.png')), 
       plot = p, 
       h = 30, 
       w = 30, 
       limitsize = FALSE
       )
bayesplot::color_scheme_set('brewer-RdYlBu')

# make posterior predictive check

# create median and 95\% credible intervals
po <- vac_poisson_log_likert_fit_m1$draws(
  variables = c('ypred'),
  inc_warmup = FALSE,
  format = "draws_df"
  )
po <- as.data.table(po)
po <- data.table::melt(po, id.vars = c('.draw','.chain','.iteration'), value.name = 'ypred')
po[, oid := as.integer(gsub('ypred\\[([0-9]+)\\]','\\1',variable)) ]
pos <- 
  po[,
     list( summary_value = quantile(ypred, prob = c(0.025, 0.25, 0.5, 0.75, 0.975)),
           summary_name = c('q_lower','iqr_lower','median','iqr_upper','q_upper') 
           ),
     by = 'oid'
     ]
pos <- 
  data.table::dcast(pos,
                    oid ~ summary_name, 
                    value.var = 'summary_value'
                    )
pos <- merge(pos, dcat3, by = 'oid')
pos[, IN_PPI := value >= q_lower & value <= q_upper]
pos[, mean(IN_PPI)]

# plot posterior predictive check
p <- ggplot(pos, aes(x = oid, group = oid)) + 
  geom_boxplot( aes( ymin = q_lower,
                     lower = iqr_lower,
                     middle = median,
                     upper = iqr_upper,
                     ymax = q_upper),
                stat = 'identity') +
  geom_point( aes(y = value, colour = IN_PPI ) ) +
  facet_grid(variable ~ arm_label+time_label, scales = 'free') +
  scale_x_discrete() +
  scale_y_continuous() +
  ggsci::scale_color_npg() +
  labs(x = '', 
       y = 'outcome', 
       colour = 'within\n95% posterior\nprediction\ninterval') +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 45,vjust = 1,hjust = 1))
ggsave(file = file.path(dir.out, paste0(file.prefix,'m1_ppcheck.png')), 
       p, 
       w = 12, 
       h = 12)
```

```{r vac_likert_poisson_with_unit_effects_extract class probs, include=FALSE, eval=TRUE, echo=FALSE, tidy=FALSE, cache=TRUE}
# extract model probabilities
po <- vac_poisson_log_likert_fit_m1$draws(
  variables = c('log_lambda_group'),
  inc_warmup = FALSE,
  format = "draws_df"
  )
po <- as.data.table(po)
po <- data.table::melt(po, id.vars = c('.draw','.chain','.iteration'))
po[, group := as.integer(gsub('log_lambda_group\\[([0-9])\\]','\\1',variable)) ]
po[, value := exp(value)]
po <- po[, list(value = 0:3, pdf = dpois(0:3, value)), by = c('.draw','group')]
tmp <- po[, list(total = sum(pdf)), by = c('.draw','group')]
po <- merge(po, tmp, by = c('.draw','group'))
po[, pdf := pdf/total]

pos <- 
  po[,
     list( summary_value = quantile(pdf, prob = c(0.025, 0.25, 0.5, 0.75, 0.975)),
           summary_name = c('q_lower','iqr_lower','median','iqr_upper','q_upper') 
           ),
     by = c('group','value')
     ]
pos <- 
  data.table::dcast(pos,
                    group + value ~ summary_name, 
                    value.var = 'summary_value'
                    )

tmp <- unique( subset(dcat3, select = c(group, arm, arm_label, time, time_label, value, value_label)) )
tmp[, value := value - 1L]
pos <- merge(pos, tmp, by = c('group','value'))

# Add empirical frequencies
tmp <- dcat3[, list(n = length(pid)), by = c('arm_label','time_label','variable','value','value_label')]
tmp2 <- tmp[, list(total = sum(n)), by = c('arm_label','time_label','variable')]
tmp <- merge(tmp, tmp2, by = c('arm_label','time_label','variable'))
tmp[, p_emp := n/total]
tmp[, value := value - 1L]
pos <- merge(pos, subset(tmp, select = -c(n, total)), by = c('arm_label','time_label','value','value_label'), all.x = TRUE)
set(pos, pos[, which(is.na(p_emp))], 'p_emp', 0.)

p <- ggplot(pos, aes(x = value)) +
  geom_col(aes(fill = variable, y = p_emp), alpha = 1, position = position_dodge(0.9)) +
  geom_linerange(aes(ymin = q_lower, ymax = q_upper), colour = 'black') +
  geom_point(aes(y = median), colour = 'black', pch = 18, size = 2) +
  scale_fill_simpsons() +
  scale_y_continuous(labels = scales::percent) +
  facet_grid(arm_label ~ time_label) +
  theme_bw() +
  theme(legend.position = 'bottom') +
  labs(x = 'Likert outcomes as counts', fill = '', colour = '', y = 'proportion of coutcomes')
ggsave(file = file.path(dir.out, paste0(file.prefix,'m1_frequencies.png')), 
       plot = p, 
       h = 10, 
       w = 8
       )
```

This is the fit of the Poisson model with participant effects. Model accuracy remains essentially unchanged.


```{r vac_likert_poisson_with_unit_effects_extract_class_probs_plot, include=TRUE, eval=TRUE, echo=FALSE, tidy=FALSE, fig.align='center', out.width='80%'}
knitr::include_graphics( file.path(dir.out, paste0(file.prefix,'m1_frequencies.png')) )
```

# VAC - Ordered Cat Latent Factor model

```{r vac_ordered_logistic_latent_factor_run_model, include=TRUE, eval=TRUE, echo=TRUE, message=FALSE, tidy=FALSE, cache=TRUE}
file.prefix <- 'vac_ordered_categorical_'

# define data in format needed for model specification
stan_data <- list()
stan_data$N <- nrow(dcat3)
stan_data$P <- max(dcat3$group)
stan_data$Q <- max(dcat3$vid)
stan_data$U <- max(dcat3$pid)
stan_data$K <- length(unique(dcat3$value))
stan_data$y <- dcat3$value
stan_data$group_of_obs <- dcat3$group
stan_data$question_of_obs <- dcat3$vid
stan_data$unit_of_obs <- dcat3$pid

# sample
vac_ordered_categorical_logit_fit_m5 <- ordered_categorical_logit_compiled_m5$sample(
  data = stan_data,
  seed = 123,
  chains = 2,
  parallel_chains = 2,
  iter_warmup = 5e2,
  iter_sampling = 15e2,
  refresh = 500,
  save_warmup = TRUE
)

# save output to RDS
vac_ordered_categorical_logit_fit_m5$save_object(file = file.path(dir.out, paste0(file.prefix,"m5_stan.rds")))
```

```{r vac_ordered_logistic_latent_factor_load_model, include=FALSE, eval=TRUE, echo=FALSE, tidy=FALSE, cache=TRUE}
vac_ordered_categorical_logit_fit_m5 <- readRDS(file = file.path(dir.out, paste0(file.prefix,'m5_stan.rds')))
```

```{r vac_ordered_logistic_latent_factor_check_mixing_convergence, include=FALSE, eval=TRUE, echo=FALSE, tidy=FALSE, cache=TRUE}
#	check convergence and check mixing
#	get 95% credible intervals
tmp <- vac_ordered_categorical_logit_fit_m5$summary(
  variables = c('factor_group','beta_unit','beta_unit_sd','beta_questions',
                'loadings_questions_m1','cut_point_1','cut_point_gaps_groups'),
  posterior::default_summary_measures(),
  posterior::default_convergence_measures(),
  extra_quantiles = ~posterior::quantile2(., probs = c(.0275, .975))
)
tmp <- as.data.table(tmp)
tmp
  
# parameter with lowest ess_bulk
worst_var <- tmp$variable[ which.min(tmp$ess_bulk) ]

# extract samples
po <- vac_ordered_categorical_logit_fit_m5$draws(
  variables = c("lp__", worst_var),
  inc_warmup = TRUE,
  format = "draws_array"
  )

# make worst trace plot
p <- bayesplot:::mcmc_trace(po,  
                            pars = c("lp__",worst_var), 
                            n_warmup = 500,
                            facet_args = list(nrow = 2)
                            )
p <- p + theme_bw()
ggsave(file = file.path(dir.out, paste0(file.prefix,'m5_worsttrace.png')), 
       plot = p, 
       h = 10, 
       w = 8
       )

# make intervals plot
po <- vac_ordered_categorical_logit_fit_m5$draws(
  variables = c('factor_group','beta_questions','loadings_questions',
                'cut_point_1','cut_point_gaps_groups',
                'beta_unit_sd','beta_unit'),
  inc_warmup = FALSE,
  format = "draws_array"
  )

color_scheme_set("teal")
p <- bayesplot::mcmc_intervals(po, prob = 0.5, prob_outer = 0.95, outer_size = 1, point_size = 2) +
  theme_bw()
ggsave(file = file.path(dir.out, paste0(file.prefix,'m5_intervals.png')), 
       plot = p, 
       h = 40, 
       w = 8, 
       limitsize = FALSE
       )

# make pairs plot
po <- vac_ordered_categorical_logit_fit_m5$draws(
  variables = c('factor_group','cut_point_1','cut_point_gaps_groups'),
  inc_warmup = FALSE,
  format = "draws_array"
  )

bayesplot::color_scheme_set('viridisC')
p <- bayesplot::mcmc_pairs(po, 
                           diag_fun = "dens", 
                           off_diag_fun = "hex"
                           ) 
ggsave(file = file.path(dir.out, paste0(file.prefix,'m5_pairsplot.png')), 
       plot = p, 
       h = 30, 
       w = 30, 
       limitsize = FALSE
       )
bayesplot::color_scheme_set('brewer-RdYlBu')

# make posterior predictive check

# create median and 95\% credible intervals
po <- vac_ordered_categorical_logit_fit_m5$draws(
  variables = c('ypred'),
  inc_warmup = FALSE,
  format = "draws_df"
  )
po <- as.data.table(po)
po <- data.table::melt(po, id.vars = c('.draw','.chain','.iteration'), value.name = 'ypred')
po[, oid := as.integer(gsub('ypred\\[([0-9]+)\\]','\\1',variable)) ]
pos <- 
  po[,
     list( summary_value = quantile(ypred, prob = c(0.025, 0.25, 0.5, 0.75, 0.975)),
           summary_name = c('q_lower','iqr_lower','median','iqr_upper','q_upper') 
           ),
     by = 'oid'
     ]
pos <- 
  data.table::dcast(pos,
                    oid ~ summary_name, 
                    value.var = 'summary_value'
                    )
pos <- merge(pos, dcat3, by = 'oid')
pos[, IN_PPI := value >= q_lower & value <= q_upper]

# plot posterior predictive check
p <- ggplot(pos, aes(x = oid, group = oid)) + 
  geom_boxplot( aes( ymin = q_lower,
                     lower = iqr_lower,
                     middle = median,
                     upper = iqr_upper,
                     ymax = q_upper),
                stat = 'identity') +
  geom_point( aes(y = value, colour = IN_PPI ) ) +
  facet_grid(variable ~ arm_label+time_label, scales = 'free') +
  scale_x_discrete() +
  scale_y_continuous() +
  ggsci::scale_color_npg() +
  labs(x = '', 
       y = 'outcome', 
       colour = 'within\n95% posterior\nprediction\ninterval') +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 45,vjust = 1,hjust = 1))
ggsave(file = file.path(dir.out, paste0(file.prefix,'m5_ppcheck.png')), 
       p, 
       w = 12, 
       h = 12)
```

```{r vac_ordered_logistic_latent_factor_extract_participant_effects, include=FALSE, eval=TRUE, echo=FALSE, tidy=FALSE, cache=TRUE}
# extract participant effects
po <- vac_ordered_categorical_logit_fit_m5$draws(
  variables = c('beta_unit_sd','beta_unit'),
  inc_warmup = FALSE,
  format = "draws_df"
  )
po <- as.data.table(po)
po <- data.table::melt(po, id.vars = c('.draw','.chain','.iteration'))
tmp <- subset(po, variable == 'beta_unit_sd')
tmp <- data.table::dcast(tmp, .draw ~ variable, value.var = 'value')
po <- subset(po, variable != 'beta_unit_sd')
po <- merge(po, tmp, by = '.draw')
po[, pid := as.integer(gsub('beta_unit\\[([0-9]+)\\]','\\1',variable)) ]
po[, value := value * beta_unit_sd]

pos <- po[, 
          list( summary_value = quantile(value, prob = c(0.025, 0.25, 0.5, 0.75, 0.975)),
                summary_name = c('q_lower','iqr_lower','median','iqr_upper','q_upper') 
                ), 
          by = c('pid')
          ]
pos <- data.table::dcast(pos, pid ~ summary_name, value.var = 'summary_value')
tmp <- unique(subset(dcat3, select = c(pid, arm_label, time_label)))
pos <- merge(pos, tmp, by = c('pid'))
tmp <- dcat1[, list(avg_value = mean(value)), by = 'pid']
tmp[, avg_value_jitter := avg_value + runif(nrow(tmp), min = -1/8, max = 1/8)]
pos <- merge(pos, tmp, by = 'pid')

tmp <- pos[, list(avg_effect = mean(median)), by = c('arm_label','time_label')]

p <- ggplot(data = pos, aes( x = avg_value_jitter)) +
  geom_hline(data = tmp, aes( yintercept = avg_effect), colour = 'grey50') +
  geom_linerange(aes( ymin = q_lower, ymax = q_upper), 
           stat = 'identity') +
  geom_point(aes( y = median)) +
  labs(x = '\naverage response of participant', y = 'participant effect') +
  theme_bw() +
  facet_grid(arm_label ~ time_label, scales = 'free') +
  theme(axis.text.x = element_text(angle = 45,vjust = 1,hjust = 1),
        legend.position = 'bottom'
        )
ggsave(file = file.path(dir.out, paste0(file.prefix,'m5_participant_effects.png')), 
       plot = p, 
       h = 8, 
       w = 12
       )
```

Here are the participant effects around the group mean latent factors, showing similar behaviour as before. This again demonstrates that (1) the added baseline response effects are not able to explain the data without latent traits, and (2) there is substantial individual-level heterogeneity around the group mean latent factors.

```{r vac_ordered_logistic_latent_factor_participant_effect_plot, include=TRUE, eval=TRUE, echo=FALSE, tidy=FALSE, fig.align='center', out.width='80%'}
knitr::include_graphics( file.path(dir.out, paste0(file.prefix,'m5_participant_effects.png')) )
```

```{r vac_ordered_logistic_latent_factor_latent_eta, include=FALSE, eval=TRUE, echo=FALSE, tidy=FALSE, cache=TRUE}
# extract model probabilities
po <- vac_ordered_categorical_logit_fit_m5$draws(
  variables = c('mean_eta'),
  inc_warmup = FALSE,
  format = "draws_df"
  )
po <- as.data.table(po)
po <- data.table::melt(po, id.vars = c('.draw','.chain','.iteration'), value.name = 'mean_eta')
po[, group := as.integer(gsub('(.*)\\[([0-9]),([0-9])\\]$','\\2',variable)) ]
po[, vid := as.integer(gsub('(.*)\\[([0-9]),([0-9])\\]$','\\3',variable)) ]
set(po, NULL, 'variable', NULL)
tmp <- unique( subset(dcat3, select = c(group, arm, arm_label, time, time_label, variable, vid)) )
po <- merge(po, tmp, by = c('group','vid'))
po <- data.table::dcast(po, .draw + arm + arm_label + time + time_label + group ~ variable, value.var = 'mean_eta')
p <- GGally::ggpairs(po, 
                columns = c('VACE_threat','VACE_refusespeak','VACE_insult',
                            'VACE_shout','VACE_abandon','VACP_object','VACP_push','VACP_hit','VACP_face'),
                ggplot2::aes(colour = interaction(arm_label, time_label)),
                lower = list(continuous = ggally_density),
                diag = list(continuous = 'blankDiag')
                )
p <- p + scale_fill_npg() + 
  theme_bw()
ggsave(file = file.path(dir.out, paste0(file.prefix,'m5_eta_pairsplot.png')), 
       plot = p, 
       h = 20, 
       w = 20
       )
```

Here is the polychoric correlation plot:

```{r vac_ordered_logistic_latent_factor_latent_eta_plot, include=TRUE, eval=TRUE, echo=FALSE, tidy=FALSE, fig.align='center', out.width='80%'}
knitr::include_graphics( file.path(dir.out, paste0(file.prefix,'m5_eta_pairsplot.png')) )
```

```{r vac_ordered_logistic_latent_factor_extract_class_probs_by_n, include=FALSE, eval=TRUE, echo=FALSE, tidy=FALSE, cache=TRUE}
po <- vac_ordered_categorical_logit_fit_m5$draws(
  variables = c('ordered_prob_by_obs'),
  inc_warmup = FALSE,
  format = "draws_df"
  )
po <- as.data.table(po)
po <- data.table::melt(po, id.vars = c('.draw','.chain','.iteration'), value.name = 'prob')
po[, value := as.integer(gsub('ordered_prob_by_obs\\[([0-9]),([0-9]+)\\]','\\1',variable)) ]
po[, oid := as.integer(gsub('ordered_prob_by_obs\\[([0-9]),([0-9]+)\\]','\\2',variable)) ]
set(po, NULL, 'variable', NULL)
tmp <- unique(subset(dcat3, select = c(oid, pid, vid, group )))
po <- merge(po, tmp, by = c('oid'))
po <- po[, list(prob = mean(prob)), by = c('.draw','group','value')]

pos <- po[, 
          list( summary_value = quantile(prob, prob = c(0.025, 0.25, 0.5, 0.75, 0.975)),
                summary_name = c('q_lower','iqr_lower','median','iqr_upper','q_upper') 
                ), 
          by = c('group','value')
          ]
pos <- data.table::dcast(pos, group + value ~ summary_name, value.var = 'summary_value')

tmp <- unique( subset(dcat3, select = c(group, arm, arm_label, time, time_label, value, value_label)) )
pos <- merge(pos, tmp, by = c('group','value'))

tmp <- dcat3[, list(n = length(pid)), by = c('arm_label','time_label','value_label','value')]
tmp2 <- tmp[, list(total = sum(n)), by = c('arm_label','time_label')]
tmp <- merge(tmp, tmp2, by = c('arm_label','time_label'))
tmp[, p_emp := n/total]
pos <- merge(pos, subset(tmp, select = -c(value, n, total)), by = c('arm_label','time_label','value_label'))

p <- ggplot(data = pos, aes( x = value_label, fill = interaction(arm_label,time_label))) +
  geom_bar(aes( y = p_emp), 
           stat = 'identity', 
           position = position_dodge()) +
  geom_point(aes( y = median),
             position = position_dodge(0.9), colour = 'black') +
  geom_boxplot( aes(ymin = q_lower, lower = iqr_lower, middle = median, upper = iqr_upper, ymax = q_upper), 
                position = position_dodge(0.9),
                stat = 'identity',
                alpha = 0,
                width = 0.3) +
  scale_fill_futurama() +
  scale_y_continuous(labels = scales::percent) +
  labs(x = '\nViolence against children\nVAC outcome', y = 'group-level\nposterior probability', fill = '') +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 45,vjust = 1,hjust = 1),
        legend.position = 'bottom'
        )
ggsave(file = file.path(dir.out, paste0(file.prefix,'m5_probs_barplot.png')), 
       plot = p, 
       h = 8, 
       w = 8
       )
```

Here is the model fit (in black) relative to the aggregated survey items. Hoorary, we have a good fit relative to our primary target statistics:

```{r vac_ordered_logistic_latent_factor_plot, include=TRUE, eval=TRUE, echo=FALSE, tidy=FALSE, fig.align='center', out.width='80%'}
knitr::include_graphics( file.path(dir.out, paste0(file.prefix,'m5_probs_barplot.png')) )
```

Here is the model fit (in black) relative to each survey item. We don't achieve a super good fit, though perhaps fine.

```{r vac_ordered_logistic_latent_factor_extract_class_probs_by_question, include=FALSE, eval=TRUE, echo=FALSE, tidy=FALSE, cache=TRUE}
po <- vac_ordered_categorical_logit_fit_m5$draws(
  variables = c('ordered_prob_by_obs'),
  inc_warmup = FALSE,
  format = "draws_df"
  )
po <- as.data.table(po)
po <- data.table::melt(po, id.vars = c('.draw','.chain','.iteration'), value.name = 'prob')
po[, value := as.integer(gsub('ordered_prob_by_obs\\[([0-9]),([0-9]+)\\]','\\1',variable)) ]
po[, oid := as.integer(gsub('ordered_prob_by_obs\\[([0-9]),([0-9]+)\\]','\\2',variable)) ]
set(po, NULL, 'variable', NULL)
tmp <- unique(subset(dcat3, select = c(oid, pid, vid, group )))
po <- merge(po, tmp, by = c('oid'))
po <- po[, list(prob = mean(prob)), by = c('.draw','group','vid','value')]

pos <- po[, 
          list( summary_value = quantile(prob, prob = c(0.025, 0.25, 0.5, 0.75, 0.975)),
                summary_name = c('q_lower','iqr_lower','median','iqr_upper','q_upper') 
                ), 
          by = c('group','vid','value')
          ]
pos <- data.table::dcast(pos, group + vid + value ~ summary_name, value.var = 'summary_value')

tmp <- unique( subset(dcat3, select = c(group, arm, arm_label, time, time_label, variable, vid, value, value_label)) )
pos <- merge(pos, tmp, by = c('group','vid','value'))

tmp <- dcat3[, list(n = length(pid)), by = c('arm_label','time_label','variable','value_label','value')]
tmp2 <- tmp[, list(total = sum(n)), by = c('arm_label','time_label','variable')]
tmp <- merge(tmp, tmp2, by = c('arm_label','time_label','variable'))
tmp[, p_emp := n/total]
pos <- merge(pos, subset(tmp, select = -c(value, n, total)), by = c('arm_label','time_label','variable','value_label'))

p <- ggplot(pos, aes(x = value_label, group = interaction(variable, value_label))) +
  geom_col(aes(fill = variable, y = p_emp), 
           position = position_dodge(0.9)
           ) +
  geom_boxplot( aes(ymin = q_lower, lower = iqr_lower, middle = median, upper = iqr_upper, ymax = q_upper), 
                position = position_dodge(0.9),
                stat = 'identity',
                alpha = 0,
                width = 0.3) +
  scale_fill_simpsons() +
  scale_y_continuous(labels = scales::percent) +
  facet_grid(arm_label ~ time_label) +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 45,vjust = 1,hjust = 1),
        legend.position = 'bottom'
        ) +
  labs(x = '', x = 'Likert scale', y = 'proportion of outcomes', fill = '')
ggsave(file = file.path(dir.out, paste0(file.prefix,'m5_frequencies_v2.png')), 
       plot = p, 
       h = 10, 
       w = 8
       )
```

```{r vac_ordered_logistic_latent_factor_extract_class_probs_by_question_plot, include=TRUE, eval=TRUE, echo=FALSE, tidy=FALSE, fig.align='center', out.width='80%'}
knitr::include_graphics( file.path(dir.out, paste0(file.prefix,'m5_frequencies_v2.png')) )
```

```{r vac_ordered_logistic_latent_factor_extract_class_probs_by_question3, include=FALSE, eval=TRUE, echo=FALSE, tidy=FALSE, cache=TRUE}
po <- vac_ordered_categorical_logit_fit_m5$draws(
  variables = c('ypred'),
  inc_warmup = FALSE,
  format = "draws_df"
  )
po <- as.data.table(po)
po <- data.table::melt(po, id.vars = c('.draw','.chain','.iteration'), value.name = 'value')
po[, oid := as.integer(gsub('ypred\\[([0-9]+)\\]','\\1',variable)) ]
set(po, NULL, 'variable', NULL)
tmp <- unique(subset(dcat3, select = c(oid, pid, variable, group )))
po <- merge(po, tmp, by = c('oid'))
tmp <- unique(subset(dcat3, select = c(value, value_label )))
po <- merge(po, tmp, by = c('value'))
po <- data.table::dcast( po, .draw + pid + group ~ variable, value.var = 'value_label')
po <- melt(po, id.vars = c('.draw','pid','group','VACE_threat'), value.name = 'value_label')
po <- po[, list(k = length(pid)), by = c('.draw', 'group', 'VACE_threat', 'value_label')]
tmp <- po[, list(n = sum(k)), by = c('.draw', 'group', 'VACE_threat')]
po <- merge(po, tmp, by = c('.draw', 'group', 'VACE_threat'))
po[, prob := k/n]
tmp <- CJ(.draw = unique(po$.draw),
          group = unique(po$group), 
          VACE_threat = unique(po$VACE_threat), 
          value_label = unique(po$VACE_threat))
po <- merge(tmp, po, by = c('.draw','group','VACE_threat','value_label'), all.x = TRUE)
set(po, po[, which(is.na(prob))], 'prob', 0.)
pos <- po[, 
          list( summary_value = quantile(prob, prob = c(0.025, 0.25, 0.5, 0.75, 0.975)),
                summary_name = c('q_lower','iqr_lower','median','iqr_upper','q_upper') 
                ), 
          by = c('group','VACE_threat','value_label')
          ]
pos <- data.table::dcast(pos, group + VACE_threat + value_label ~ summary_name, value.var = 'summary_value')
tmp <- unique( subset(dcat3, select = c(group, arm, arm_label, time, time_label)) )
pos <- merge(pos, tmp, by = c('group'))

tmp <- data.table:::dcast( dcat3, time_label + arm_label + pid ~ variable, value.var = 'value_label')
tmp <- melt(tmp, id.vars = c('time_label','arm_label','pid','VACE_threat'))
tmp <- tmp[, list(k = length(pid)), by = c('time_label','arm_label','VACE_threat','value')]
tmp2 <- tmp[, list(n = sum(k)), by = c('time_label','arm_label','VACE_threat')]
tmp <- merge(tmp, tmp2, by = c('time_label','arm_label','VACE_threat'))
tmp[, p_emp := k/n]
set(tmp, NULL, c('k','n'), NULL)
tmp2 <- CJ(time_label = unique(tmp$time_label), 
           arm_label = unique(tmp$arm_label), 
           VACE_threat = unique(tmp$VACE_threat), 
           value = unique(tmp$VACE_threat))
tmp <- merge(tmp2, tmp, by = c('arm_label','time_label','VACE_threat','value'), all.x = TRUE)
set(tmp, tmp[, which(is.na(p_emp))], 'p_emp', 0.)
setnames(tmp, 'value','value_label')
pos <- merge(pos, tmp, by = c('arm_label','time_label','VACE_threat','value_label'), all.y = TRUE)

p <- ggplot(pos, aes( x = VACE_threat, group = interaction(VACE_threat, value_label))) +
  geom_col(aes(y = p_emp, fill = value_label), 
           stat = 'identity',
           position = position_dodge(0.9)) +
  geom_boxplot( aes(ymin = q_lower, lower = iqr_lower, middle = median, upper = iqr_upper, ymax = q_upper), 
                position = position_dodge(0.9),
                stat = 'identity',
                alpha = 0,
                width = 0.3) +
  scale_fill_bmj() +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 45,vjust = 1,hjust = 1),
        legend.position = 'bottom'
        ) +
  labs(x = 'response to VACE_threat question', y = 'proportion', fill = 'responses to other VAC questions') +
  facet_grid(arm_label ~ time_label)
ggsave(file = file.path(dir.out, paste0(file.prefix,'m5_polychoric_correlations_across_PHQ_questions_same_arm-time.png')),
       plot = p, 
       h = 8, 
       w = 8
       )
```
Here is a visualisation of the estimated polychoric correlations. We can clearly see how the model is inducing some
correlations between item responses:

```{r vac_ordered_logistic_latent_factor_extract_class_probs_by_question2_plot, include=TRUE, eval=TRUE, echo=FALSE, tidy=FALSE, fig.align='center', out.width='80%'}
knitr::include_graphics( file.path(dir.out, paste0(file.prefix,'m5_polychoric_correlations_across_PHQ_questions_same_arm-time.png')) )
```


# VAC - Compare models

Here is the model comparison in terms of Bayesian leave-one-out (Bayes-LOO) predictive performance measured by expected log posterior density (ELPD):

```{r vac_Compare Bayesian LOO approximations, include=FALSE, eval=TRUE, echo=FALSE, message=FALSE, tidy=FALSE, cache=TRUE}
vac_ordered_categorical_logit_fit_m0 <- readRDS(file = file.path(dir.out, 'vac_ordered_categorical_m0_stan.rds'))
vac_ordered_categorical_logit_fit <- readRDS(file = file.path(dir.out, 'vac_ordered_categorical_m1_stan.rds'))
vac_ordered_categorical_logit_fit_m5 <- readRDS(file = file.path(dir.out, 'vac_ordered_categorical_m5_stan.rds'))
vac_poisson_log_likert_fit_m0 <- readRDS(file = file.path(dir.out, 'vac_poisson_likert_m0_stan.rds'))
vac_poisson_log_likert_fit_m1 <- readRDS(file = file.path(dir.out, 'vac_poisson_likert_m1_stan.rds'))

vac_poisson_log_likert_fit_m0_loo <- vac_poisson_log_likert_fit_m0$loo()
vac_poisson_log_likert_fit_m1_loo <- vac_poisson_log_likert_fit_m1$loo()
vac_ordered_categorical_logit_fit_m0_loo <- vac_ordered_categorical_logit_fit_m0$loo()
vac_ordered_categorical_logit_fit_loo <- vac_ordered_categorical_logit_fit$loo()
vac_ordered_categorical_logit_fit_m5_loo <- vac_ordered_categorical_logit_fit_m5$loo()
```

We see that the 1D latent factor model is the best performing model.

```{r vac_Compare Bayesian LOO approximations table, include=TRUE, eval=TRUE, echo=TRUE, tidy=FALSE, cache=TRUE}
# Model 1 - Poisson no participant effects
# Model 2 - Poisson with participant random effects
# Model 4 - Multi-group Ordered Cat, no participant effects
# Model 5 - Multi-group Ordered Cat, with participant random effects
# Model 9 - First Multi-group 1D Latent factor model
comp <- loo::loo_compare(vac_poisson_log_likert_fit_m0_loo,
                         vac_poisson_log_likert_fit_m1_loo,
                         vac_ordered_categorical_logit_fit_m0_loo, 
                         vac_ordered_categorical_logit_fit_loo,
                         vac_ordered_categorical_logit_fit_m5_loo
                         )

print(comp, simplify = FALSE)
```


# Final analysis

## Primary endpoints

The primary endpoint in this pre-post study is comparing the average change $\Delta_{g,f}$ in negative outcomes in the latent behavioral factors $f$ (parental mental health, violence against children) that occurr 'Nearly every day' between Baseline and Endline among the Intervention and Control groups $g \in \{I, C\}$, respectively. We will quantify the corresponding risk differences,
\begin{equation}
\text{RD}(f) = \Delta_{I,f} - \Delta_{C,f},
\end{equation}
for both the latent parental mental health and violence against children behaviours.
Here, the average change $\Delta_{g,f}$ in the intervention and control groups is defined as
\begin{equation}
\Delta_{g,f} = \frac{1}{N_g}\sum_{i \in g} \frac{1}{J}\sum_j p^{f, t=0}_{i,j,k=4} - p^{f, t=1}_{i,k=4},
\end{equation}
where $p^{f, t=1}_{i,j,k=4}$ describes the probability of a 'Nearly every day' response ($k=4$) to survey item $j$ of participant $i$ at time $t$.

```{r final_analysis_process_outputs, include=TRUE, eval=TRUE, echo=TRUE, tidy=FALSE, cache=TRUE}
# process Monte Carlo outputs

# process VAC
po <- vac_ordered_categorical_logit_fit_m5$draws(
  variables = c('ordered_prob_by_obs'),
  inc_warmup = FALSE,
  format = "draws_df"
  )
po <- as.data.table(po)
po <- data.table::melt(po, id.vars = c('.draw','.chain','.iteration'), value.name = 'prob')
po[, value := as.integer(gsub('ordered_prob_by_obs\\[([0-9]),([0-9]+)\\]','\\1',variable)) ]
po[, oid := as.integer(gsub('ordered_prob_by_obs\\[([0-9]),([0-9]+)\\]','\\2',variable)) ]
set(po, NULL, 'variable', NULL)
tmp <- unique(subset(dcat3, select = c(oid, pid, vid, group, arm, arm_label, time, time_label )))
po <- merge(po, tmp, by = c('oid'))
po <- subset(po, value == 4)
po <- data.table::dcast(po, .draw + pid + vid + arm + arm_label ~ time_label, value.var = 'prob')
po[, diff := Baseline - Endline]
po[, ratio := 1 - Endline / Baseline]
po <- po[, list(diff = mean(diff), ratio = mean(ratio)), by = c('.draw','arm','arm_label')]
po[, trait_label := 'Violence Against Children']
po.vac <- copy(po)

# process PHQ
po <- ordered_categorical_logit_fit_m5$draws(
  variables = c('ordered_prob_by_obs'),
  inc_warmup = FALSE,
  format = "draws_df"
  )
po <- as.data.table(po)
po <- data.table::melt(po, id.vars = c('.draw','.chain','.iteration'), value.name = 'prob')
po[, value := as.integer(gsub('ordered_prob_by_obs\\[([0-9]),([0-9]+)\\]','\\1',variable)) ]
po[, oid := as.integer(gsub('ordered_prob_by_obs\\[([0-9]),([0-9]+)\\]','\\2',variable)) ]
set(po, NULL, 'variable', NULL)
tmp <- unique(subset(dcat1, select = c(oid, pid, vid, group, arm, arm_label, time, time_label )))
po <- merge(po, tmp, by = c('oid'))
po <- subset(po, value == 4)
po <- data.table::dcast(po, .draw + pid + vid + arm + arm_label ~ time_label, value.var = 'prob')
po[, diff := Baseline - Endline]
po[, ratio := 1 - Endline / Baseline]
po <- po[, list(diff = mean(diff), ratio = mean(ratio)), by = c('.draw','arm','arm_label')]
po[, trait_label := 'Parental Mental Health']
po.pmh <- copy(po)
```

```{r final_analysis_plots_by_group, include=FALSE, eval=TRUE, echo=FALSE, tidy=FALSE, cache=TRUE}
po <- rbind(po.pmh, po.vac)
po <- melt(po, id.vars = c('.draw','arm','arm_label','trait_label'), measure.vars = c('diff','ratio'))
pos <- po[, 
          list( summary_value = quantile(value, prob = c(0.025, 0.25, 0.5, 0.75, 0.975)),
                summary_name = c('q_lower','iqr_lower','median','iqr_upper','q_upper') 
                ), 
          by = c('arm','arm_label','trait_label','variable')
          ]
pos <- data.table::dcast(pos, arm + arm_label + trait_label + variable ~ summary_name, value.var = 'summary_value')
pos[, variable_label := factor(variable, levels = c('diff','ratio'),
                               labels = c('risk reduction of outcome occurring Nearly Every Day\n100*(Baseline - Endline)\n(positive indicates improvement)','percent change in probability of outcome occurring Nearly Every Day\n100*(1 - Endline / Baseline)\n(positive indicates improvement)'))
    ]

p <- ggplot(subset(pos, variable == 'diff'), aes(x = trait_label, fill = arm_label)) +
  geom_hline(yintercept = 0, lty = 1, lwd = 1.25, colour = 'grey30') +
  geom_col(aes(y = median), 
           position = position_dodge(0.9)) +
  geom_errorbar(aes(ymin = q_lower, ymax = q_upper), 
                 colour = 'black', 
                 width = 0.3,
                 position = position_dodge(0.9)) +
  scale_fill_aaas() +
  scale_y_continuous(labels = scales::percent) +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 45,vjust = 1,hjust = 1),
        legend.position = 'top'
        ) +
  labs(x = 'latent trait', y = 'risk reduction of outcome occurring Nearly Every Day\n100*(Baseline - Endline)\n(positive indicates improvement)', fill = '') 
ggsave(file = file.path(dir.out, paste0('final_riskdifference_in_group.png')),
       plot = p, 
       h = 8, 
       w = 8
       )

p <- ggplot(subset(pos, variable == 'ratio'), aes(x = trait_label, fill = arm_label)) +
  geom_hline(yintercept = 0, lty = 1, lwd = 1.25, colour = 'grey30') +
  geom_col(aes(y = median), 
           position = position_dodge(0.9)) +
  geom_errorbar(aes(ymin = q_lower, ymax = q_upper), 
                 colour = 'black', 
                 width = 0.3,
                 position = position_dodge(0.9)) +
  scale_fill_aaas() +
  scale_y_continuous(labels = scales::percent) +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 45,vjust = 1,hjust = 1),
        legend.position = 'top'
        ) +
  labs(x = 'latent trait', y = 'percent change in probability of outcome occurring Nearly Every Day\n100*(1 - Endline / Baseline)\n(positive indicates improvement)', fill = '') 
ggsave(file = file.path(dir.out, paste0('final_percentchange_in_group.png')),
       plot = p, 
       h = 8, 
       w = 8
       )

p <- ggplot(pos, aes(x = trait_label, fill = arm_label)) +
  geom_hline(yintercept = 0, lty = 1, lwd = 1.25, colour = 'grey30') +
  geom_col(aes(y = median), 
           position = position_dodge(0.9)) +
  geom_errorbar(aes(ymin = q_lower, ymax = q_upper), 
                 colour = 'black', 
                 width = 0.3,
                 position = position_dodge(0.9)) +
  scale_fill_aaas() +
  scale_y_continuous(labels = scales::percent) +
  facet_grid(. ~ variable_label) +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 45,vjust = 1,hjust = 1),
        legend.position = 'top'
        ) +
  labs(x = 'latent trait', y = '', fill = '') 
ggsave(file = file.path(dir.out, paste0('final_percentchange_and_riskdifference_in_group.png')),
       plot = p, 
       h = 8, 
       w = 8
       )
```

I first plot the average changes Endline versus Outline among participants in the Intervention and Control group:

```{r final_analysis_plots_by_group_plot, include=TRUE, eval=TRUE, echo=FALSE, tidy=FALSE, fig.align='center', out.width='80%'}
knitr::include_graphics( file.path(dir.out, paste0('final_percentchange_and_riskdifference_in_group.png')) )
```

And here are the estimates of the primary endpoint for both Parental Mental Health and Violence Against Children:

```{r final_analysis_primary_endpoint, include=TRUE, eval=TRUE, echo=FALSE, tidy=FALSE, cache=TRUE}
po <- rbind(po.pmh, po.vac)
po <- melt(po, id.vars = c('.draw','arm','arm_label','trait_label'), measure.vars = c('diff','ratio'))
po <- data.table::dcast(po, .draw + trait_label ~ arm_label + variable, value.var = 'value')
po[, diff := Intervention_diff - Control_diff]
po[, ratio := 1 - (1 - Intervention_ratio) / (1 - Control_ratio) ]
po <- melt(po, id.vars = c('.draw','trait_label'), measure.vars = c('diff','ratio'))
pos <- po[, 
          list( summary_value = quantile(value, prob = c(0.025, 0.25, 0.5, 0.75, 0.975)),
                summary_name = c('q_lower','iqr_lower','median','iqr_upper','q_upper') 
                ), 
          by = c('variable','trait_label')
          ]
pos <- data.table::dcast(pos, variable + trait_label  ~ summary_name, value.var = 'summary_value')
pos[, label := paste0(round(median * 100, d = 1), '% (', round(q_lower * 100, d = 1), '% - ', round(q_upper * 100, d = 1),'%)')]
pos
```
