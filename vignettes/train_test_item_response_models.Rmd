---
title: "Train and test item response models on HPC"
date: "`r Sys.Date()`"
output: 
  bookdown::html_document2:
    toc: TRUE
    toc_float: TRUE
    highlight: tango
  bookdown::pdf_book:
    keep_tex: yes
---

<style type="text/css">
h1{
  font-size: 24pt;
}
h2{
  font-size: 18pt;
}
body{
  font-size: 12pt;
}
</style>

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Overview

This vignette demonstrates how to submit item response model validation jobs to an HPC cluster using PBS scheduling. The workflow involves:

1. Creating a JSON configuration file with model parameters
2. Generating a PBS job submission script
3. Submitting the job to the HPC scheduler

# Load packages and functions

```{r load-packages, include=TRUE, eval=TRUE, message=FALSE, echo=TRUE, warning=FALSE}
library(jsonlite)
library(here)

source(here::here("R", "hpc_submit_generic_job.R"))
```

# Define directories and job parameters

```{r define-paths, include=TRUE, eval=TRUE, message=FALSE, echo=TRUE, warning=FALSE}
# Define directories
dir_home <- "/Users/or105/Library/CloudStorage/OneDrive-ImperialCollegeLondon/OR_Work/2025/2025_project_Hope_Groups"
dir_data <- file.path(dir_home, "data")
dir_out <- file.path(dir_home, "colombia-validate-creditmodel-hpc")
dir_logs <- file.path(dir_out, "logs")

# Create output directories if they don't exist
if (!dir.exists(dir_out)) {
    dir.create(dir_out, recursive = TRUE)
}
if (!dir.exists(dir_logs)) {
    dir.create(dir_logs, recursive = TRUE)
}

# Define job name
job_name <- "validate_credit_model"
```

# Create JSON configuration file

```{r create-json-config, include=TRUE, eval=TRUE, message=FALSE, echo=TRUE, warning=FALSE}
# Define model parameters
config <- list(
    test_individuals_p = 0.5,
    test_items_p = 0.5,
    n_splits = 5,
    stan_file = here::here("src", "stan", "credit_model_2cats_v251120.stan"),
    dir_data = dir_data,
    dir_out = dir_out,
    chains = 4,
    parallel_chains = 4,
    iter_warmup = 1000,
    iter_sampling = 2000,
    seed = 42
)

# Save configuration to JSON file
json_file <- file.path(dir_out, paste0(job_name, "_config.json"))
write_json(config, json_file, pretty = TRUE, auto_unbox = TRUE)

cat("Created JSON configuration file:", json_file, "\n")
cat("\nConfiguration:\n")
cat(readLines(json_file), sep = "\n")
```

# Create and submit PBS job

```{r create-pbs-script, include=TRUE, eval=FALSE, message=FALSE, echo=TRUE, warning=FALSE}
# Define PBS job parameters
pbs_config <- list(
    script_path = here::here("R", "train_test_item_response_models.Rscript"),
    json_path = json_file,
    job_name = job_name,
    output_dir = dir_logs,
    walltime = "48:00:00",
    memory = "32gb",
    ncpus = 4,
    email = NULL, # Set to your email address for notifications
    email_options = "abe", # a=abort, b=begin, e=end
    header = '
    module load miniforge/3 tools/prod zlib libxml2/2.11.5-GCCcore-13.2.0\n
    eval "$(~/miniforge3/bin/conda shell.bash hook)"\n
    eval "$(mamba shell hook --shell bash)"\n
    mamba activate birtistic\n
    ',
    submit = FALSE # Set to TRUE to submit immediately
)

# Create PBS submission script
pbs_script <- do.call(hpc_submit_generic_job, pbs_config)

cat("\nPBS submission script created:", pbs_script, "\n")
cat("\nTo submit the job, run:\n")
cat("  qsub", pbs_script, "\n")
```

# Results

After the job completes, results will be available in the output directory:

- **Train/test splits**: `cm_split*_train_test_split.RData`
- **Model fits**: `cm_split*_trainingfit_*.rds`
- **Generated quantities**: `cm_split*_test_generated_quantities.rds`
- **Posterior predictive checks**: `cm_split*_test_ppcheck.png`
- **Summary statistics**: `validation_summary.rds` and `validation_summary.csv`


# Session information

```{r session-info, include=TRUE, eval=TRUE, message=FALSE, echo=TRUE, warning=FALSE}
sessionInfo()
```
